{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Q_USY0q3elC"
   },
   "source": [
    "MURA Pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "4PDxmp00SxDJ",
    "outputId": "fb6c9d1c-64d4-48c6-c4d7-0c58fbf25f2d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mnx0ibR8IF0L"
   },
   "outputs": [],
   "source": [
    "input_size = (320, 320)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "b0ffc5zyEyZJ"
   },
   "outputs": [],
   "source": [
    "#@title Paths definition\n",
    "data_folder = 'data/'\n",
    "model_folder= 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "falaRZoV3s8y"
   },
   "outputs": [],
   "source": [
    "#@title Function to load data from hdf5\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def import_from_hdf5(path, verbose=False):\n",
    "    with h5py.File(path, \"r\", driver='core') as hdf5_file:\n",
    "\n",
    "        train_data = None\n",
    "        batch_begin, batch_end = 0, 200\n",
    "        train_samples = len(hdf5_file['train_img'])\n",
    "        while True:\n",
    "            batch_end = min(batch_end, train_samples)\n",
    "            batch = hdf5_file['train_img'][batch_begin:batch_end, :, :, :]\n",
    "            if train_data is None:\n",
    "                train_data = batch\n",
    "            else:\n",
    "                train_data = np.concatenate([train_data, batch], axis=0)\n",
    "            if verbose:\n",
    "                print('Loaded', batch_end, 'samples of', train_samples, 'samples of train data')\n",
    "            if batch_end == train_samples:\n",
    "                break\n",
    "            else:\n",
    "                batch_begin = batch_end\n",
    "                batch_end += 200\n",
    "\n",
    "        train_labels = hdf5_file['train_labels'][:]\n",
    "\n",
    "        valid_data = None\n",
    "        batch_begin, batch_end = 0, 200\n",
    "        val_samples = len(hdf5_file['val_img'])\n",
    "        while True:\n",
    "            batch_end = min(batch_end, val_samples)\n",
    "            batch = hdf5_file['val_img'][batch_begin:batch_end, :, :, :]\n",
    "            if valid_data is None:\n",
    "                valid_data = batch\n",
    "            else:\n",
    "                valid_data = np.concatenate([valid_data, batch], axis=0)\n",
    "            if verbose:\n",
    "                print('Loaded', batch_end, 'samples of', val_samples, 'samples of val data')\n",
    "            if batch_end == val_samples:\n",
    "                break\n",
    "            else:\n",
    "                batch_begin = batch_end\n",
    "                batch_end += 200\n",
    "\n",
    "        valid_labels = hdf5_file['val_labels'][:]\n",
    "\n",
    "        train_data, valid_data = train_data[:, :, :, :], valid_data[:, :, :, :]\n",
    "\n",
    "    return train_data, train_labels, valid_data, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jrF6F8kWc9xW",
    "outputId": "5fba1a66-3d7e-425a-ed1c-09c1af78feab"
   },
   "outputs": [],
   "source": [
    "#@title Load hand data\n",
    "hand_train, hand_train_labels, hand_valid, hand_valid_labels = import_from_hdf5(data_folder + 'hand.hdf5')\n",
    "\n",
    "print(hand_train.shape, hand_train_labels.shape, hand_valid.shape, hand_valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "ETYR5qrf0V3O",
    "outputId": "14c1f3be-945b-46c7-875b-1388d6ff6394"
   },
   "outputs": [],
   "source": [
    "#@title Visualize some samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shift = 1520\n",
    "n_cols, n_rows = 4, 3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5 * n_cols, 5 * n_rows), ncols=n_cols, nrows=n_rows)\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for k in range(n_cols):\n",
    "        label = 'Normal' if hand_train_labels[shift + i * n_cols + k] == 0 else 'Abnormal'\n",
    "        ax[i, k].imshow(hand_train[shift + i * n_cols + k].astype(np.uint8))\n",
    "        ax[i, k].set_xlabel(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "orSjw4deI4xo",
    "outputId": "ab7db42f-d410-4089-8ec5-73befe4820e7"
   },
   "outputs": [],
   "source": [
    "#@title Model definition\n",
    "#Model definition\n",
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201, preprocess_input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "K.set_learning_phase(1) #?\n",
    "\n",
    "#Hack to add a regularization to a pre-trained model\n",
    "#https://sthalles.github.io/keras-regularizer/\n",
    "def add_regularization(model, regularizer=tf.keras.regularizers.l2(0.0001)):\n",
    "\n",
    "    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):\n",
    "        print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    # Save the weights before reloading the model.\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "    #tmp_weights_path = model_folder + 'tmp_weights.h5'\n",
    "    #model.save_weights(tmp_weights_path, overwrite=True)\n",
    "\n",
    "    # load the model from the config\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model_instance(lr=0.0001, version=121, pooling='avg', optimizer='sgd', beta1=0.9, beta2=0.999,\n",
    "                                regularizer=None, reg_lambda=0.01, label_smoothing=0.0):\n",
    "\n",
    "    def loss(y_true, y_pred): #Wrapping a tf loss to the keras interface with only 2 args\n",
    "        return binary_crossentropy(y_true, y_pred, label_smoothing=label_smoothing)\n",
    "    \n",
    "    pure_loss=loss #Will be passed as a metric to be watched as a \"pure\" (without regularization) loss\n",
    "\n",
    "    def get_lr_metric(optimizer): #Custom metric to monitor learning rate\n",
    "        def lr(y_true, y_pred):\n",
    "            return optimizer.lr\n",
    "        return lr\n",
    "    \n",
    "    if regularizer=='l1':\n",
    "        regularizer=regularizers.l1(reg_lambda)\n",
    "    elif regularizer=='l2':\n",
    "        regularizer=regularizers.l2(reg_lambda)\n",
    "    elif not regularizer:\n",
    "        pass\n",
    "    elif len(reg_lambda)==2:\n",
    "        l1, l2 = reg_lambda\n",
    "        regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    else:\n",
    "        raise VaelueError('Unknown regularizer')\n",
    "    \n",
    "    base_model = None\n",
    "    if version==121:\n",
    "        base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=None, pooling=pooling)\n",
    "    elif version==169:\n",
    "        base_model = DenseNet169(include_top=False, weights='imagenet', input_shape=None, pooling=pooling)\n",
    "    elif version==201:\n",
    "        base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=None, pooling=pooling)\n",
    "    else:\n",
    "        raise ValueError('Unknown DenseNet version')\n",
    "\n",
    "    for layer in base_model.layers: layer.trainable = True #Is this necessary?\n",
    "    \n",
    "    if regularizer:\n",
    "        base_model=add_regularization(base_model, regularizer=regularizer)\n",
    "\n",
    "    x = base_model.output\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    #predictions = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    if optimizer=='adam':\n",
    "        optimizer=Adam(lr=lr, beta_1=beta1, beta_2=beta2)\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer=SGD(lr=lr, momentum=beta1, nesterov=True)\n",
    "    else:\n",
    "        raise ValueError('Unknown optimizer')\n",
    "\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', lr_metric, pure_loss])\n",
    "    \n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "#get_compiled_model_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ZTtqaBldq_sh"
   },
   "outputs": [],
   "source": [
    "#@title Data generators definition\n",
    "#Data generators\n",
    "\n",
    "#Warning, RAM-eager!\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#train_data, val_data = preprocess_input(hand_train), preprocess_input(hand_valid)\n",
    "\n",
    "#train_datagen = ImageDataGenerator(\n",
    "#    horizontal_flip=True, #Augment\n",
    "#    vertical_flip=True,\n",
    "#    rotation_range=90,\n",
    "#    #width_shift_range=0.2,\n",
    "#    #height_shift_range=0.2\n",
    "#    )\n",
    "\n",
    "#valid_datagen = ImageDataGenerator()\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center = True, #Mean-shift\n",
    "    featurewise_std_normalization = True, #Standardization\n",
    "    horizontal_flip=True, #Augment\n",
    "    vertical_flip=True,\n",
    "    rotation_range=90,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2\n",
    "    )\n",
    "\n",
    "train_datagen.fit(hand_train)\n",
    "\n",
    "valid_datagen = ImageDataGenerator( #Data standardization, no augment\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization = True,\n",
    ")\n",
    "\n",
    "valid_datagen.fit(hand_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4fCg5O43H9b"
   },
   "outputs": [],
   "source": [
    "#Learning rate scheduler - reduce by factor 0.1 on plateau longer than 10 epochs\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_delta=0.001, verbose=1, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jB8-eR2J3T3w",
    "outputId": "090d870a-8a8f-457c-e4c0-df20cefa7f0c"
   },
   "outputs": [],
   "source": [
    "#@title Class weights computing\n",
    "#Class weights computing to penalize errors on more rare class\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(hand_train_labels), hand_train_labels)\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "F1Nm5VMs-fJH"
   },
   "outputs": [],
   "source": [
    "#@title Modular live loss plotter\n",
    "\n",
    "#Modular live loss plotter for Keras models\n",
    "#Allows to create custom layouts of per-batch or per-epoch plots for different metrics\n",
    "\n",
    "#Monitor class defines a plot, which either may be batch or epoch-scoped, and may contain several graphs\n",
    "#Batch monitor plots its values per batch, and refreshes itself on new epoch begin\n",
    "#Epoch monitor plots its values per epoch, and performs no refresh\n",
    "#All values/last N values displaying\n",
    "#Log-scale/Linear scale displaying\n",
    "\n",
    "#Plotter callback handles different Monitors and responds to the actual plotting\n",
    "#Defines a grid where Monitors will be drawn, grid size, refresh rate in batches\n",
    "#when the Monitors will be re-drawn in addition to per-epoch update\n",
    "#Plotter can be silenced to disable plotting and only archivate per-epoch data\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Monitor():\n",
    "    def __init__(self, scope='epoch', monitors= [ 'loss' ], plot_last=-1, log_scale=False, precision=4):\n",
    "        self.scope = scope.lower()\n",
    "        self.monitors = [ monitor.lower() for monitor in monitors ]\n",
    "        self.plot_last = max(0, plot_last)\n",
    "        self.x = []\n",
    "        self.ys = [ [] for monitor in monitors ]\n",
    "        self.log_scale = log_scale\n",
    "        self.precision = precision\n",
    "\n",
    "    def reinit(self):\n",
    "        self.x = []\n",
    "        self.ys = [ [] for monitor in self.monitors ]\n",
    "\n",
    "    def update(self, iteration, logs={}):\n",
    "        self.x.append(iteration)\n",
    "        [ self.ys[i].append(logs.get(monitor)) for i, monitor in enumerate(self.monitors) ]\n",
    "\n",
    "    def plot(self, axis):\n",
    "        x_data = self.x[ -self.plot_last : ]\n",
    "        y_array = [ y_data[ -self.plot_last : ] for y_data in self.ys ]\n",
    "\n",
    "        for i, y_data in enumerate(y_array):\n",
    "            label = self.monitors[i] + '_' + self.scope\n",
    "            if self.log_scale:\n",
    "                axis.set_yscale('log')\n",
    "            axis.plot(x_data, y_data, label=label)\n",
    "            if self.precision > 0 and len(x_data) > 0:\n",
    "                text = str(round(y_data[-1],  self.precision))\n",
    "                axis.text(x_data[-1], y_data[-1], text)\n",
    "            axis.legend()\n",
    "\n",
    "\n",
    "class Plotter(Callback):\n",
    "    def __init__(self, scale=5, n_cols=2, n_rows=1, monitors=[], refresh_rate=-1, silent=False):\n",
    "        if (n_cols * n_rows < len(monitors)):\n",
    "            raise ValueError('Grid is too small to fit all monitors!')\n",
    "\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.scale = scale\n",
    "\n",
    "        self.monitors = monitors\n",
    "\n",
    "        self.batch_monitors, self.epoch_monitors = [], []\n",
    "\n",
    "        for monitor in monitors:\n",
    "            if monitor.scope == 'epoch':\n",
    "                self.epoch_monitors.append(monitor)\n",
    "            elif monitor.scope == 'batch':\n",
    "                self.batch_monitors.append(monitor)\n",
    "\n",
    "        self.refresh_rate = refresh_rate\n",
    "        self.silent = False\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        [ monitor.reinit() for monitor in self.batch_monitors ]\n",
    "\n",
    "    def plot(self):\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        figsize = ( self.scale * self.n_cols, self.scale * self.n_rows)\n",
    "        fig, ax = plt.subplots(figsize=figsize, ncols=self.n_cols, nrows=self.n_rows)\n",
    "\n",
    "        for i in range(self.n_rows):\n",
    "            for k in range(self.n_cols):\n",
    "                index = i * self.n_cols + k\n",
    "                axis = ax[i, k]\n",
    "                self.monitors[index].plot(axis)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        [ monitor.update(batch, logs) for monitor in self.batch_monitors ]\n",
    "\n",
    "        if self.silent or batch == 0 or self.refresh_rate <= 0 or batch % self.refresh_rate != 0:\n",
    "            return\n",
    "\n",
    "        self.plot()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        [ monitor.update(epoch, logs) for monitor in self.epoch_monitors ]\n",
    "\n",
    "        if self.silent:\n",
    "            return\n",
    "\n",
    "        self.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-vcw28Sph2T"
   },
   "outputs": [],
   "source": [
    "monitors = [\n",
    "    Monitor(scope='batch', monitors = ['loss']),\n",
    "    Monitor(scope='epoch', monitors = ['loss', 'val_loss']),\n",
    "    Monitor(scope='epoch', monitors = ['loss', 'val_loss'], plot_last=16),\n",
    "    \n",
    "    #No-reg loss monitor\n",
    "    Monitor(scope='epoch', monitors = ['pure_loss', 'val_pure_loss']),\n",
    "    \n",
    "    Monitor(scope='batch', monitors = ['accuracy'], precision=3),\n",
    "    Monitor(scope='epoch', monitors = ['accuracy', 'val_accuracy'], precision=3),\n",
    "    Monitor(scope='epoch', monitors = ['lr'], log_scale=False, precision=7), #lr represents custom metric defined to watch the learning rate\n",
    "]\n",
    "\n",
    "plotter = Plotter(monitors=monitors, n_rows=2, n_cols=4, refresh_rate=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "CGduAZYRI3Fu"
   },
   "outputs": [],
   "source": [
    "#@title Epoch counter used to monitor epochs through multiple training launches\n",
    "\n",
    "class EpochCounter(Callback):\n",
    "    def __init__(self, initial_epoch=0):\n",
    "        self.epoch = initial_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch += 1\n",
    "\n",
    "epoch_counter = EpochCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xhUW515FtEh"
   },
   "outputs": [],
   "source": [
    "#TensorBoard as an advanced tool to watch the learning process\n",
    "\n",
    "#%load_ext tensorboard.notebook\n",
    "\n",
    "#from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
    "\n",
    "#tbc=TensorBoardColab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlVhUpXJm8q9"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint #Mandatory for a long-time training\n",
    "\n",
    "#Validation loss-based checkpointing\n",
    "#filepath=export_folder + \"dn-mura-pretrain-{epoch:03d}-{val_loss:.4f}-{lr:.7f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto', period=5)\n",
    "\n",
    "#Validation accuracy-based checkpointing\n",
    "filepath=model_folder + \"dn-mura-pretrain-{epoch:03d}-{val_loss:.4f}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "asTaLIQTZtc1"
   },
   "outputs": [],
   "source": [
    "#@title Restarting lr callback\n",
    "#Restarting LR with different slopes\n",
    "\n",
    "class RestartingLR(Callback):\n",
    "    def __init__(self, min_lr=0.0001, max_lr=0.001, warmup_t=0, decrease_t=50, warmup='linear', decrease='cosine',\n",
    "               warmup_enlengthen=1.0, decrease_enlengthen=1.0, cycles_demphing=1.0, epoch_shift=0,\n",
    "               checkpoint_on_cycle_end=False, path=None, weights_only=False, verbose=True):\n",
    "    \n",
    "        super(RestartingLR, self).__init__()\n",
    "        self.min_lr=min_lr\n",
    "        self.max_lr=max_lr\n",
    "        self.decrease_t=decrease_t\n",
    "        self.warmup_t=warmup_t\n",
    "\n",
    "        #LR value functions depending on the passed epoch number from last restart squashed to the [0, 1] interval\n",
    "        #Functions correspond to decrease, to compute increase with them use 1 - x instead\n",
    "        functions =  {\n",
    "            'linear' : lambda x, min_lr, max_lr : (min_lr - max_lr) * x + max_lr,\n",
    "            'cosine' : lambda x, min_lr, max_lr : (max_lr - min_lr) * (np.cos(x * np.pi) + 1) * 1/2 + min_lr,\n",
    "\n",
    "            #Linear descend of lr's exponents. If min_lr=1e-7 and max_lr=1e-3,\n",
    "            #lr will be 10^f(x), where f(x) is a linear function with values descending from -3 to -7 \n",
    "            'log' : lambda x, min_lr, max_lr: np.exp(np.log(min_lr/max_lr) * x + np.log(max_lr))\n",
    "            }\n",
    "\n",
    "        if warmup not in functions.keys(): raise ValueError('No such warmup function available')\n",
    "        if decrease not in functions.keys(): raise ValueError('No such decrease function available')\n",
    "\n",
    "        self.warmup=functions[warmup]\n",
    "        self.decrease=functions[decrease]\n",
    "\n",
    "        self.warmup_enlengthen=warmup_enlengthen\n",
    "        self.decrease_enlengthen=decrease_enlengthen\n",
    "        self.cycles_demphing=cycles_demphing\n",
    "\n",
    "        if epoch_shift < 0: raise ValueError('Epoch shift should be >= 0')\n",
    "\n",
    "        self.epoch_shift=epoch_shift\n",
    "\n",
    "        self.checkpoint=checkpoint_on_cycle_end\n",
    "        if path is None and checkpoint_on_cycle_end: raise ValueError('Empty path while checkpointing is on')\n",
    "\n",
    "        self.path=path\n",
    "        self.verbose=verbose\n",
    "        self.weights_only=weights_only\n",
    "\n",
    "    def lr_value(self, epoch=0):\n",
    "        epoch+=self.epoch_shift\n",
    "        cycles_passed, decrease_t, warmup_t = 0, self.decrease_t, self.warmup_t\n",
    "\n",
    "        cycle_epoch=epoch\n",
    "\n",
    "        while True:\n",
    "            if cycle_epoch - (decrease_t + warmup_t) >= 0:\n",
    "                cycle_epoch -= decrease_t + warmup_t\n",
    "                decrease_t *= self.decrease_enlengthen\n",
    "                warmup_t *= self.warmup_enlengthen\n",
    "                decrease_t, warmup_t = int(decrease_t), int(warmup_t)\n",
    "                cycles_passed += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        lr, min_lr, max_lr = None, self.min_lr, self.min_lr + (self.max_lr - self.min_lr) * (self.cycles_demphing ** cycles_passed)\n",
    "\n",
    "        if cycle_epoch < warmup_t:\n",
    "            arg = 1 - cycle_epoch / warmup_t\n",
    "            lr = self.warmup(arg, min_lr, max_lr)\n",
    "\n",
    "        elif cycle_epoch < warmup_t + decrease_t:\n",
    "            arg = (cycle_epoch - warmup_t) / decrease_t\n",
    "            lr = self.decrease(arg, min_lr, max_lr)\n",
    "\n",
    "        return lr\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        K.set_value(self.model.optimizer.lr, self.lr_value())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.checkpoint and (self.model.optimizer.lr - self.min_lr) < 1e-10:\n",
    "            if self.weights_only:\n",
    "                self.mode.save_weights(self.path, overwrite=True)\n",
    "            else:\n",
    "                self.model.save(self.path, overwrite=True)\n",
    "            if self.verbose:\n",
    "                print('Saved model on the cycle end at epoch', epoch)\n",
    "        lr = self.lr_value(epoch + 1)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def get_lr_values(self, epochs=100):\n",
    "        return [ self.lr_value(i) for i in range(epochs) ]\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "Yjsf5XDTYfwD",
    "outputId": "20155f3b-0d54-492f-beff-04cbe8ede79e"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = model_folder + \"dn-mura-pretrain-cycle-{epoch:03d}-{val_loss:.4f}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "clr = RestartingLR(min_lr=1e-5, max_lr=1e-4, warmup_t=0, decrease_t=50, warmup='cosine', decrease='cosine',\n",
    "                  warmup_enlengthen=1.0, decrease_enlengthen=1.0, cycles_demphing=1.0, epoch_shift=0,\n",
    "                  checkpoint_on_cycle_end=True, path=checkpoint_path)\n",
    "       \n",
    "\n",
    "iterations = 200\n",
    "plt.figure()\n",
    "#plt.yscale('log')\n",
    "plt.plot(range(iterations), clr.get_lr_values(iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQ69Ec8c-dnd"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "             checkpoint,\n",
    "             #reduce_lr,\n",
    "             clr,\n",
    "             plotter,\n",
    "             epoch_counter,\n",
    "             #TensorBoardColabCallback(tbc),\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "-jRLQACIyZ5r"
   },
   "outputs": [],
   "source": [
    "#@title Functions to load model from file\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_filename(path):\n",
    "    text = re.compile('[\\w]+-(\\d+)-([-+]?[0-9]*\\.?[0-9]+)-([-+]?[0-9]*\\.?[0-9]+).hdf5').findall(path)\n",
    "    if len(text) == 0:\n",
    "        return None\n",
    "    text = text[0]\n",
    "    return int(text[0]), float(text[1]), float(text[2])\n",
    "  \n",
    "def load_model_from_checkpoint(path, label_smoothing=0.0):\n",
    "  \n",
    "    def loss(y_true, y_pred): #Wrapping a tf loss to the keras interface with only 2 args\n",
    "        return binary_crossentropy(y_true, y_pred, label_smoothing=label_smoothing)\n",
    "\n",
    "    def get_lr_metric(optimizer): #Custom metric to monitor learning rate\n",
    "        def lr(y_true, y_pred):\n",
    "            return optimizer.lr\n",
    "        return lr\n",
    "\n",
    "    def placeholder_lr(x, y): #We need placeholder since lr metric definition requires model's optimizer\n",
    "        return 1.0\n",
    "\n",
    "    model = load_model(path, custom_objects={'loss' : loss, 'lr' : placeholder_lr})#Load the model with placeholder\n",
    "    lr = get_lr_metric(model.optimizer) #Define the correct lr metric\n",
    "    metrics = [ metric if metric.name != 'placeholder_lr' else lr for metric in model.metrics ] #Replace the placeholder with the true metric\n",
    "\n",
    "    model.compile(loss=model.loss, #Recompile model\n",
    "                optimizer=model.optimizer,\n",
    "                metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "#parse_filename('drive/My Drive/Work/ML/RA2/dn121-mura-pretrain-020-0.79-0.0001000.hdf5')\n",
    "\n",
    "#model = load_model_from_checkpoint('drive/My Drive/Work/ML/RA2/dn121-mura-pretrain-020-0.79-0.0001000.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "colab_type": "code",
    "id": "ysgOQ-euEudP",
    "outputId": "425bcfb7-54af-4687-e170-5d8d4add5363"
   },
   "outputs": [],
   "source": [
    "#Get newly compiled pre-trained model with small initial lr\n",
    "#to not disturb pre-trained weights too much\n",
    "hand_model = get_compiled_model_instance(lr=0.0001, regularizer='l2', reg_lambda=1e-5)\n",
    "hand_model.losses\n",
    "\n",
    "#Load previously saved model checkpoint\n",
    "#model_path = export_folder + 'dn121-mura-pretrain-020-0.79-0.0001000.hdf5'\n",
    "#hand_model = load_model_from_checkpoint(model_path)\n",
    "#epoch_counter.epoch = parse_filename(model_path)[0]\n",
    "\n",
    "#print('Model checkpoint epoch', epoch_counter.epoch)\n",
    "\n",
    "#Evaluate model to ensure that model was loaded and recompiled correctly\n",
    "#print('Model evaluation:', hand_model.evaluate_generator(valid_datagen.flow(hand_valid, hand_valid_labels, batch_size=len(hand_valid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sn7H_9MCFHbW"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "#!!! Workaround to avoid the FailedPreconditionError at the epoch end !!!\n",
    "\n",
    "#K.set_session(tf.Session(graph=hand_model.output.graph))\n",
    "#init = tf.global_variables_initializer()\n",
    "#K.get_session().run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLu37kFbtaLt"
   },
   "outputs": [],
   "source": [
    "#hand_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "nnquurdo4S8G",
    "outputId": "1777556d-dcc8-4803-9d8d-c9e47f7286d8"
   },
   "outputs": [],
   "source": [
    "#Launch primary model training\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "val_batch_size=10\n",
    "validation_steps= len(hand_valid_labels) // val_batch_size #Validate on full set\n",
    "\n",
    "epochs = 50 + epoch_counter.epoch #For multiple launches\n",
    "\n",
    "hand_model_history = hand_model.fit_generator(\n",
    "    train_datagen.flow(hand_train, hand_train_labels, batch_size=batch_size, shuffle=True),\n",
    "    validation_data=valid_datagen.flow(hand_valid, hand_valid_labels, batch_size=val_batch_size, shuffle=True), #Validation set is used only at the epoch end\n",
    "    validation_steps=validation_steps,\n",
    "    #train_datagen.flow(train_data, hand_train_labels, batch_size=batch_size, shuffle=True),\n",
    "    #validation_data=valid_datagen.flow(val_data, hand_valid_labels, batch_size=val_batch_size, shuffle=True),\n",
    "    class_weight=class_weight,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    initial_epoch=epoch_counter.epoch,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "histories[epoch_counter.epoch]=hand_model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IQoBcrGYYVN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ra2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
