@BOOK{machine_learning,
	title = "Machine Learning",
	author = "Tom Mitchell",
	isbn = "0070428077",
	publisher = "McGraw Hill",
	year = "1997",
	url = "http://www.cs.cmu.edu/~tom/mlbook.html"
}

@BOOK{pattern_recognition_and_ml,
	title = "Pattern Recognition and Machine Learning",
	author = "C. M. Bishop",
	isbn = "978-0-387-31073-2",
	publisher = "Springer",
	year = "2006"
}

@BOOK{ml_foundations,
	title = "Foundations of Machine Learning",
	author = "Mehryar Mohri and Afshin Rostamizadeh and Ameet Talwalkar",
	isbn = "9780262018258",
	publisher = "MIT Press",
	year = "2012"
}

@MISC{titanic,
	title = "Titanic: Machine Learning from Disaster",
	howpublished = "Available at \url{https://www.kaggle.com/c/titanic/data}"
}

@article{trees,
	author = {Hssina, Badr and MERBOUHA, Abdelkarim and Ezzikouri, Hanane and Erritali, Mohammed},
	year = {2014},
	month = {07},
	pages = {},
	title = {A comparative study of decision tree ID3 and C4.5},
	volume = {Special Issue on Advances in Vehicular Ad Hoc Networking and Applications},
	journal = {(IJACSA) International Journal of Advanced Computer Science and Applications},
	doi = {10.14569/SpecialIssue.2014.040203}
}

@article{theoretical_ml,
	author = "Pranjal Awasthi and Neelesh Kumar",
	title = "Theoretical Machine Learning",
	url = "https://www.cs.rutgers.edu/~pa336/mlt_f17/lec-1.pdf",
	year="2017"
}

@article{overfitting,
	author = {Hawkins, Douglas},
	year = {2004},
	month = {05},
	pages = {1-12},
	title = {The Problem of Overfitting},
	volume = {44},
	journal = {Journal of chemical information and computer sciences},
	doi = {10.1021/ci0342472}
}

@misc{simonyan2013deep,
	title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
	author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
	year={2013},
	eprint={1312.6034},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{fan2020interpretability,
	title={On Interpretability of Artificial Neural Networks},
	author={Fenglei Fan and Jinjun Xiong and Ge Wang},
	year={2020},
	eprint={2001.02522},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{LeCunBoserDenkerEtAl89,
	added-at = {2008-09-16T23:39:07.000+0200},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	biburl = {https://www.bibsonomy.org/bibtex/296c98d6adbcfbbea71385179ed056128/brian.mingus},
	description = {CCNLab BibTeX},
	interhash = {f532aea0ac3d409fbcae2c9ce8d5d1a2},
	intrahash = {96c98d6adbcfbbea71385179ed056128},
	journal = {Neural Computation},
	keywords = {nnets},
	pages = {541-551},
	timestamp = {2008-09-16T23:40:31.000+0200},
	title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	volume = 1,
	year = 1989
}

@article{McCulloch43,
	author = {W. S. McCulloch and W. Pitts},
	journal = {Bulletin of Mathematical Biophysics},
	pages = {115-133},
	title = {A Logical Calculus of the Idea Immanent in Nervous Activity},
	volume = {5},
	year = {1943},
} 

@book{hebb-organization-of-behavior-1949,
	abstract = {{Donald Hebb pioneered many current themes in
	behavioural neuroscience. He saw psychology as a
	biological science, but one in which the organization
	of behaviour must remain the central concern. Through
	penetrating theoretical concepts, including the "cell
	assembly," "phase sequence," and "Hebb synapse," he
	offered a way to bridge the gap between cells, circuits
	and behaviour. He saw the brain as a dynamically
	organized system of multiple distributed parts, with
	roots that extend into foundations of development and
	evolutionary heritage. He understood that behaviour, as
	brain, can be sliced at various levels and that one of
	our challenges is to bring these levels into both
	conceptual and empirical register. He could move
	between theory and fact with an ease that continues to
	inspire both students and professional investigators.
	Although facts continue to accumulate at an
	accelerating rate in both psychology and neuroscience,
	and although these facts continue to force revision in
	the details of Hebb's earlier contributions, his
	overall insistence that we look at behaviour and brain
	together â within a dynamic, relational and
	multilayered framework â remains. His work touches
	upon current studies of population coding, contextual
	factors in brain representations, synaptic plasticity,
	developmental construction of brain/behaviour
	relations, clinical syndromes, deterioration of
	performance with age and disease, and the formal
	construction of connectionist models. The collection of
	papers in this volume represent these and related
	themes that Hebb inspired. We also acknowledge our
	appreciation for Don Hebb as teacher, colleague and
	friend.}},
	added-at = {2011-06-02T00:21:57.000+0200},
	address = {New York},
	author = {Hebb, Donald O.},
	biburl = {https://www.bibsonomy.org/bibtex/26432ae617e6db0127c8b197bf760d99e/mhwombat},
	citeulike-article-id = {500649},
	citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0805843000},
	citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0805843000},
	citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0805843000},
	citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0805843000},
	citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0805843000/citeulike00-21},
	citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0805843000},
	citeulike-linkout-6 = {http://www.worldcat.org/isbn/0805843000},
	citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0805843000},
	citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0805843000\&index=books\&linkCode=qs},
	citeulike-linkout-9 = {http://www.librarything.com/isbn/0805843000},
	day = 15,
	file = {:neural_nets/Hebb 1949.pdf:PDF},
	groups = {public},
	howpublished = {Hardcover},
	interhash = {ba8f8b92a0de2c83bdbcc9d742235a59},
	intrahash = {6432ae617e6db0127c8b197bf760d99e},
	isbn = {0-8058-4300-0},
	keywords = {MSc checked network neural seminal},
	month = jun,
	posted-at = {2006-02-10 16:35:34},
	priority = {2},
	publisher = {Wiley},
	timestamp = {2016-07-12T19:25:30.000+0200},
	title = {The organization of behavior: {A} neuropsychological
	theory},
	username = {mhwombat},
	year = 1949
}

@article{cs231n,
	title= {CS231n: Convolutional Neural Networks for Visual Recognition 2016},
	keywords= {},
	journal= {},
	author= {Fei-Fei Li and Andrej Karpathy and Justin Johnson},
	year= {},
	url= {http://cs231n.stanford.edu/},
	license= {},
	abstract= {Course Description
	Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge.},
	superseded= {http://academictorrents.com/details/ed8a16ebb346e14119a03371665306609e485f13},
	terms= {}
}

@misc{nn_zoo,
	title = {Neural Network Zoo},
	author = "Fjodor van Veen",
	howpublished = {\url{https://www.asimovinstitute.org/neural-network-zoo/}},
	note = {Accessed: 28.04.2020}
}

@misc{nn-svg,
	title = {NN-SVG},
	author = {Alex Lenail},
	howpublished = {\url{http://alexlenail.me/NN-SVG/index.html}},
	note = {Accessed: 28.04.2020}
}

@misc{universal_approx_wiki,
	author = "{Wikipedia contributors}",
	title = "Universal approximation theorem --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2020",
	url = "https://en.wikipedia.org/w/index.php?title=Universal_approximation_theorem&oldid=945920534",
	note = "[Online; accessed 
	
	28-April-2020
	]"
}

@book{nndl_universal_approx,
	author="Michael A. Nielsen",
	title = "Neural Networks and Deep Learning",
	publisher = "Determination Press",
	year="2015",
	url = "http://neuralnetworksanddeeplearning.com",
}

@misc{softmax,
	author = "{Wikipedia contributors}",
	title = "Softmax function --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2020",
	howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=951108453}",
	note = "[Online; accessed 
	
	28-April-2020
	]"
}

@misc{chain_rule,
	author = "{Wikipedia contributors}",
	title = "Chain rule --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2020",
	url = "https://en.wikipedia.org/w/index.php?title=Chain_rule&oldid=950737064",
	note = "[Online; accessed 
	
	29-April-2020
	]"
}

@inproceedings{nn_saturation,
	author = {Bosman, Anna and Engelbrecht, Andries},
	year = {2015},
	month = {12},
	pages = {},
	title = {Measuring Saturation in Neural Networks},
	doi = {10.1109/SSCI.2015.202}
}

@misc{wiki_relu,
	author = "{Wikipedia contributors}",
	title = "Rectifier (neural networks) --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2020",
	url = "https://en.wikipedia.org/w/index.php?title=Rectifier_(neural_networks)&oldid=952377033",
	note = "[Online; accessed 
	
	29-April-2020
	]"
}

@article{transfusion,
	author    = {Maithra Raghu and
	Chiyuan Zhang and
	Jon M. Kleinberg and
	Samy Bengio},
	title     = {Transfusion: Understanding Transfer Learning with Applications to
	Medical Imaging},
	journal   = {CoRR},
	volume    = {abs/1902.07208},
	year      = {2019},
	url       = {http://arxiv.org/abs/1902.07208},
	archivePrefix = {arXiv},
	eprint    = {1902.07208},
	timestamp = {Tue, 21 May 2019 18:03:36 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1902-07208.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{how_transferable,
	author    = {Jason Yosinski and
	Jeff Clune and
	Yoshua Bengio and
	Hod Lipson},
	title     = {How transferable are features in deep neural networks?},
	journal   = {CoRR},
	volume    = {abs/1411.1792},
	year      = {2014},
	url       = {http://arxiv.org/abs/1411.1792},
	archivePrefix = {arXiv},
	eprint    = {1411.1792},
	timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/YosinskiCBL14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{unet,
	author    = {Olaf Ronneberger and
	Philipp Fischer and
	Thomas Brox},
	title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	journal   = {CoRR},
	volume    = {abs/1505.04597},
	year      = {2015},
	url       = {http://arxiv.org/abs/1505.04597},
	archivePrefix = {arXiv},
	eprint    = {1505.04597},
	timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/RonnebergerFB15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{alexnet,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{autodiff,
	author    = {Atilim Gunes Baydin and
	Barak A. Pearlmutter and
	Alexey Andreyevich Radul},
	title     = {Automatic differentiation in machine learning: a survey},
	journal   = {CoRR},
	volume    = {abs/1502.05767},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.05767},
	archivePrefix = {arXiv},
	eprint    = {1502.05767},
	timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/BaydinPR15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{zfnet,
	author    = {Matthew D. Zeiler and
	Rob Fergus},
	title     = {Visualizing and Understanding Convolutional Networks},
	journal   = {CoRR},
	volume    = {abs/1311.2901},
	year      = {2013},
	url       = {http://arxiv.org/abs/1311.2901},
	archivePrefix = {arXiv},
	eprint    = {1311.2901},
	timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/ZeilerF13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@techreport{lecun_generalization,
	title = "Generalization and network design strategies",
	author = "Yann Lecun",
	year = "1989",
	institution = "Department of Computer Science, University of Toronto"
}

@article{dropout,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	number  = {56},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{vgg,
	abstract = {In this work we investigate the effect of the convolutional network depth on
	its accuracy in the large-scale image recognition setting. Our main
	contribution is a thorough evaluation of networks of increasing depth using an
	architecture with very small (3x3) convolution filters, which shows that a
	significant improvement on the prior-art configurations can be achieved by
	pushing the depth to 16-19 weight layers. These findings were the basis of our
	ImageNet Challenge 2014 submission, where our team secured the first and the
	second places in the localisation and classification tracks respectively. We
	also show that our representations generalise well to other datasets, where
	they achieve state-of-the-art results. We have made our two best-performing
	ConvNet models publicly available to facilitate further research on the use of
	deep visual representations in computer vision.},
	added-at = {2020-04-23T13:45:34.000+0200},
	author = {Simonyan, Karen and Zisserman, Andrew},
	biburl = {https://www.bibsonomy.org/bibtex/2739e5899ff222c852fb3a97a0ae983a6/mo_xime},
	description = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	interhash = {4e6fa56cb7cf99400d5701543ee228de},
	intrahash = {739e5899ff222c852fb3a97a0ae983a6},
	keywords = {deep-learning},
	note = {cite arxiv:1409.1556},
	timestamp = {2020-04-23T13:45:34.000+0200},
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	year = 2014
}

@article{hubel1962receptive,
	added-at = {2009-04-01T22:42:24.000+0200},
	author = {Hubel, D. and Wiesel, T.},
	bdsk-file-1 = {YnBsaXN0MDDUAQIDBAUGCQpYJHZlcnNpb25UJHRvcFkkYXJjaGl2ZXJYJG9iamVjdHMSAAGGoNEHCFRyb290gAFfEA9OU0tleWVkQXJjaGl2ZXKoCwwXGBkdJCVVJG51bGzTDQ4PEBEUViRjbGFzc1dOUy5rZXlzWk5TLm9iamVjdHOAB6ISE4ACgAOiFRaABIAGWWFsaWFzRGF0YVxyZWxhdGl2ZVBhdGjSDRobHFdOUy5kYXRhgAVPEQGWAAAAAAGWAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADE34cZSCsAAACS/1wNaHViZWwxOTYyLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJL/2sE0QtMAAAAAAAAAAAABAAMAAAkgAAAAAAAAAAAAAAAAAAAABlBhcGVycwAQAAgAAMTfv1kAAAARAAgAAME0exMAAAABABQAkv9cAJ4LpwAUeS0AFHkWAAB7XAACAD5NYWNpbnRvc2ggSEQ6VXNlcnM6YW5keTpEb2N1bWVudHM6YmliZGVzazpQYXBlcnM6aHViZWwxOTYyLnBkZgAOABwADQBoAHUAYgBlAGwAMQA5ADYAMgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAMVVzZXJzL2FuZHkvRG9jdW1lbnRzL2JpYmRlc2svUGFwZXJzL2h1YmVsMTk2Mi5wZGYAABMAAS8AABUAAgAL//8AANIeHyAhWCRjbGFzc2VzWiRjbGFzc25hbWWjISIjXU5TTXV0YWJsZURhdGFWTlNEYXRhWE5TT2JqZWN0XxAUUGFwZXJzL2h1YmVsMTk2Mi5wZGbSHh8mJ6InI1xOU0RpY3Rpb25hcnkACAARABoAHwApADIANwA6AD8AQQBTAFwAYgBpAHAAeACDAIUAiACKAIwAjwCRAJMAnQCqAK8AtwC5AlMCWAJhAmwCcAJ+AoUCjgKlAqoCrQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAK6},
	biburl = {https://www.bibsonomy.org/bibtex/227b441702a492b35fd0ea29dc4d8117d/sourcefilter},
	date-added = {2006-04-28 11:34:57 -0400},
	date-modified = {2009-03-26 17:06:30 -0400},
	interhash = {5a9f3d4ed4fcd16426c011fd359887d9},
	intrahash = {27b441702a492b35fd0ea29dc4d8117d},
	journal = {Journal of Physiology},
	keywords = {imported},
	local-url = {file://localhost/Volumes/iDisk/Documents/bibdesk/Papers/hubel1962.pdf},
	pages = {106--154},
	timestamp = {2009-04-01T22:43:24.000+0200},
	title = {Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex},
	volume = 160,
	year = 1962
}

@article{resnet,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	journal   = {CoRR},
	volume    = {abs/1512.03385},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.03385},
	archivePrefix = {arXiv},
	eprint    = {1512.03385},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{going_deeper,
	author    = {Christian Szegedy and
	Wei Liu and
	Yangqing Jia and
	Pierre Sermanet and
	Scott E. Reed and
	Dragomir Anguelov and
	Dumitru Erhan and
	Vincent Vanhoucke and
	Andrew Rabinovich},
	title     = {Going Deeper with Convolutions},
	journal   = {CoRR},
	volume    = {abs/1409.4842},
	year      = {2014},
	url       = {http://arxiv.org/abs/1409.4842},
	archivePrefix = {arXiv},
	eprint    = {1409.4842},
	timestamp = {Mon, 13 Aug 2018 16:48:52 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SzegedyLJSRAEVR14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{densenet,
	author    = {Gao Huang and
	Zhuang Liu and
	Kilian Q. Weinberger},
	title     = {Densely Connected Convolutional Networks},
	journal   = {CoRR},
	volume    = {abs/1608.06993},
	year      = {2016},
	url       = {http://arxiv.org/abs/1608.06993},
	archivePrefix = {arXiv},
	eprint    = {1608.06993},
	timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{highway_net,
	author    = {Rupesh Kumar Srivastava and
	Klaus Greff and
	J{\"{u}}rgen Schmidhuber},
	title     = {Training Very Deep Networks},
	journal   = {CoRR},
	volume    = {abs/1507.06228},
	year      = {2015},
	url       = {http://arxiv.org/abs/1507.06228},
	archivePrefix = {arXiv},
	eprint    = {1507.06228},
	timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SrivastavaGS15a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{stochastic_depth,
	author    = {Gao Huang and
	Yu Sun and
	Zhuang Liu and
	Daniel Sedra and
	Kilian Q. Weinberger},
	title     = {Deep Networks with Stochastic Depth},
	journal   = {CoRR},
	volume    = {abs/1603.09382},
	year      = {2016},
	url       = {http://arxiv.org/abs/1603.09382},
	archivePrefix = {arXiv},
	eprint    = {1603.09382},
	timestamp = {Sat, 15 Dec 2018 13:25:43 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/HuangSLSW16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{fractal_net,
	author    = {Gustav Larsson and
	Michael Maire and
	Gregory Shakhnarovich},
	title     = {FractalNet: Ultra-Deep Neural Networks without Residuals},
	journal   = {CoRR},
	volume    = {abs/1605.07648},
	year      = {2016},
	url       = {http://arxiv.org/abs/1605.07648},
	archivePrefix = {arXiv},
	eprint    = {1605.07648},
	timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LarssonMS16a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rectifier,
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Y.},
	year = {2010},
	month = {01},
	pages = {},
	title = {Deep Sparse Rectifier Neural Networks},
	volume = {15},
	journal = {Journal of Machine Learning Research}
}

@article{identity_mappings,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Identity Mappings in Deep Residual Networks},
	journal   = {CoRR},
	volume    = {abs/1603.05027},
	year      = {2016},
	url       = {http://arxiv.org/abs/1603.05027},
	archivePrefix = {arXiv},
	eprint    = {1603.05027},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZR016.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{covariate_shift,
	author = {Shimodaira, Hidetoshi},
	year = {2000},
	month = {10},
	pages = {227-244},
	title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
	volume = {90},
	journal = {Journal of Statistical Planning and Inference},
	doi = {10.1016/S0378-3758(00)00115-4}
}



@article{batchnorm,
	author    = {Sergey Ioffe and
	Christian Szegedy},
	title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
	Internal Covariate Shift},
	journal   = {CoRR},
	volume    = {abs/1502.03167},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.03167},
	archivePrefix = {arXiv},
	eprint    = {1502.03167},
	timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{he_init,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
	ImageNet Classification},
	journal   = {CoRR},
	volume    = {abs/1502.01852},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.01852},
	archivePrefix = {arXiv},
	eprint    = {1502.01852},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZR015.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{efficient_backprop,
	added-at = {2017-05-16T00:00:00.000+0200},
	author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
	biburl = {https://www.bibsonomy.org/bibtex/2bdb95f0c55c51543f38f485288a0e4f7/dblp},
	booktitle = {Neural Networks: Tricks of the Trade (2nd ed.)},
	editor = {Montavon, Grégoire and Orr, Genevieve B. and Müller, Klaus-Robert},
	ee = {https://doi.org/10.1007/978-3-642-35289-8_3},
	interhash = {cc0d55e40d1d2e9b95b9308e79d6708c},
	intrahash = {bdb95f0c55c51543f38f485288a0e4f7},
	isbn = {978-3-642-35288-1},
	keywords = {dblp},
	pages = {9-48},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {2019-09-26T12:30:49.000+0200},
	title = {Efficient BackProp.},
	volume = 7700,
	year = 2012
}



@article{exploding_gradient,
	author    = {George Philipp and
	Dawn Song and
	Jaime G. Carbonell},
	title     = {Gradients explode - Deep Networks are shallow - ResNet explained},
	journal   = {CoRR},
	volume    = {abs/1712.05577},
	year      = {2017},
	url       = {http://arxiv.org/abs/1712.05577},
	archivePrefix = {arXiv},
	eprint    = {1712.05577},
	timestamp = {Mon, 13 Aug 2018 16:46:32 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1712-05577.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{xavier_init,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{rcnn,
	author    = {Ross B. Girshick and
	Jeff Donahue and
	Trevor Darrell and
	Jitendra Malik},
	title     = {Rich feature hierarchies for accurate object detection and semantic
	segmentation},
	journal   = {CoRR},
	volume    = {abs/1311.2524},
	year      = {2013},
	url       = {http://arxiv.org/abs/1311.2524},
	archivePrefix = {arXiv},
	eprint    = {1311.2524},
	timestamp = {Mon, 13 Aug 2018 16:48:09 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/GirshickDDM13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{selective_search,
	author = {J.R.R. Uijlings and K.E.A. van de Sande and T. Gevers and A.W.M.
	Smeulders},
	title = {Selective Search for Object Recognition},
	journal = {International Journal of Computer Vision},
	year = {2013},
	doi = {10.1007/s11263-013-0620-5},
	owner = {jrruijli},
	timestamp = {2013.02.06},
	url = {http://www.huppelen.nl/publications/selectiveSearchDraft.pdf}
}

@inproceedings{svm,
	author = {Evgeniou, Theodoros and Pontil, Massimiliano},
	year = {2001},
	month = {01},
	pages = {249-257},
	title = {Support Vector Machines: Theory and Applications},
	volume = {2049},
	doi = {10.1007/3-540-44673-7_12}
}

@article{ridge,
	added-at = {2009-01-29T03:39:09.000+0100},
	author = {Hoerl, A. E. and Kennard, R. W.},
	biburl = {https://www.bibsonomy.org/bibtex/2257711fd5e6222fce40b8fbcb66a906a/swpark81},
	interhash = {6468748d5b4b1f5d02107b4c131e9ea6},
	intrahash = {257711fd5e6222fce40b8fbcb66a906a},
	journal = {Technometrics},
	keywords = {ridge-regg},
	pages = {55--67},
	timestamp = {2009-01-29T03:40:48.000+0100},
	title = {Ridge Regression: Biased
	Estimation for Nonorthogonal Problems},
	volume = 12,
	year = 1970
}



@article{fast_rcnn,
	author    = {Ross B. Girshick},
	title     = {Fast {R-CNN}},
	journal   = {CoRR},
	volume    = {abs/1504.08083},
	year      = {2015},
	url       = {http://arxiv.org/abs/1504.08083},
	archivePrefix = {arXiv},
	eprint    = {1504.08083},
	timestamp = {Mon, 13 Aug 2018 16:49:11 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Girshick15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{faster_rcnn,
	author    = {Shaoqing Ren and
	Kaiming He and
	Ross B. Girshick and
	Jian Sun},
	title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
	Networks},
	journal   = {CoRR},
	volume    = {abs/1506.01497},
	year      = {2015},
	url       = {http://arxiv.org/abs/1506.01497},
	archivePrefix = {arXiv},
	eprint    = {1506.01497},
	timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/RenHG015.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{mask_rcnn,
	author    = {Kaiming He and
	Georgia Gkioxari and
	Piotr Doll{\'{a}}r and
	Ross B. Girshick},
	title     = {Mask {R-CNN}},
	journal   = {CoRR},
	volume    = {abs/1703.06870},
	year      = {2017},
	url       = {http://arxiv.org/abs/1703.06870},
	archivePrefix = {arXiv},
	eprint    = {1703.06870},
	timestamp = {Mon, 13 Aug 2018 16:46:36 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeGDG17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{momentum,
	added-at = {2018-11-14T00:00:00.000+0100},
	author = {Qian, Ning},
	biburl = {https://www.bibsonomy.org/bibtex/25467c3fc1e5a8200fc01310208258c53/dblp},
	ee = {https://www.wikidata.org/entity/Q52019658},
	interhash = {2b93b2cc86fc9b2dc20e2e367344acb4},
	intrahash = {5467c3fc1e5a8200fc01310208258c53},
	journal = {Neural Networks},
	keywords = {dblp},
	number = 1,
	pages = {145-151},
	timestamp = {2018-11-15T14:32:05.000+0100},
	title = {On the momentum term in gradient descent learning algorithms.},
	volume = 12,
	year = 1999
}



@article{specialized_dropouts,
	author    = {Kun Wan and
	Boyuan Feng and
	Lingwei Xie and
	Yufei Ding},
	title     = {Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized
	Dropout},
	journal   = {CoRR},
	volume    = {abs/1810.00091},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.00091},
	archivePrefix = {arXiv},
	eprint    = {1810.00091},
	timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-00091.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{learning_representations,
	added-at = {2019-05-21T10:10:49.000+0200},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	biburl = {https://www.bibsonomy.org/bibtex/2a392597c4f9cff2cd3c96c2191fa1eb6/sxkdz},
	doi = {10.1038/323533a0},
	interhash = {c354bc293fa9aa7caffc66d40a014903},
	intrahash = {a392597c4f9cff2cd3c96c2191fa1eb6},
	journal = {Nature},
	keywords = {imported},
	number = 6088,
	pages = {533--536},
	timestamp = {2019-05-21T10:10:49.000+0200},
	title = {{Learning Representations by Back-propagating Errors}},
	url = {http://www.nature.com/articles/323533a0},
	volume = 323,
	year = 1986
}

@article{nesterov,
	author="Nesterov, Y. E.",
	title="A method for solving the convex programming problem with convergence rate $O(1/k^2)$",
	journal="Dokl. Akad. Nauk SSSR",
	ISSN="",
	publisher="",
	year="1983",
	month="",
	volume="269",
	number="",
	pages="472-547",
	URL="https://ci.nii.ac.jp/naid/10029946121/en/",
	DOI="",
}

@article{adagrad,
	added-at = {2017-12-30T09:55:39.000+0100},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	biburl = {https://www.bibsonomy.org/bibtex/2f548fa2a3cb4881b968ec3a06dd8c49c/leerooy},
	interhash = {d2bb1dcfcc9549a93e0b1f0dd8d23cf9},
	intrahash = {f548fa2a3cb4881b968ec3a06dd8c49c},
	journal = {Journal of Machine Learning Research},
	keywords = {},
	number = {Jul},
	pages = {2121--2159},
	timestamp = {2017-12-30T09:55:39.000+0100},
	title = {Adaptive subgradient methods for online learning and stochastic optimization},
	volume = 12,
	year = 2011
}

@misc{rmsprop,
	title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
	author={Tieleman, T. and Hinton, G.},
	howpublished={COURSERA: Neural Networks for Machine Learning},
	year={2012}
}

@misc{adam,
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
	of stochastic objective functions, based on adaptive estimates of lower-order
	moments. The method is straightforward to implement, is computationally
	efficient, has little memory requirements, is invariant to diagonal rescaling
	of the gradients, and is well suited for problems that are large in terms of
	data and/or parameters. The method is also appropriate for non-stationary
	objectives and problems with very noisy and/or sparse gradients. The
	hyper-parameters have intuitive interpretations and typically require little
	tuning. Some connections to related algorithms, on which Adam was inspired, are
	discussed. We also analyze the theoretical convergence properties of the
	algorithm and provide a regret bound on the convergence rate that is comparable
	to the best known results under the online convex optimization framework.
	Empirical results demonstrate that Adam works well in practice and compares
	favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
	a variant of Adam based on the infinity norm.},
	added-at = {2020-01-17T03:14:27.000+0100},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	biburl = {https://www.bibsonomy.org/bibtex/2d53bcfff0fe1a1d3a4a171352ee6e92c/simon_diener},
	description = {An upgrade over the standard stochastic gradient descend as it is able to apply changes to the learning rate by itself to be able to escape local maxima etc. This methods was used for the dynamic explainable recommender framework by Chen et al.},
	interhash = {57d2ac873f398f21bb94790081e80394},
	intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
	keywords = {final thema:neural_attentional_rating_regression},
	note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
	timestamp = {2020-01-17T03:14:27.000+0100},
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	year = 2014
}

@inproceedings{sparse_rectifier,
	added-at = {2014-04-01T20:16:10.000+0200},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	biburl = {https://www.bibsonomy.org/bibtex/256f5ffd25378f109c8cc14394bcfdabb/prlz77},
	booktitle = {AISTATS},
	editor = {Gordon, Geoffrey J. and Dunson, David B. and Dudík, Miroslav},
	ee = {http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf},
	interhash = {fbf04ef5079b11118f3f3184b1068d88},
	intrahash = {56f5ffd25378f109c8cc14394bcfdabb},
	keywords = {Bengio Deep Networks Neural Rectifier Relu Sparse},
	pages = {315-323},
	publisher = {JMLR.org},
	series = {JMLR Proceedings},
	timestamp = {2014-04-01T20:16:10.000+0200},
	title = {Deep Sparse Rectifier Neural Networks.},
	url = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
	volume = 15,
	year = 2011
}

@MISC{splash_of_color,
	author =       {Waleed Abdulla},
	title =        {Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow},
	editor =       {Medium.com},
	month =        {March},
	year =         {2018},
	url = {https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46},
	note =         {[Online; posted 20-March-2018]},
}

@misc{bilinear_interpolation,
	author = "{Wikipedia contributors}",
	title = "Bilinear interpolation --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2020",
	howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Bilinear_interpolation&oldid=957374633}",
	note = "[Online; accessed 1-June-2020]"
}


@article{feature_pyramid_nets,
	author    = {Tsung{-}Yi Lin and
	Piotr Doll{\'{a}}r and
	Ross B. Girshick and
	Kaiming He and
	Bharath Hariharan and
	Serge J. Belongie},
	title     = {Feature Pyramid Networks for Object Detection},
	journal   = {CoRR},
	volume    = {abs/1612.03144},
	year      = {2016},
	url       = {http://arxiv.org/abs/1612.03144},
	archivePrefix = {arXiv},
	eprint    = {1612.03144},
	timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LinDGHHB16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ra_ultrasound,
	author    = {Andersen J.K.H. and Pedersen J.S. and Laursen M.S.},
	title     = {Neural networks for automatic scoring of arthritis disease activity on ultrasound images},
	journal   = {RMD Open},
	year      = {2019},
	url       = {https://rmdopen.bmj.com/content/rmdopen/5/1/e000891.full.pdf},
	month     = {05},
	doi       = {doi:10.1136/rmdopen-2018-000891}
}

@inproceedings{ra_computer_aided,
	author = {Morita, Kento and Tashita, Atsuki and Nii, Manabu and Kobashi, Syoji},
	year = {2017},
	month = {07},
	pages = {357-360},
	title = {Computer-aided diagnosis system for Rheumatoid Arthritis using machine learning},
	doi = {10.1109/ICMLC.2017.8108947}
}

@article{dl_ra,
	author = {Hirano, Toru and Nishide, Masayuki and Nonaka, Naoki and Seita, Jun and Ebina, Kosuke and Sakurada, Kazuhiro and Kumanogoh, Atsushi},
	title = "{Development and validation of a deep-learning model for scoring of radiographic finger joint destruction in rheumatoid arthritis}",
	journal = {Rheumatology Advances in Practice},
	volume = {3},
	number = {2},
	year = {2019},
	month = {11},
	abstract = "{The purpose of this research was to develop a deep-learning model to assess radiographic finger joint destruction in RA.The model comprises two steps: a joint-detection step and a joint-evaluation step. Among 216 radiographs of 108 patients with RA, 186 radiographs were assigned to the training/validation dataset and 30 to the test dataset. In the training/validation dataset, images of PIP joints, the IP joint of the thumb or MCP joints were manually clipped and scored for joint space narrowing (JSN) and bone erosion by clinicians, and then these images were augmented. As a result, 11 160 images were used to train and validate a deep convolutional neural network for joint evaluation. Three thousand seven hundred and twenty selected images were used to train machine learning for joint detection. These steps were combined as the assessment model for radiographic finger joint destruction. Performance of the model was examined using the test dataset, which was not included in the training/validation process, by comparing the scores assigned by the model and clinicians.The model detected PIP joints, the IP joint of the thumb and MCP joints with a sensitivity of 95.3\\% and assigned scores for JSN and erosion. Accuracy (percentage of exact agreement) reached 49.3–65.4\\% for JSN and 70.6–74.1\\% for erosion. The correlation coefficient between scores by the model and clinicians per image was 0.72–0.88 for JSN and 0.54–0.75 for erosion.Image processing with the trained convolutional neural network model is promising to assess radiographs in RA.}",
	issn = {2514-1775},
	doi = {10.1093/rap/rkz047},
	url = {https://doi.org/10.1093/rap/rkz047},
	note = {rkz047},
	eprint = {https://academic.oup.com/rheumap/article-pdf/3/2/rkz047/31569737/rkz047.pdf},
}

@misc{matterport_maskrcnn_2017,
	title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},
	author={Waleed Abdulla},
	year={2017},
	publisher={Github},
	journal={GitHub repository},
	howpublished={\url{https://github.com/matterport/Mask_RCNN}},
}

@misc{synapse_ra2,
	author= {{University of Alabama at Birmingham} and {IBM} and {University of Colorado Anschutz Medical Campus} and {Sage Bionetworks}},
	year  = {2020},
	title = {RA2 DREAM Challenge},
	note  = {\url{https://www.synapse.org/#!Synapse:syn20545111/wiki/594083}, 
	Last accessed on 04.06.2020},
}