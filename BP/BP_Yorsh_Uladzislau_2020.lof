\babel@toc {english}{}
\babel@toc {english}{}
\babel@toc {czech}{}
\babel@toc {english}{}
\babel@toc {czech}{}
\babel@toc {english}{}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {1.1}{\ignorespaces A toy overfit example. Red points represent two-dimensional linear data with some noise. While linear model explains the data not perfectly, it achieves a good generalization. The more powerful seventh degree polynomial model fitted the data ideally, but failed to generalize. \relax }}{5}{figure.caption.8}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Typical loss-by-iteration training graph. The blue graph represents training loss, which monotonically (in practice with some oscillation) decreases the more training iterations the model passed. The yellow graph represents validation loss, which starts to grow at the same time the model starts to overfit\relax }}{6}{figure.caption.9}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces A comparison of a biological neuron (top) and its matematical model (bottom). Pictures taken from the CS231n course\cite {cs231n}.\relax }}{10}{figure.caption.10}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Example of a FFNN with two hidden layers. Visualized using NN-SVG\cite {nn-svg}.\relax }}{11}{figure.caption.11}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces Gradient descent visualization. The two-dimensional variable space represents model parameter values; $z$-axis is the training loss function value. Starting at some point (pre-trained or randomly initialized), the training algorithm computes the gradient at this point, multiplies it by the learning rate and subtracts it, yielding the new point with the lower loss function value. In case of a real ANN, a variable space is being million-dimensional with a non-trivial relief.\relax }}{13}{figure.caption.12}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Low and high learning rate toy examples. A too low LR value (above) slows the training down and sometimes may lead to being stuck in a shallow local minimum (not illustrated). A too high LR value (below) leads to several another problems, one of which is overshooting loss minima.\relax }}{14}{figure.caption.13}%
\contentsline {figure}{\numberline {1.7}{\ignorespaces Example computational graph of a single neuron with the sigmoid activation function. On the forward pass the neuron receives inputs and computes an output. During the backward pass the differentials of all nodes are being numerically computed using the forward pass values and gradients of their nodes-successors. The computational graph receives an initial gradient value of 1.0, since this is the derivative of the computational graph output by the computational graph's last node output (which are the same).\relax }}{15}{figure.caption.14}%
\contentsline {figure}{\numberline {1.8}{\ignorespaces Matrix form of the FFNN layer computational graph with the sigmoid activation function, where $\nabla F$ is a global gradient by the layer's output. The "$+$" node is defined as "add the $b$ vector to the every $XW$ row". $\nabla \sigma '$ therefore means mean value of $\nabla \sigma $ rows---a sum of $b$ gradients scaled down to be not dependent on the input batch size.\relax }}{16}{figure.caption.15}%
\contentsline {figure}{\numberline {1.9}{\ignorespaces Three common activation functions illustrated. While logistic sigmoid and $\qopname \relax o{tanh}$ functions saturate in both directions, ReLU saturates in only one. ReLU is also easier to compute both during forward and backward pass.\relax }}{17}{figure.caption.16}%
\contentsline {figure}{\numberline {1.10}{\ignorespaces Illustration taken from the Dropout paper\cite {dropout}. During training time only fraction of ANN connections is being subsampled and properly rescaled. During test time neurons see all their inputs. One of the proposed Dropout interpretations says, that during the test time different models sampled from the original one are being averaged.\relax }}{19}{figure.caption.17}%
\contentsline {figure}{\numberline {1.11}{\ignorespaces VGG16\cite {vgg} architecture illustrated. Taking a $224 \times 224 \times 3$ image as input, by an alternating application of $3 \times 3$ filters, non-linearities and MaxPools the model gradually shrinks the data spatially, adding more and more of a high-level information and stacking it along the depth axis. The later layers operate a high-level information, relying on features extracted by previous layers and capturing a significant spatial context.\relax }}{21}{figure.caption.18}%
\contentsline {figure}{\numberline {1.12}{\ignorespaces Pooling layer illustrated. The $4 \times 4$ grid represents an image, while $2 \times 2$ grids represent Max (top) and Average (bottom) pooling operation results. Precisely, max/average $2 \times 2$ pooling with stride 2.\relax }}{22}{figure.caption.19}%
\contentsline {figure}{\numberline {1.13}{\ignorespaces im2col possible implementation illustrated. The arrows denote im2col operation and its inversion.\relax }}{23}{figure.caption.20}%
\contentsline {figure}{\numberline {1.14}{\ignorespaces Regularization illustrated by the polynomial regression example. The blue graph shows 7-degree polynomial with no regularization; the orange one is being yielded by the similar model, but with an added $L_2$ regularizer. The second function is smoother, and intended to generalize better. Note, that it fits the training data worse.\relax }}{24}{figure.caption.21}%
\contentsline {figure}{\numberline {1.15}{\ignorespaces 5-fold cross-validation illustrated.\relax }}{25}{figure.caption.22}%
\contentsline {figure}{\numberline {1.16}{\ignorespaces Model averaging illustrated. By averaging several models a smoother function may be obtained.\relax }}{26}{figure.caption.23}%
\addvspace {10pt}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Comparison of BN-accelerated models with the baseline taken from the BN paper\cite {batchnorm}. The baseline model was the Inception\cite {going_deeper}; BN-baseline is the Inception with BN before each non-linearity; BN-x5 is the BN-baseline modified to accelerate training (no dropout, reduced $L_2$ regularization, $\times $5 larger initial LR, more aggressive LR decay, etc.); BN-x30 is BN-x5, but with LR increased by factor 30 instead of 5; BN-x5-Sigmoid is the BN-x5 but with ReLUs replaced by sigmoid non-linearity. BatchNormalized models show significant boost both in terms of training speed and accuracy relative to the baseline model.\relax }}{29}{figure.caption.24}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Degradation problem illustrated, plots taken from the paper\cite {resnet}. The plots clarify that the problem isn't related to overfitting, since deeper models are harder to optimize for training data.\relax }}{33}{figure.caption.25}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of two CNN pairs---plain 18-layer, plain 34-layer and their residual variants. Plot is taken from the paper\cite {resnet}. Skip connections bypass every two layers. While providing no additional parameters, residual networks show better results.\relax }}{34}{figure.caption.26}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces A 5-layer dense block with growth rate = 4. Picture from the DenseNet paper\cite {densenet}.\relax }}{35}{figure.caption.27}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces The four CV tasks illustrated. Picture from\cite {splash_of_color}.\relax }}{39}{figure.caption.28}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces The Regions with CNN scheme. The algorithm extracts region proposals from an original image, feeds them to CNN and classifies them by multiple SVMs. Picture from\cite {rcnn}.\relax }}{39}{figure.caption.29}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces The Fast R-CNN architecture scheme. Instead of repeating CNN applications, the model applies the CNN only once. The proposed regions are being projected to the CNN output, pooled, fed to fully-connected layers and finally passed to the two heads for classification and bounding box regression. Picture from\cite {fast_rcnn}.\relax }}{40}{figure.caption.30}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Left: The Faster R-CNN architecture brief scheme. The RPN slides over the backbone output and utilises the extracted features to propose regions. The proposed regions are then being processed similarly to Fast R-CNN. Right: The RPN application example. On a certain position for each of $k$ anchor shapes it predicts $2k$ values for "objectness" score (is there any object or not), and $4k$ values for bounding box regression relative to the anchor boundaries. Pictures from \cite {faster_rcnn}.\relax }}{41}{figure.caption.31}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces The Mask R-CNN architecture. Note the additional convolutional layer sequence predicting the masks and the RoI Align layer. Picture from\cite {mask_rcnn}.\relax }}{43}{figure.caption.32}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Left: Bilinear interpolation illustration. To interpolate the point, the four neighbours are needed. The interpolation is being computed along the one axis firstly (producing points $R_1$ and $R_2$), and then along another (using $R_1$ and $R_2$), resulting in three linear interpolations. Right: RoI Align example. The solid line represents the proposed region being pooled, while the stroke line delimits the backbone output. Each pooling bin has got four sampling points, each of which gets a bilinearly interpolated values of the feature map pixels. The pooling is being applied on sampling points, not on feature map pixels. Pictures from \cite {bilinear_interpolation} and \cite {mask_rcnn} respectively.\relax }}{44}{figure.caption.33}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces The proposed mask predictors for ResNet and Feature Pyramid Network\cite {feature_pyramid_nets} backbones respectively. Arrows denote either convolution, deconvolution or fully-connected layers as can be inferred from context. All convolutions are $3 \times 3$, except the output convolution which is $1 \times 1$. Deconvolutions are $2 \times 2$ with stride 2, all layers use ReLU non-linearity. Picture from\cite {mask_rcnn}.\relax }}{45}{figure.caption.34}%
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Regions for which erosion or narrowing needs to be assessed. Left: erosion regions. Right: narrowing regions.\relax }}{48}{figure.caption.35}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Label values distribution in log scale. Note that for some labels only 1-31 labels are available.\relax }}{49}{figure.caption.36}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Example images of a left hand and a right foot from the dataset.\relax }}{50}{figure.caption.37}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces The selected training plots of the detection models. Note the absence of noticeable validation loss degradation during training.\relax }}{52}{figure.caption.38}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Left: validation results. Right: test results. The tested models include variant one and two trained with 30 and 45 degress image augmentation. The fifth model is a V.2 with 30-degree augmentation initialized with CIFAR-pretrained weights.\relax }}{55}{figure.caption.39}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Oversampling and combined approach illustrated.\relax }}{57}{figure.caption.40}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Left: different loss functions illustrated. Right: their derivatives.\relax }}{59}{figure.caption.41}%
\addvspace {10pt}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {B.1}{\ignorespaces Training plot for a second model variant trained from scratch on 30-degree augmented images\relax }}{72}{figure.caption.44}%
\addvspace {10pt}
