{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ra2_docker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVtjvYjkJd+sHXwY1EBLkL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EternalSorrrow/bak/blob/master/ra2_docker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwJeO5nWHllK",
        "colab_type": "code",
        "outputId": "aa02aa98-4bdc-4f88-c941-710ab8298d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Install Docker\n",
        "\n",
        "! sudo apt update\n",
        "! sudo apt install apt-transport-https ca-certificates curl software-properties-common\n",
        "! curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n",
        "! sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"\n",
        "! sudo apt update\n",
        "! sudo apt install docker-ce"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,626 B 0%] [Wa\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connectin\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 2,587 B/88.7 k\u001b[0m\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 2,587 B/88.7 k\u001b[0m\r                                                                               \rGet:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 5,483 B/88.7 k\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 5,483 B/88.7 k\u001b[0m\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 11.3 kB/88.7 k\u001b[0m\r                                                                               \rGet:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [819 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [9 InRelease 2,604 B/88.7 kB 3%] [4 InRelease 14.\u001b[0m\u001b[33m\r0% [9 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 18.5 kB/88.7 kB 21%] [Waiting\u001b[0m\u001b[33m\r0% [3 InRelease gpgv 242 kB] [9 InRelease 15.6 kB/88.7 kB 18%] [4 InRelease 18.\u001b[0m\r                                                                               \rGet:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [87.3 kB]\n",
            "Get:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [141 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [829 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.7 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,149 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.1 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [34.1 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7,640 B]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [853 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [48.1 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,358 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,245 B]\n",
            "Get:26 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,784 kB]\n",
            "Get:27 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [861 kB]\n",
            "Fetched 7,498 kB in 4s (1,945 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "97 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20180409).\n",
            "curl is already the newest version (7.58.0-2ubuntu3.8).\n",
            "software-properties-common is already the newest version (0.96.24.32.12).\n",
            "The following NEW packages will be installed:\n",
            "  apt-transport-https\n",
            "0 upgraded, 1 newly installed, 0 to remove and 97 not upgraded.\n",
            "Need to get 1,692 B of archives.\n",
            "After this operation, 153 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 apt-transport-https all 1.6.12 [1,692 B]\n",
            "Fetched 1,692 B in 0s (11.8 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package apt-transport-https.\n",
            "(Reading database ... 134448 files and directories currently installed.)\n",
            "Preparing to unpack .../apt-transport-https_1.6.12_all.deb ...\n",
            "Unpacking apt-transport-https (1.6.12) ...\n",
            "Setting up apt-transport-https (1.6.12) ...\n",
            "OK\n",
            "Get:1 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Get:10 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages [11.8 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Fetched 76.3 kB in 1s (82.8 kB/s)\n",
            "Reading package lists... Done\n",
            "Hit:1 https://download.docker.com/linux/ubuntu bionic InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "97 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor aufs-tools cgroupfs-mount containerd.io docker-ce-cli iptables\n",
            "  libip6tc0 libiptc0 libmnl0 libnetfilter-conntrack3 libnfnetlink0\n",
            "  libxtables12 pigz\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils\n",
            "The following NEW packages will be installed:\n",
            "  apparmor aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli\n",
            "  iptables libip6tc0 libiptc0 libmnl0 libnetfilter-conntrack3 libnfnetlink0\n",
            "  libxtables12 pigz\n",
            "0 upgraded, 14 newly installed, 0 to remove and 97 not upgraded.\n",
            "Need to get 86.6 MB of archives.\n",
            "After this operation, 389 MB of additional disk space will be used.\n",
            "Get:1 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.2.13-1 [20.1 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnfnetlink0 amd64 1.0.1-3 [13.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmnl0 amd64 1.0.4-2 [12.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxtables12 amd64 1.6.1-2ubuntu2 [27.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 apparmor amd64 2.12-4ubuntu5.1 [487 kB]\n",
            "Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.8~3-0~ubuntu-bionic [42.6 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libip6tc0 amd64 1.6.1-2ubuntu2 [19.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libiptc0 amd64 1.6.1-2ubuntu2 [9,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnetfilter-conntrack3 amd64 1.0.6-2 [37.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 iptables amd64 1.6.1-2ubuntu2 [269 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6,320 B]\n",
            "Get:14 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.8~3-0~ubuntu-bionic [22.9 MB]\n",
            "Fetched 86.6 MB in 2s (50.8 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 14.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnfnetlink0:amd64.\n",
            "(Reading database ... 134452 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libnfnetlink0_1.0.1-3_amd64.deb ...\n",
            "Unpacking libnfnetlink0:amd64 (1.0.1-3) ...\n",
            "Selecting previously unselected package pigz.\n",
            "Preparing to unpack .../01-pigz_2.4-1_amd64.deb ...\n",
            "Unpacking pigz (2.4-1) ...\n",
            "Selecting previously unselected package libmnl0:amd64.\n",
            "Preparing to unpack .../02-libmnl0_1.0.4-2_amd64.deb ...\n",
            "Unpacking libmnl0:amd64 (1.0.4-2) ...\n",
            "Selecting previously unselected package libxtables12:amd64.\n",
            "Preparing to unpack .../03-libxtables12_1.6.1-2ubuntu2_amd64.deb ...\n",
            "Unpacking libxtables12:amd64 (1.6.1-2ubuntu2) ...\n",
            "Selecting previously unselected package apparmor.\n",
            "Preparing to unpack .../04-apparmor_2.12-4ubuntu5.1_amd64.deb ...\n",
            "Unpacking apparmor (2.12-4ubuntu5.1) ...\n",
            "Selecting previously unselected package libip6tc0:amd64.\n",
            "Preparing to unpack .../05-libip6tc0_1.6.1-2ubuntu2_amd64.deb ...\n",
            "Unpacking libip6tc0:amd64 (1.6.1-2ubuntu2) ...\n",
            "Selecting previously unselected package libiptc0:amd64.\n",
            "Preparing to unpack .../06-libiptc0_1.6.1-2ubuntu2_amd64.deb ...\n",
            "Unpacking libiptc0:amd64 (1.6.1-2ubuntu2) ...\n",
            "Selecting previously unselected package libnetfilter-conntrack3:amd64.\n",
            "Preparing to unpack .../07-libnetfilter-conntrack3_1.0.6-2_amd64.deb ...\n",
            "Unpacking libnetfilter-conntrack3:amd64 (1.0.6-2) ...\n",
            "Selecting previously unselected package iptables.\n",
            "Preparing to unpack .../08-iptables_1.6.1-2ubuntu2_amd64.deb ...\n",
            "Unpacking iptables (1.6.1-2ubuntu2) ...\n",
            "Selecting previously unselected package aufs-tools.\n",
            "Preparing to unpack .../09-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\n",
            "Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\n",
            "Selecting previously unselected package cgroupfs-mount.\n",
            "Preparing to unpack .../10-cgroupfs-mount_1.4_all.deb ...\n",
            "Unpacking cgroupfs-mount (1.4) ...\n",
            "Selecting previously unselected package containerd.io.\n",
            "Preparing to unpack .../11-containerd.io_1.2.13-1_amd64.deb ...\n",
            "Unpacking containerd.io (1.2.13-1) ...\n",
            "Selecting previously unselected package docker-ce-cli.\n",
            "Preparing to unpack .../12-docker-ce-cli_5%3a19.03.8~3-0~ubuntu-bionic_amd64.deb ...\n",
            "Unpacking docker-ce-cli (5:19.03.8~3-0~ubuntu-bionic) ...\n",
            "Selecting previously unselected package docker-ce.\n",
            "Preparing to unpack .../13-docker-ce_5%3a19.03.8~3-0~ubuntu-bionic_amd64.deb ...\n",
            "Unpacking docker-ce (5:19.03.8~3-0~ubuntu-bionic) ...\n",
            "Setting up aufs-tools (1:4.9+20170918-1ubuntu1) ...\n",
            "Setting up containerd.io (1.2.13-1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /lib/systemd/system/containerd.service.\n",
            "Setting up libiptc0:amd64 (1.6.1-2ubuntu2) ...\n",
            "Setting up apparmor (2.12-4ubuntu5.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up cgroupfs-mount (1.4) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libxtables12:amd64 (1.6.1-2ubuntu2) ...\n",
            "Setting up libnfnetlink0:amd64 (1.0.1-3) ...\n",
            "Setting up docker-ce-cli (5:19.03.8~3-0~ubuntu-bionic) ...\n",
            "Setting up libmnl0:amd64 (1.0.4-2) ...\n",
            "Setting up pigz (2.4-1) ...\n",
            "Setting up libip6tc0:amd64 (1.6.1-2ubuntu2) ...\n",
            "Setting up libnetfilter-conntrack3:amd64 (1.0.6-2) ...\n",
            "Setting up iptables (1.6.1-2ubuntu2) ...\n",
            "Setting up docker-ce (5:19.03.8~3-0~ubuntu-bionic) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /lib/systemd/system/docker.service.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/systemd/system/docker.socket.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Processing triggers for systemd (237-3ubuntu10.39) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR9BSfxBHnUx",
        "colab_type": "code",
        "outputId": "9a4892f7-771f-4a2b-b50a-b3b990284cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Ensure Docker is correctly installed\n",
        "\n",
        "! docker --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Docker version 19.03.8, build afacb8b7f0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkQH3upoORHL",
        "colab_type": "code",
        "outputId": "aa79fb48-49b3-4bd6-df03-7c78145b8f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Required to use GPU: Copy and paste the hash-marked section with PATH exports to your run.sh file. This will make \n",
        "# NVIDIA drivers available to your container. Alternatively, you can define these system-wide in the Dockerfile\n",
        "\n",
        "######\n",
        "export CUDA_HOME=/cm/local/apps/cuda/libs/current\n",
        "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${CUDA_HOME}/lib64\n",
        "PATH=${CUDA_HOME}/bin:${PATH}\n",
        "export PATH\n",
        "\n",
        "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/cm/shared/apps/cuda10.0/toolkit/10.0.130/lib64\n",
        "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/share/apps/rc/software/cuDNN/7.6.2.24-CUDA-10.1.243/lib64\n",
        "######\n",
        "\n",
        "# Required for MRCNN modules to be detected\n",
        "######\n",
        "export $PYTHONPATH=$PYTHONPATH:/Mask_RCNN\n",
        "######\n",
        "\n",
        "# Command to run test\n",
        "#python3 /usr/local/bin/test.py\n",
        "\n",
        "# Command to run the ra2 script\n",
        "python3 /usr/local/bin/ra2.py\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing run.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4ScMIFbI-Q1",
        "colab_type": "code",
        "outputId": "d9af99b9-338f-4320-b59c-9593e452233a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Start from this Docker image\n",
        "FROM ubuntu\n",
        "\n",
        "# Install python3 and regular dependencies\n",
        "RUN apt-get update -y && \\\n",
        "    apt-get install software-properties-common -y && \\\n",
        "    add-apt-repository ppa:deadsnakes/ppa -y && \\\n",
        "    apt-get install python3 -y && \\\n",
        "    apt-get install -y git wget && \\\n",
        "    apt-get install -y libpython3-dev && \\\n",
        "    apt-get install -y python-pip libjpeg-dev && \\\n",
        "    apt-get install -y python3-pip && \\\n",
        "    apt-get install -y libsm6 libxext6 libxrender-dev && \\\n",
        "    pip3 install --upgrade pip && \\\n",
        "    pip3 install opencv-python pandas numpy && \\\n",
        "    pip3 install scikit-learn\n",
        "\n",
        "#For Matterport MRCNN support\n",
        "RUN pip3 install scikit-image && pip3 install ipython\n",
        "\n",
        "# Install tensorflow and keras to train and run models\n",
        "RUN pip3 install tensorflow-gpu==1.15 keras==2.2.5\n",
        "\n",
        "#Instal Matterport MRCNN\n",
        "RUN git clone https://github.com/matterport/Mask_RCNN.git && cd Mask_RCNN && python3 setup.py install && pip show mask-rcnn\n",
        "\n",
        "# Required: Create /train /test and /output directories\n",
        "RUN mkdir /train && mkdir /test && mkdir /output\n",
        "\n",
        "# Create directories for auxilary files\n",
        "RUN mkdir /trained_models && mkdir /weights && mkdir /output/logs\n",
        "\n",
        "# Required for GPU: run.sh defines PATHs to find GPU drivers, see run.sh for specific commands\n",
        "COPY run.sh /run.sh\n",
        "\n",
        "COPY trained_models /trained_models\n",
        "COPY weights /weights\n",
        "\n",
        "COPY test.py /usr/local/bin/test.py\n",
        "COPY ra2.py /usr/local/bin/ra2.py\n",
        "\n",
        "# Make model and runfiles executable\n",
        "#RUN chmod 775 /usr/local/bin/model.R\n",
        "RUN chmod 775 /run.sh && chmod 775 /usr/local/bin/test.py && chmod 775 /usr/local/bin/ra2.py\n",
        "\n",
        "RUN chmod 775 /weights && chmod 775 /output && chmod 775 /trained_models\n",
        "\n",
        "# This is for the virtualenv defined above, if not using a virtualenv, this is not necessary\n",
        "#RUN chmod 755 /root #to make virtualenv accessible to singularity user\n",
        "\n",
        "# Required: define an entrypoint. run.sh will run the model for us, but in a different configuration\n",
        "# you could simply call the model file directly as an entrypoint\n",
        "ENTRYPOINT [\"/bin/bash\", \"/run.sh\"]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Dockerfile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anpu9-tpWVJ1",
        "colab_type": "code",
        "outputId": "a633ce56-b612-43b4-a8b7-ee79affc0d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile test.py\n",
        "\n",
        "# confirm tensorflow sees the GPU\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "# confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
        "from keras import backend\n",
        "print(backend.tensorflow_backend._get_available_gpus())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtHcru9xBWK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ra2_evaluators.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1TL0__YdBIt5BC_YsBdhpCTXejSB3Ue_3\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "#@title Install dependencies\n",
        "\n",
        "#!pip3 uninstall tensorflow\n",
        "#!pip3 install tensorflow==1.15\n",
        "\n",
        "#!git clone https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "# %cd Mask_RCNN\n",
        "#!python setup.py install\n",
        "#!pip show mask-rcnn\n",
        "\n",
        "# %cd ..\n",
        "#!pip3 install imgaug\n",
        "\n",
        "#@title Mount the Google Drive\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#@title General training constants\n",
        "\n",
        "EARLY_STOPPING = True\n",
        "\n",
        "EPOCHS = 250\n",
        "FOLDS = 5\n",
        "VERBOSE= 2\n",
        "\n",
        "PATIENCE=60\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "#@title Import Mask R-CNN dependencies\n",
        "\n",
        "# %cd Mask_RCNN/\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n",
        "import mrcnn\n",
        "from mrcnn.utils import Dataset\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "from os import listdir\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# %cd ..\n",
        "\n",
        "#@title Configurations for pre-trained joint extractors\n",
        "\n",
        "max_instances_to_detect = 128\n",
        "\n",
        "FEET_REGIONS = 6\n",
        "HAND_REGIONS = 11\n",
        "WRIST_REGIONS = 6\n",
        "\n",
        "MODEL_DIR = '/output/logs'\n",
        "SAVE_MODELS = False\n",
        "\n",
        "class FeetJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"FeetJoints_config\"\n",
        "\n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = FEET_REGIONS + 1\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = 1\n",
        "    #VALIDATION_STEPS = 1\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "\n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "\n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=FEET_REGIONS\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "\n",
        "class HandJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"HandJoints_config\"\n",
        "\n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = HAND_REGIONS + 1\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = max(1, train_samples // IMAGES_PER_GPU)\n",
        "    #VALIDATION_STEPS = max(1, val_samples // IMAGES_PER_GPU)\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "\n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "\n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=HAND_REGIONS\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "class WristJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"WristJoints_config\"\n",
        "\n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = WRIST_REGIONS + 1\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = max(1, train_samples // IMAGES_PER_GPU)\n",
        "    #VALIDATION_STEPS = max(1, val_samples // IMAGES_PER_GPU)\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 256\n",
        "    IMAGE_MAX_DIM = 256\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    MEAN_PIXEL = [117.8, 117.8, 117.8]\n",
        "    USE_MINI_MASK = False\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "\n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "\n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=6\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "\n",
        "f_config = FeetJointsConfig()\n",
        "h_config = HandJointsConfig()\n",
        "w_config = WristJointsConfig()\n",
        "\n",
        "#@title Paths definition\n",
        "\n",
        "TRAIN_PATH      = '/train/'\n",
        "DATAFRAME_PATH  = '/train/training.csv'\n",
        "\n",
        "TEST_SET_PATH = '/test/'\n",
        "TEST_DATAFRAME_PATH = '/test/template.csv'\n",
        "OUTPUT_DATAFRAME_PATH = '/output/predictions.csv'\n",
        "\n",
        "MODEL_OUTPUT_PATH = '/output/'\n",
        "\n",
        "HAND_DETECTOR_PATH    = '/trained_models/mrcnn_hand_mrcnn_class_loss_best-200.hdf5'\n",
        "FEET_DETECTOR_PATH    = '/trained_models/mrcnn_feet_mrcnn_class_loss_best-160.hdf5'\n",
        "WRIST_E_DETECTOR_PATH = '/trained_models/mrcnn_we_loss_best-320.hdf5'\n",
        "WRIST_N_DETECTOR_PATH = '/trained_models/mrcnn_wn_mrcnn_class_loss_best-320.hdf5'\n",
        "\n",
        "DENSENET121_WEIGHT_PATH_NO_TOP = '/weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "DENSENET169_WEIGHT_PATH_NO_TOP = '/weights/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "DENSENET201_WEIGHT_PATH_NO_TOP = '/weights/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "#@title Image sizes setup\n",
        "\n",
        "HAND_REGION_SCALE = 1.2\n",
        "FEET_REGION_SCALE = 1.2\n",
        "\n",
        "WRIST_E_REGION_SCALE = 1.2\n",
        "WRIST_N_REGION_SCALE = 1.2\n",
        "\n",
        "FEET_REGION_IMAGE_SHAPE    = (64,  64,  3)\n",
        "FINGER_REGION_IMAGE_SHAPE  = (64,  64,  3)\n",
        "WRIST_TEMP_REGION_IMAGE_SHAPE  = (256, 256, 3)\n",
        "WRIST_E_REGION_IMAGE_SHAPE = (64,  64,  3)\n",
        "WRIST_N_REGION_IMAGE_SHAPE = (64,  64,  3)\n",
        "\n",
        "#@title Detectors's region names\n",
        "\n",
        "FEET_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'mtp_1',\n",
        "    2 : 'mtp_2',\n",
        "    3 : 'mtp_3',\n",
        "    4 : 'mtp_4',\n",
        "    5 : 'mtp_5',\n",
        "    6 : 'mtp_ip',\n",
        "}\n",
        "\n",
        "HAND_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'pip_1',\n",
        "    2 : 'pip_2',\n",
        "    3 : 'pip_3',\n",
        "    4 : 'pip_4',\n",
        "    5 : 'pip_5',\n",
        "    6 : 'mcp_1',\n",
        "    7 : 'mcp_2',\n",
        "    8 : 'mcp_3',\n",
        "    9 : 'mcp_4',\n",
        "    10 : 'mcp_5',\n",
        "    11 : 'carp',\n",
        "}\n",
        "\n",
        "WRIST_N_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'cmc3',\n",
        "    2 : 'cmc4',\n",
        "    3 : 'cmc5',\n",
        "    4 : 'mna',\n",
        "    5 : 'capnlun',\n",
        "    6 : 'radcar',\n",
        "}\n",
        "\n",
        "WRIST_E_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'mc1',\n",
        "    2 : 'mul',\n",
        "    3 : 'nav',\n",
        "    4 : 'lunate',\n",
        "    5 : 'ulna',\n",
        "    6 : 'radius',\n",
        "}\n",
        "\n",
        "#@title Column names\n",
        "\n",
        "LF_NARROWING_REGION_NAMES = [\n",
        "    'LF_mtp_J__1',\n",
        "    'LF_mtp_J__2',\n",
        "    'LF_mtp_J__3',\n",
        "    'LF_mtp_J__4',\n",
        "    'LF_mtp_J__5',\n",
        "    'LF_mtp_J__ip',\n",
        "]\n",
        "\n",
        "RF_NARROWING_REGION_NAMES = [\n",
        "    'RF_mtp_J__1',\n",
        "    'RF_mtp_J__2',\n",
        "    'RF_mtp_J__3',\n",
        "    'RF_mtp_J__4',\n",
        "    'RF_mtp_J__5',\n",
        "    'RF_mtp_J__ip',\n",
        "]\n",
        "\n",
        "LF_EROSION_REGION_NAMES = [\n",
        "    'LF_mtp_E__1',\n",
        "    'LF_mtp_E__2',\n",
        "    'LF_mtp_E__3',\n",
        "    'LF_mtp_E__4',\n",
        "    'LF_mtp_E__5',\n",
        "    'LF_mtp_E__ip',\n",
        "]\n",
        "\n",
        "RF_EROSION_REGION_NAMES = [\n",
        "    'RF_mtp_E__1',\n",
        "    'RF_mtp_E__2',\n",
        "    'RF_mtp_E__3',\n",
        "    'RF_mtp_E__4',\n",
        "    'RF_mtp_E__5',\n",
        "    'RF_mtp_E__ip',\n",
        "]\n",
        "\n",
        "LH_FINGER_EROSION_REGION_NAMES = [\n",
        "    'LH_mcp_E__ip',\n",
        "    'LH_pip_E__2',\n",
        "    'LH_pip_E__3',\n",
        "    'LH_pip_E__4',\n",
        "    'LH_pip_E__5',\n",
        "    'LH_mcp_E__1',\n",
        "    'LH_mcp_E__2',\n",
        "    'LH_mcp_E__3',\n",
        "    'LH_mcp_E__4',\n",
        "    'LH_mcp_E__5',\n",
        "]\n",
        "\n",
        "RH_FINGER_EROSION_REGION_NAMES = [\n",
        "    'RH_mcp_E__ip',\n",
        "    'RH_pip_E__2',\n",
        "    'RH_pip_E__3',\n",
        "    'RH_pip_E__4',\n",
        "    'RH_pip_E__5',\n",
        "    'RH_mcp_E__1',\n",
        "    'RH_mcp_E__2',\n",
        "    'RH_mcp_E__3',\n",
        "    'RH_mcp_E__4',\n",
        "    'RH_mcp_E__5',\n",
        "]\n",
        "\n",
        "LH_FINGER_NARROWING_REGION_NAMES = [\n",
        "    'LH_pip_J__2',\n",
        "    'LH_pip_J__3',\n",
        "    'LH_pip_J__4',\n",
        "    'LH_pip_J__5',\n",
        "    'LH_mcp_J__1',\n",
        "    'LH_mcp_J__2',\n",
        "    'LH_mcp_J__3',\n",
        "    'LH_mcp_J__4',\n",
        "    'LH_mcp_J__5',\n",
        "]\n",
        "\n",
        "RH_FINGER_NARROWING_REGION_NAMES = [\n",
        "    'RH_pip_J__2',\n",
        "    'RH_pip_J__3',\n",
        "    'RH_pip_J__4',\n",
        "    'RH_pip_J__5',\n",
        "    'RH_mcp_J__1',\n",
        "    'RH_mcp_J__2',\n",
        "    'RH_mcp_J__3',\n",
        "    'RH_mcp_J__4',\n",
        "    'RH_mcp_J__5',\n",
        "]\n",
        "\n",
        "LH_WRIST_EROSION_REGION_NAMES = [\n",
        "    'LH_wrist_E__mc1',\n",
        "    'LH_wrist_E__mul',\n",
        "    'LH_wrist_E__nav',\n",
        "    'LH_wrist_E__lunate',\n",
        "    'LH_wrist_E__ulna',\n",
        "    'LH_wrist_E__radius',\n",
        "]\n",
        "\n",
        "RH_WRIST_EROSION_REGION_NAMES = [\n",
        "    'RH_wrist_E__mc1',\n",
        "    'RH_wrist_E__mul',\n",
        "    'RH_wrist_E__nav',\n",
        "    'RH_wrist_E__lunate',\n",
        "    'RH_wrist_E__ulna',\n",
        "    'RH_wrist_E__radius',\n",
        "]\n",
        "\n",
        "LH_WRIST_NARROWING_REGION_NAMES = [\n",
        "    'LH_wrist_J__cmc3',\n",
        "    'LH_wrist_J__cmc4',\n",
        "    'LH_wrist_J__cmc5',\n",
        "    'LH_wrist_J__mna',\n",
        "    'LH_wrist_J__capnlun',\n",
        "    'LH_wrist_J__radcar',\n",
        "]\n",
        "\n",
        "RH_WRIST_NARROWING_REGION_NAMES = [\n",
        "    'RH_wrist_J__cmc3',\n",
        "    'RH_wrist_J__cmc4',\n",
        "    'RH_wrist_J__cmc5',\n",
        "    'RH_wrist_J__mna',\n",
        "    'RH_wrist_J__capnlun',\n",
        "    'RH_wrist_J__radcar',\n",
        "]\n",
        "\n",
        "#@title Max scores per evaluation\n",
        "\n",
        "HAND_EROSION_SCALING = 6\n",
        "HAND_NARROWING_SCALING = 5\n",
        "\n",
        "FEET_EROSION_SCALING = 11\n",
        "FEET_NARROWING_SCALING = 5\n",
        "\n",
        "#@title Create detectors and load weights\n",
        "\n",
        "feet_detector    = modellib.MaskRCNN(mode=\"inference\", config=f_config, model_dir=MODEL_DIR)\n",
        "hand_detector    = modellib.MaskRCNN(mode=\"inference\", config=h_config, model_dir=MODEL_DIR)\n",
        "wrist_e_detector = modellib.MaskRCNN(mode=\"inference\", config=w_config, model_dir=MODEL_DIR)\n",
        "wrist_n_detector = modellib.MaskRCNN(mode=\"inference\", config=w_config, model_dir=MODEL_DIR)\n",
        "\n",
        "feet_detector.load_weights(FEET_DETECTOR_PATH, by_name=True)\n",
        "hand_detector.load_weights(HAND_DETECTOR_PATH, by_name=True)\n",
        "wrist_e_detector.load_weights(WRIST_E_DETECTOR_PATH, by_name=True)\n",
        "wrist_n_detector.load_weights(WRIST_N_DETECTOR_PATH, by_name=True)\n",
        "\n",
        "#@title Read example dataframe with labels\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#pd.read_csv(DATAFRAME_PATH).head()\n",
        "\n",
        "#@title Auxilary drawing function\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#def get_ax(rows=1, cols=1, size=8):\n",
        "    # \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    # all visualizations in the notebook. Provide a\n",
        "    # central point to control graph sizes.\n",
        "    #\n",
        "    # Change the default size attribute to control the size\n",
        "    # of rendered images\n",
        "    # \"\"\"\n",
        "    # _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    # return ax\n",
        "\n",
        "#@title Functions to extract regions and fix errors\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def fix_feet_misclassification(instances):\n",
        "\n",
        "  #Order in which missing regions will be fixed\n",
        "  #Most important -> less important\n",
        "  priority_order = [1, 6, 2, 3, 4, 5]\n",
        "\n",
        "  #Define, which classes where not detected\n",
        "  classes_to_detect = sorted(FEET_DETECTOR_REGION_NAMES.keys())[1:]\n",
        "  empty_classes = set(classes_to_detect) - set(instances.keys())\n",
        "  empty_classes = list(empty_classes)\n",
        "  empty_classes = [ item for item in priority_order if item in empty_classes ] #Sort empty classes according to the priority order\n",
        "\n",
        "  fixed_instances = deepcopy(instances)\n",
        "\n",
        "  detected_instances_count = sum(\n",
        "      [ len(item) for key, item in instances.items() ]\n",
        "  )\n",
        "\n",
        "  #Look for a missing region in another class's detections\n",
        "  for empty_class in empty_classes:\n",
        "\n",
        "    if empty_class == 6:#If mtp_ip not detected\n",
        "\n",
        "      if(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ] #Use the second activation of mtp_1\n",
        "\n",
        "    elif empty_class == 1: #If mtp_1 not detected\n",
        "\n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ] #Use the second activation of mtp_ip\n",
        "\n",
        "    elif empty_class == 2:#If mtp_2 not detected\n",
        "\n",
        "      if(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ] #Use the second activation of mtp_3\n",
        "\n",
        "      elif (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of mtp_4\n",
        "\n",
        "    elif empty_class == 3:#If mtp_3 not detected, may be the most common case\n",
        "\n",
        "      if (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of mtp_4\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ] #Use the second activation of mtp_2\n",
        "\n",
        "    elif empty_class == 4:#If mtp_4 not detected, may be the second most common case\n",
        "\n",
        "      if (len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ]#Use the second activation of mtp_3\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ]#Use the second activation of mtp_2\n",
        "\n",
        "    elif empty_class == 5:#If mtp_5 not detected\n",
        "\n",
        "      if (len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mtp_ip\n",
        "\n",
        "      elif(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ]#Use the second activation of mtp_1\n",
        "\n",
        "      elif(len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ]#Use the second activation of mtp_4\n",
        "\n",
        "  return fixed_instances\n",
        "\n",
        "def fix_hands_misclassification(instances):\n",
        "\n",
        "  #Order in which missing regions will be fixed\n",
        "  #Most important -> less important\n",
        "  priority_order = [11, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5] #carp, mcp, pip\n",
        "\n",
        "  #Define, which classes where not detected\n",
        "  classes_to_detect = sorted(HAND_DETECTOR_REGION_NAMES.keys())[1:]\n",
        "  empty_classes = set(classes_to_detect) - set(instances.keys())\n",
        "  empty_classes = list(empty_classes)\n",
        "  empty_classes = [ item for item in priority_order if item in empty_classes ] #Sort empty classes according to the priority order\n",
        "\n",
        "  fixed_instances = deepcopy(instances)\n",
        "\n",
        "  detected_instances_count = sum(\n",
        "      [ len(item) for key, item in instances.items() ]\n",
        "  )\n",
        "\n",
        "  #Look for a missing region in another class's detections\n",
        "  for empty_class in empty_classes:\n",
        "\n",
        "    if empty_class == 6:#If mcp_1 not detected\n",
        "\n",
        "      if(len(fixed_instances[10]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[10].pop(1) ] #Use the second activation of mcp_5\n",
        "\n",
        "      elif (len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ] #Use the second activation of pip_1\n",
        "\n",
        "    elif empty_class == 7:#If mcp_2 not detected\n",
        "\n",
        "      if(len(fixed_instances[8]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[8].pop(1) ] #Use the second activation of mcp_3\n",
        "\n",
        "      elif (len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ] #Use the second activation of mcp_4\n",
        "\n",
        "    elif empty_class == 8:#If mcp_3 not detected\n",
        "\n",
        "      if (len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ] #Use the second activation of mcp_4\n",
        "\n",
        "      elif(len(fixed_instances[7]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[7].pop(1) ] #Use the second activation of mcp_2\n",
        "\n",
        "    elif empty_class == 9:#If mcp_4 not detected\n",
        "\n",
        "      if (len(fixed_instances[8]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[8].pop(1) ]#Use the second activation of mcp_3\n",
        "\n",
        "      elif(len(fixed_instances[7]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[7].pop(1) ]#Use the second activation of mcp_2\n",
        "\n",
        "    elif empty_class == 10:#If mcp_5 not detected\n",
        "\n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mcp_1\n",
        "\n",
        "      elif(len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ]#Use the second activation of mcp_4\n",
        "\n",
        "    elif empty_class == 1:#If pip_1 not detected\n",
        "\n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ] #Use the second activation of mcp_1\n",
        "\n",
        "      elif (len(fixed_instances[5]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[5].pop(1) ] #Use the second activation of pip_5\n",
        "\n",
        "    elif empty_class == 2:#If pip_2 not detected\n",
        "\n",
        "      if(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ] #Use the second activation of pip_3\n",
        "\n",
        "      elif (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of pip_4\n",
        "\n",
        "    elif empty_class == 3:#If pip_3 not detected\n",
        "\n",
        "      if (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of pip_4\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ] #Use the second activation of pip_2\n",
        "\n",
        "    elif empty_class == 4:#If pip_4 not detected\n",
        "\n",
        "      if (len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ]#Use the second activation of pip_2\n",
        "\n",
        "      elif(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ]#Use the second activation of pip_3\n",
        "\n",
        "    elif empty_class == 5:#If pip_5 not detected\n",
        "\n",
        "      if(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ]#Use the second activation of pip_5\n",
        "\n",
        "      elif(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mcp_1\n",
        "\n",
        "  return fixed_instances\n",
        "\n",
        "def extract_regions(detector, image, classes_num, fix_function):\n",
        "  result = detector.detect([ image ], verbose=0)\n",
        "  result = result[0]\n",
        "\n",
        "  #Reshape masks from a 3d [H, W, Instances] tensor to the array of masks\n",
        "  masks = []\n",
        "  for i in range(result['masks'].shape[2]):\n",
        "    masks.append(result['masks'][:, :, i])\n",
        "\n",
        "  #Reshape dictionary to list\n",
        "  result = list(zip(\n",
        "      result['rois'],\n",
        "      result['class_ids'],\n",
        "      result['scores'],\n",
        "      masks\n",
        "  ))\n",
        "\n",
        "  groupped = dict()\n",
        "\n",
        "  #Group detected instances by class\n",
        "  for item in result:\n",
        "    roi, class_id, score, mask = item\n",
        "\n",
        "    if class_id not in groupped.keys():\n",
        "      groupped[class_id] = []\n",
        "\n",
        "    groupped[class_id].append((score, roi, mask))\n",
        "\n",
        "  #Sort each group by confidence scores\n",
        "  for key in groupped.keys():\n",
        "    groupped[key] = sorted(groupped[key], key = lambda x: x[0], reverse=True)\n",
        "\n",
        "  detected_classes = len(list(groupped.keys()))\n",
        "\n",
        "\n",
        "  #Ensure the model has detected all needed regions\n",
        "  misclassified = False\n",
        "\n",
        "  #Some joints may be misclassified, in most cases it may be fixed in trivial way\n",
        "  if detected_classes < classes_num:\n",
        "    groupped = fix_function(groupped)\n",
        "    misclassified = True\n",
        "\n",
        "  #Take max confidence result for each class as a region proposal\n",
        "  result = dict(\n",
        "      [ (key, groupped[key][0]) for key in groupped.keys() ]\n",
        "  )\n",
        "\n",
        "  return result, misclassified\n",
        "\n",
        "def convert_to_original_format(instances):\n",
        "  #Reshape { class : (score, roi, mask) } dictionary to the format the results are yielded by models\n",
        "  #Useful for visualization with MRCNN built-in functions\n",
        "  classes, rois, scores, masks = [], [], [], []\n",
        "\n",
        "  for key, value in instances.items():\n",
        "    score, roi, mask = value\n",
        "\n",
        "    classes.append(key)\n",
        "    rois.append(roi)\n",
        "    scores.append(score)\n",
        "    masks.append(mask)\n",
        "\n",
        "  result = {\n",
        "      'rois' : np.array(rois),\n",
        "      'scores' : np.array(scores),\n",
        "      'class_ids' : np.array(classes),\n",
        "      'masks' : np.array(masks).swapaxes(0,2).swapaxes(0,1)\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "def extract_regions_from_image(image, result, scale=1.0):\n",
        "  regions = dict()\n",
        "\n",
        "  for class_, region in zip(result['class_ids'], result['rois']):\n",
        "    #Rescale the box\n",
        "    x1, y1, x2, y2, = region\n",
        "    cx, cy = (x2 + x1)/2, (y2 + y1)/2\n",
        "    x1, x2 = int((x1-cx) * scale + cx), int((x2-cx) * scale + cx)\n",
        "    y1, y2 = int((y1-cy) * scale + cy), int((y2-cy) * scale + cy)\n",
        "\n",
        "    #Yes, yes, i know, y-axis is the 0 axis, but whatever\n",
        "    x1, x2, y1, y2 = max(0, x1), min(x2, image.shape[0]), max(0, y1), min(y2, image.shape[1])\n",
        "\n",
        "    regions[class_] = image[ x1:x2, y1:y2 ]\n",
        "\n",
        "  return regions\n",
        "\n",
        "#@title Extract regions from images\n",
        "\n",
        "def apply_detector(detector, image, classes_num, scale, fix_function=lambda x: x):\n",
        "  regions, misclassified = extract_regions(detector, image, classes_num, fix_function)\n",
        "\n",
        "  #Convert function output to the Matterport format compatible with their functions\n",
        "  regions = convert_to_original_format(regions)\n",
        "\n",
        "  #TODO: Create an output for badly classified image\n",
        "\n",
        "  regions = extract_regions_from_image(image, regions, scale)\n",
        "  return regions\n",
        "\n",
        "def process_wrist(image):\n",
        "  #Erosion and narrowing regions\n",
        "  E, N = dict(), dict()\n",
        "\n",
        "  if image is not None:\n",
        "    image = cv2.resize(image, WRIST_TEMP_REGION_IMAGE_SHAPE[:2])\n",
        "    E, N = apply_detector(wrist_e_detector, image, WRIST_REGIONS, WRIST_E_REGION_SCALE), \\\n",
        "           apply_detector(wrist_n_detector, image, WRIST_REGIONS, WRIST_N_REGION_SCALE)\n",
        "\n",
        "  return E, N\n",
        "\n",
        "def read_training_dataset(dataframe_path, images_path):\n",
        "  ExtractedRegions = dict()\n",
        "\n",
        "  df = pd.read_csv(dataframe_path)\n",
        "\n",
        "  for i, P_ID in enumerate(df['Patient_ID']):\n",
        "    PatientRegions = {\n",
        "        'info' : df[ df['Patient_ID'] == P_ID ]\n",
        "    }\n",
        "\n",
        "    #Compose full image filenames\n",
        "    LF = images_path + P_ID + '-LF.jpg'\n",
        "    RF = images_path + P_ID + '-RF.jpg'\n",
        "    LH = images_path + P_ID + '-LH.jpg'\n",
        "    RH = images_path + P_ID + '-RH.jpg'\n",
        "\n",
        "    #Read images\n",
        "    LF, RF, LH, RH = cv2.imread(LF), cv2.imread(RF), cv2.imread(LH), cv2.imread(RH)\n",
        "\n",
        "    #Flip right limb images since detectors were trained on left limb images only\n",
        "    RF, RH = np.flip(RF, axis=1), np.flip(RH, axis=1)\n",
        "\n",
        "    #Extract regions from images\n",
        "    LF, RF, LH, RH = apply_detector(feet_detector, LF, FEET_REGIONS, FEET_REGION_SCALE, fix_feet_misclassification),  \\\n",
        "                     apply_detector(feet_detector, RF, FEET_REGIONS, FEET_REGION_SCALE, fix_feet_misclassification),  \\\n",
        "                     apply_detector(hand_detector, LH, HAND_REGIONS, HAND_REGION_SCALE, fix_hands_misclassification), \\\n",
        "                     apply_detector(hand_detector, RH, HAND_REGIONS, HAND_REGION_SCALE, fix_hands_misclassification)\n",
        "\n",
        "    LW = LH.pop(11, None) #Get the wrist image for further segmentation\n",
        "    RW = RH.pop(11, None)\n",
        "\n",
        "    LWE, LWN, RWE, RWN = *process_wrist(LW), *process_wrist(RW)\n",
        "\n",
        "    PatientRegions['LF']  = LF\n",
        "    PatientRegions['RF']  = RF\n",
        "    PatientRegions['LH']  = LH\n",
        "    PatientRegions['RH']  = RH\n",
        "    PatientRegions['LWE'] = LWE\n",
        "    PatientRegions['LWN'] = LWN\n",
        "    PatientRegions['RWE'] = RWE\n",
        "    PatientRegions['RWN'] = RWN\n",
        "\n",
        "    ExtractedRegions[P_ID] = PatientRegions\n",
        "\n",
        "    if (not ( (i + 1) % 50 )) or (i + 1) == len(df['Patient_ID']):\n",
        "      print('Applied detectors on', i + 1, 'PIDs')\n",
        "\n",
        "  return ExtractedRegions\n",
        "\n",
        "#@title Functions to build numpy arrays from image metadata and extracted regions\n",
        "\n",
        "def get_sorted_patient_data(region_dictionary):\n",
        "  region_dictionary = sorted(\n",
        "      list( region_dictionary.items() ), key = lambda x: x[0]\n",
        "  )\n",
        "  return region_dictionary\n",
        "\n",
        "def form_feet_datasets(sorted_region_data):\n",
        "  Images  = []\n",
        "  Erosion = []\n",
        "  Narrowing = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LF_data = sorted(\n",
        "        list( data['LF'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LF_data:\n",
        "      image = cv2.resize(image, FEET_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      erosion_score_column   = LF_EROSION_REGION_NAMES   [ region_class - 1 ]\n",
        "      narrowing_score_column = LF_NARROWING_REGION_NAMES [ region_class - 1 ]\n",
        "\n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "      narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Erosion.append(erosion_score)\n",
        "      Narrowing.append(narrowing_score)\n",
        "\n",
        "    sorted_RF_data = sorted(\n",
        "        list( data['RF'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RF_data:\n",
        "      image = cv2.resize(image, FEET_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      erosion_score_column   = RF_EROSION_REGION_NAMES   [ region_class - 1 ]\n",
        "      narrowing_score_column = RF_NARROWING_REGION_NAMES [ region_class - 1 ]\n",
        "\n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "      narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Erosion.append(erosion_score)\n",
        "      Narrowing.append(narrowing_score)\n",
        "\n",
        "  return np.array(Images), np.array(Erosion), np.array(Narrowing)\n",
        "\n",
        "def form_hand_datasets(sorted_region_data):\n",
        "  ErosionImages = []\n",
        "  ErosionLabels = []\n",
        "\n",
        "  NarrowingImages = []\n",
        "  NarrowingLabels = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LH_data = sorted(\n",
        "        list( data['LH'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LH_data:\n",
        "      image = cv2.resize(image, FINGER_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      erosion_score_column   = LH_FINGER_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      narrowing_score_column = LH_FINGER_NARROWING_REGION_NAMES[ region_class - 2 ] if region_class > 1 else None\n",
        "\n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "\n",
        "      ErosionImages.append(image)\n",
        "      ErosionLabels.append(erosion_score)\n",
        "\n",
        "      if narrowing_score_column is not None:\n",
        "        narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "        NarrowingImages.append(image)\n",
        "        NarrowingLabels.append(narrowing_score)\n",
        "\n",
        "    sorted_RH_data = sorted(\n",
        "        list( data['RH'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RH_data:\n",
        "      image = cv2.resize(image, FINGER_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      erosion_score_column   = RH_FINGER_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      narrowing_score_column = RH_FINGER_NARROWING_REGION_NAMES[ region_class - 2 ] if region_class > 1 else None\n",
        "\n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "\n",
        "      ErosionImages.append(image)\n",
        "      ErosionLabels.append(erosion_score)\n",
        "\n",
        "      if narrowing_score_column is not None:\n",
        "        narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "        NarrowingImages.append(image)\n",
        "        NarrowingLabels.append(narrowing_score)\n",
        "\n",
        "  return np.array(ErosionImages), np.array(ErosionLabels), np.array(NarrowingImages), np.array(NarrowingLabels)\n",
        "\n",
        "def form_wrist_erosion_dataset(sorted_region_data):\n",
        "  Images = []\n",
        "  Labels = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LWE_data = sorted(\n",
        "        list( data['LWE'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LWE_data:\n",
        "      image = cv2.resize(image, WRIST_E_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = LH_WRIST_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "    sorted_RWE_data = sorted(\n",
        "        list( data['RWE'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RWE_data:\n",
        "      image = cv2.resize(image, WRIST_E_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = RH_WRIST_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "  return np.array(Images), np.array(Labels)\n",
        "\n",
        "def form_wrist_narrowing_dataset(sorted_region_data):\n",
        "  Images = []\n",
        "  Labels = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LWN_data = sorted(\n",
        "        list( data['LWN'].items() ), key = lambda x: [0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LWN_data:\n",
        "      image = cv2.resize(image, WRIST_N_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = LH_WRIST_NARROWING_REGION_NAMES[ region_class - 1 ]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "    sorted_RWN_data = sorted(\n",
        "        list( data['RWN'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RWN_data:\n",
        "      image = cv2.resize(image, WRIST_N_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = RH_WRIST_NARROWING_REGION_NAMES[ region_class - 1]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "  return np.array(Images), np.array(Labels)\n",
        "\n",
        "print('Loading training data:')\n",
        "training_data = read_training_dataset(DATAFRAME_PATH, TRAIN_PATH)\n",
        "\n",
        "sorted_data = get_sorted_patient_data(training_data)\n",
        "\n",
        "feet_joints, feet_erosion, feet_narrowing = form_feet_datasets(sorted_data)\n",
        "finger_erosion_images, finger_erosion, finger_narrowing_images, finger_narrowing = form_hand_datasets(sorted_data)\n",
        "wrist_erosion_images, wrist_erosion = form_wrist_erosion_dataset(sorted_data)\n",
        "wrist_narrowing_images, wrist_narrowing = form_wrist_narrowing_dataset(sorted_data)\n",
        "\n",
        "\n",
        "print('Data shape:', feet_joints.shape, feet_erosion.shape, feet_narrowing.shape, finger_erosion_images.shape,\n",
        "                    finger_erosion.shape,finger_narrowing_images.shape, finger_narrowing.shape, wrist_erosion_images.shape,\n",
        "                    wrist_erosion.shape, wrist_narrowing_images.shape, wrist_narrowing.shape)\n",
        "\n",
        "#@title Dataset processing functions\n",
        "\n",
        "def normalize_labels(vector, smoothing=0.0, max_value = None):\n",
        "  #Normalizes integer non-negative values to [0, 1] interval\n",
        "  #Smoothing value in range [0, 1] shifts value to the center\n",
        "\n",
        "  #If no scale value provided, get the max\n",
        "  if max_value is None or max_value < np.max(vector):\n",
        "    max_value = np.max(vector)\n",
        "\n",
        "  #Normalize\n",
        "  vector = vector.astype(np.float64) / max_value\n",
        "\n",
        "  #Smooth\n",
        "  smooth = lambda x: (0.5 + (x - 0.5) * (1 - smoothing))\n",
        "  return np.array([\n",
        "          smooth(value) for value in vector\n",
        "  ])\n",
        "\n",
        "def denormalize_labels(vector, max_value, smoothing=0.0):\n",
        "  #Inverse operation, denormalizes [0, 1] values to [0, N] range\n",
        "\n",
        "  extend = lambda x: (0.5 + (x - 0.5) / (1 - smoothing))\n",
        "  vector = np.array([ extend(value) for value in vector ]) * max_value\n",
        "  return vector\n",
        "\n",
        "def downsample_class(samples, labels, multiplier, target_class):\n",
        "  #Get classes represented in data set, and their sample counts\n",
        "  classes, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "  #Compute index of the class to downsample\n",
        "  for i, class_ in enumerate(classes):\n",
        "    if class_ == target_class:\n",
        "      target_class = i\n",
        "      break\n",
        "\n",
        "  #Compute the count of the downsampled class's samples\n",
        "  target_count = int(counts[target_class] * multiplier)\n",
        "\n",
        "  output_data   = []\n",
        "  output_labels = []\n",
        "\n",
        "  #Permute the data set\n",
        "  permutation = np.random.permutation(len(labels))\n",
        "  samples = [ samples[i] for i in permutation ]\n",
        "  labels  = [ labels[ i] for i in permutation]\n",
        "\n",
        "  #Counter for already sampled samples of class to downsample\n",
        "  sampled_values = 0\n",
        "\n",
        "  #Traverse the data set\n",
        "  for i in range(len(samples)):\n",
        "    if labels[i] == classes[target_class]:\n",
        "      if sampled_values >= target_count:\n",
        "        pass\n",
        "      else:\n",
        "        sampled_values += 1\n",
        "\n",
        "        output_data.append(samples[i])\n",
        "        output_labels.append(labels[i])\n",
        "    else:\n",
        "      output_data.append(samples[i])\n",
        "      output_labels.append(labels[i])\n",
        "\n",
        "  return np.array(output_data), np.array(output_labels)\n",
        "\n",
        "def train_test_split(data, labels, split):\n",
        "  permutation = np.random.permutation(range(labels.shape[0]))\n",
        "\n",
        "  split = int(labels.shape[0] * split)\n",
        "  train_indices, val_indices = permutation[:split], permutation[split:]\n",
        "\n",
        "  return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]\n",
        "\n",
        "def balanced_train_test_split(data, labels, split, rounding='trunc'):\n",
        "  #Initialize dict with labels as keys and empty\n",
        "  data_per_label = dict([ (label, []) for label in np.unique(labels) ])\n",
        "  [ data_per_label[label].append(sample) for sample, label in zip(data, labels) ]\n",
        "\n",
        "  #Select function which will round the train sample count in certain class\n",
        "  #Distinct effect only on classes with few samples\n",
        "  rounding_functions = {\n",
        "      'trunc'  : lambda x: int(x),\n",
        "      'round'  : lambda x: int(round(x)),\n",
        "      'random' : lambda x: int(x) if np.random.rand() < 0.5 else int(round(x))\n",
        "  }\n",
        "  rounding = rounding_functions[rounding]\n",
        "\n",
        "  x_train, y_train, x_valid, y_valid = [], [], [], []\n",
        "\n",
        "  #Permute samples of several class and split them into train/val according to 'split' value\n",
        "  for label, sample_list in data_per_label.items():\n",
        "    permutation = np.random.permutation(len(sample_list))\n",
        "    separator = rounding(split * len(permutation))\n",
        "    sample_list = [ sample_list[i] for i in permutation ]\n",
        "\n",
        "    [ (x_train.append(sample_list[i]), y_train.append(label)) for i in permutation[:separator] ]\n",
        "    [ (x_valid.append(sample_list[i]), y_valid.append(label)) for i in permutation[separator:] ]\n",
        "\n",
        "  #Permute splitted sets\n",
        "  train_permutation = np.random.permutation(len(x_train))\n",
        "  valid_permutation = np.random.permutation(len(x_valid))\n",
        "\n",
        "  x_train, y_train = [ x_train[i] for i in train_permutation ], [ y_train[i] for i in train_permutation ]\n",
        "  x_valid, y_valid = [ x_valid[i] for i in valid_permutation ], [ y_valid[i] for i in valid_permutation ]\n",
        "\n",
        "  return np.array(x_train), np.array(y_train), np.array(x_valid), np.array(y_valid)\n",
        "\n",
        "def naive_oversampling(data, labels):\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  classes, frequencies = np.unique(labels, return_counts=True) #labels count\n",
        "\n",
        "  oversampled_count = np.max(frequencies) * len(frequencies) - data.shape[0] #oversampled data size\n",
        "\n",
        "  frequencies = np.max(frequencies) * 1 / frequencies #inverse values to convert them to sampling weights\n",
        "  frequencies = frequencies - frequencies[np.argmin(frequencies)]  #do not oversample the most common class\n",
        "\n",
        "  oversampled_data = np.zeros(\n",
        "      shape = (oversampled_count,) + data.shape[1:],\n",
        "      dtype = data.dtype\n",
        "  )\n",
        "\n",
        "  oversampled_labels = np.zeros(\n",
        "      shape = oversampled_count,\n",
        "      dtype = labels.dtype\n",
        "  )\n",
        "\n",
        "  frequencies = np.array([\n",
        "            frequencies[np.argwhere( classes == class_ ).flat[0]] for class_ in labels\n",
        "  ])\n",
        "  frequencies = frequencies / np.sum(frequencies)\n",
        "\n",
        "  indices = np.random.choice(data.shape[0], size = oversampled_count, p = frequencies)\n",
        "\n",
        "  for i, index in enumerate(indices):\n",
        "    oversampled_data[i] = data[index]\n",
        "    oversampled_labels[i] = labels[index]\n",
        "\n",
        "  return np.concatenate([data, oversampled_data], axis=0), np.concatenate([labels, oversampled_labels], axis=0)\n",
        "\n",
        "def downsample_most_common_class(data, labels):\n",
        "  #Downsamples the most common class to the values number of the second common class\n",
        "  classes, frequencies = np.unique(labels, return_counts=True)\n",
        "  second_maximum, first_maximum = sorted(frequencies)[-2], sorted(frequencies)[-1]\n",
        "  downsample_factor = second_maximum / first_maximum\n",
        "  class_to_downsample = classes[ np.argmax(frequencies) ]\n",
        "\n",
        "  return downsample_class(data, labels, downsample_factor, class_to_downsample)\n",
        "\n",
        "#@title Modified DenseNet definition\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "import keras.utils as keras_utils\n",
        "\n",
        "from keras_applications import imagenet_utils\n",
        "from keras_applications.imagenet_utils import decode_predictions\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.layers import SpatialDropout2D\n",
        "\n",
        "def dense_block(x, blocks, name, regularizer_factory):\n",
        "    for i in range(blocks):\n",
        "        x = conv_block(x, 32, name=name + '_block' + str(i + 1), regularizer_factory=regularizer_factory)\n",
        "    return x\n",
        "\n",
        "def sd_dense_block(x, blocks, name, min_dropout_rate, max_dropout_rate, regularizer_factory):\n",
        "\n",
        "  bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "  rates = np.linspace(\n",
        "      start=min_dropout_rate,\n",
        "      stop=max_dropout_rate,\n",
        "      num=blocks,\n",
        "      endpoint=True\n",
        "  )\n",
        "\n",
        "  outputs = [x]\n",
        "\n",
        "  for i in range(blocks):\n",
        "    rates_slice = rates[:i+1]\n",
        "    block_name = name + '_block' + str(i + 1)\n",
        "\n",
        "    specific_block_input = [ SpatialDropout2D(rate)(x) for rate, x in zip(rates_slice, outputs) ]\n",
        "    if len(specific_block_input) > 1:\n",
        "      specific_block_input = layers.Concatenate(axis=bn_axis, name=block_name + '_concat')(specific_block_input)\n",
        "    else:\n",
        "      specific_block_input = specific_block_input[0]\n",
        "\n",
        "    output = sd_conv_block(specific_block_input, 32, name=block_name, regularizer_factory=regularizer_factory)\n",
        "    outputs = [output] + outputs\n",
        "\n",
        "  return layers.Concatenate(axis=bn_axis, name=name + '_concat')(outputs)\n",
        "\n",
        "def transition_block(x, reduction, name, regularizer_factory):\n",
        "\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_bn')(x)\n",
        "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
        "    x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n",
        "                      use_bias=False,\n",
        "                      name=name + '_conv',\n",
        "                      kernel_regularizer=regularizer_factory())(x)\n",
        "    x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(x, growth_rate, name, regularizer_factory):\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
        "                                   epsilon=1.001e-5,\n",
        "                                   name=name + '_0_bn')(x)\n",
        "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
        "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
        "                       use_bias=False,\n",
        "                       name=name + '_1_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                   name=name + '_1_bn')(x1)\n",
        "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3,\n",
        "                       padding='same',\n",
        "                       use_bias=False,\n",
        "                       name=name + '_2_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
        "    return x\n",
        "\n",
        "def sd_conv_block(x, growth_rate, name, regularizer_factory):\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
        "                                   epsilon=1.001e-5,\n",
        "                                   name=name + '_0_bn')(x)\n",
        "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
        "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
        "                       use_bias=False,\n",
        "                       name=name + '_1_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                   name=name + '_1_bn')(x1)\n",
        "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3,\n",
        "                       padding='same',\n",
        "                       use_bias=False,\n",
        "                       name=name + '_2_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "\n",
        "    return x1\n",
        "\n",
        "\n",
        "def DenseNet(blocks,\n",
        "             include_top=True,\n",
        "             weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "             input_tensor=None,\n",
        "             input_shape=None,\n",
        "             pooling=None,\n",
        "             classes=1000,\n",
        "             blocks_to_include=3,\n",
        "             include_large_conv=False,\n",
        "             min_dropout_rate=0.0,\n",
        "             max_dropout_rate=0.0,\n",
        "             regularizer_factory=lambda: None):\n",
        "\n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=224,\n",
        "                                      min_size=32,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    if include_large_conv:\n",
        "      x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
        "      x = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv', kernel_regularizer=regularizer_factory())(x)\n",
        "    else:\n",
        "      #new layer\n",
        "      x = layers.Conv2D(64, 3, use_bias=False, padding='same', name='conv1/conv_replaced', kernel_regularizer=regularizer_factory())(img_input)\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
        "    x = layers.Activation('relu', name='conv1/relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\n",
        "\n",
        "    if abs(max_dropout_rate) < 1e-4: #If dropout rate defined as zero, use non-dropout version\n",
        "      x = dense_block(x, blocks[0], name='conv2', regularizer_factory=regularizer_factory)\n",
        "      x = transition_block(x, 0.5, name='pool2',  regularizer_factory=regularizer_factory)\n",
        "      x = dense_block(x, blocks[1], name='conv3', regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 2:\n",
        "        x = transition_block(x, 0.5, name='pool3',  regularizer_factory=regularizer_factory)\n",
        "        x = dense_block(x, blocks[2], name='conv4', regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 3:\n",
        "        x = transition_block(x, 0.5, name='pool4',  regularizer_factory=regularizer_factory)\n",
        "        x = dense_block(x, blocks[3], name='conv5', regularizer_factory=regularizer_factory)\n",
        "    else: #If non-zero dropout rate, use specialized dropout blocks\n",
        "      x = sd_dense_block(x, blocks[0], 'conv2', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      x = transition_block(x, 0.5, 'pool2', regularizer_factory=regularizer_factory)\n",
        "      x = sd_dense_block(x, blocks[1], 'conv3', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 2:\n",
        "        x = transition_block(x, 0.5, 'pool3', regularizer_factory=regularizer_factory)\n",
        "        x = sd_dense_block(x, blocks[2], 'conv4', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 3:\n",
        "        x = transition_block(x, 0.5, 'pool4', regularizer_factory=regularizer_factory)\n",
        "        x = sd_dense_block(x, blocks[3], 'conv5', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "\n",
        "    bn_name = 'bn'\n",
        "    if blocks_to_include < 3:\n",
        "      bn_name = 'bn_replaced'\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=bn_name)(x)\n",
        "    x = layers.Activation('relu', name='relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "\n",
        "    #Compute conv layers number to name the model 'densenetX'\n",
        "\n",
        "    true_blocks = blocks[:blocks_to_include]\n",
        "    model_name = sum([1] + list(map(lambda x: x * 2 + 1, true_blocks)))\n",
        "    model_name = 'densenet' + str(model_name)\n",
        "\n",
        "    model = models.Model(inputs, x, name=model_name)\n",
        "\n",
        "    # Load weights.\n",
        "    if weights is not None:\n",
        "        model.load_weights(weights, by_name=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def ModifiedDenseNet(\n",
        "      blocks_set = 121,\n",
        "      weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      blocks_to_include=3,\n",
        "      include_large_conv=False,\n",
        "      min_dropout_rate = 0.0,\n",
        "      max_dropout_rate = 0.0,\n",
        "      regularizer_factory=lambda: None\n",
        "    ):\n",
        "\n",
        "    blocks = {\n",
        "        121 : [6, 12, 24, 16],\n",
        "        169 : [6, 12, 32, 32],\n",
        "        201 : [6, 12, 48, 32]\n",
        "    }\n",
        "    if isinstance(blocks_set, list):\n",
        "      blocks = blocks_set\n",
        "    elif isinstance(blocks_set, int):\n",
        "      if blocks_set in blocks.keys():\n",
        "        blocks = blocks[blocks_set]\n",
        "      else:\n",
        "        raise ValueError('No such blocks set defined')\n",
        "    else:\n",
        "      raise ValueError('No such blocks set defined')\n",
        "\n",
        "    return DenseNet(blocks,\n",
        "                    False, weights,\n",
        "                    None, input_shape,\n",
        "                    pooling, 1000, blocks_to_include, include_large_conv, min_dropout_rate, max_dropout_rate, regularizer_factory)\n",
        "\n",
        "#@title Model defining functions\n",
        "\n",
        "import os\n",
        "#import tempfile\n",
        "import re\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.applications.resnet import ResNet50, ResNet101, ResNet152\n",
        "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201, preprocess_input\n",
        "from keras.layers import Dense, Dropout, SpatialDropout2D\n",
        "from keras.models import Model\n",
        "from keras.losses import categorical_crossentropy, mean_absolute_error, mean_squared_error, logcosh\n",
        "from keras.optimizers import adam, sgd\n",
        "from keras import regularizers\n",
        "from keras.models import model_from_json\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "\n",
        "def denormalized_mae(y_true, y_pred, max_value, smoothing):\n",
        "  y_true = K.constant(0.5) + (y_true - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "  y_pred = K.constant(0.5) + (y_pred - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "\n",
        "  y_true = y_true * max_value\n",
        "  y_pred = y_pred * max_value\n",
        "\n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "def denormalized_mse(y_true, y_pred, max_value, smoothing):\n",
        "  y_true = K.constant(0.5) + (y_true - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "  y_pred = K.constant(0.5) + (y_pred - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "\n",
        "  y_true = y_true * max_value\n",
        "  y_pred = y_pred * max_value\n",
        "\n",
        "  return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "def get_denormalized_function(max_value, func_name, smoothing):\n",
        "  def mae(y_true, y_pred):\n",
        "    return denormalized_mae(y_true, y_pred, max_value, smoothing)\n",
        "\n",
        "  def mse(y_true, y_pred):\n",
        "    return denormalized_mse(y_true, y_pred, max_value, smoothing)\n",
        "\n",
        "  funcs = {\n",
        "      'mae' : mae,\n",
        "      'mse' : mse,\n",
        "  }\n",
        "\n",
        "  return funcs[func_name]\n",
        "\n",
        "def get_lr_metric(optimizer): #Custom metric to monitor learning rate\n",
        "  def lr(y_true, y_pred):\n",
        "      return optimizer.lr\n",
        "  return lr\n",
        "\n",
        "def get_feet_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FEET_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_feet_erosion'\n",
        "                )\n",
        "\n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  mae = get_denormalized_function(FEET_EROSION_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(FEET_EROSION_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "\n",
        "                #loss=mean_squared_error,\n",
        "                loss=mean_absolute_error,\n",
        "                #loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_feet_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FEET_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_feet_narrowing'\n",
        "                )\n",
        "\n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  mae = get_denormalized_function(FEET_NARROWING_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(FEET_NARROWING_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "\n",
        "                #loss=mean_squared_error,\n",
        "                loss=mean_absolute_error,\n",
        "                #loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_hand_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FINGER_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory,\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_hand_erosion'\n",
        "                )\n",
        "\n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  mae = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "\n",
        "                #loss=mean_squared_error,\n",
        "                loss=mean_absolute_error,\n",
        "                #loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_hand_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FINGER_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_hand_narrowing'\n",
        "                )\n",
        "\n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  mae = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "\n",
        "                #loss=mean_squared_error,\n",
        "                loss=mean_absolute_error,\n",
        "                #loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_wrist_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=WRIST_E_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_wrist_erosion'\n",
        "                )\n",
        "\n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  mae = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "\n",
        "                #loss=mean_squared_error,\n",
        "                loss=mean_absolute_error,\n",
        "                #loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_wrist_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=WRIST_N_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_wrist_narrowing'\n",
        "                )\n",
        "\n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  mae = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "\n",
        "                #loss=mean_squared_error,\n",
        "                loss=mean_absolute_error,\n",
        "                #loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "SMOOTHING=0.0\n",
        "\n",
        "#Functions to apply on data before fold splitting and after fold splitting\n",
        "#Pre: downsampling most frequent class\n",
        "#Post: oversampling and normalizing labels to [0, 1]\n",
        "\n",
        "#def data_pre_sample(data, labels):\n",
        "#  return downsample_most_common_class(data, labels)\n",
        "\n",
        "def data_post_process(x_train, y_train, x_val, y_val, c_num, smoothing, downsample_valid = True):\n",
        "  x_train, y_train = downsample_most_common_class(x_train, y_train)\n",
        "  if downsample_valid:\n",
        "      x_val, y_val = downsample_most_common_class(x_val, y_val)\n",
        "\n",
        "  x_train, y_train = naive_oversampling(x_train, y_train)\n",
        "  x_val,   y_val   = naive_oversampling(x_val,   y_val)\n",
        "\n",
        "  y_train = normalize_labels(y_train, max_value = c_num - 1, smoothing=smoothing)\n",
        "  y_val   = normalize_labels(y_val,   max_value = c_num - 1, smoothing=smoothing)\n",
        "\n",
        "  return x_train, y_train, x_val, y_val\n",
        "\n",
        "#Function to yield folds of a given samples/labels pair, applying the functions defined above\n",
        "# '''def generate_folds(data, labels, scaling, label_smoothing, folds=3):\n",
        "#   data, labels = data_pre_sample(data, labels)\n",
        "#\n",
        "#   indices     = np.arange(len(labels))\n",
        "#   permutation = np.random.permutation(indices)\n",
        "#   permutation = list(permutation)\n",
        "#\n",
        "#   fold_indices = []\n",
        "#   fold_len     = len(labels) // folds\n",
        "#\n",
        "#   for i in range(folds):\n",
        "#     fold = None\n",
        "#\n",
        "#     if i < folds - 1:\n",
        "#       fold = np.array([ permutation.pop(-1) for i in range(fold_len) ])\n",
        "#     else:\n",
        "#       fold = permutation\n",
        "#\n",
        "#     fold_indices.append(np.array(fold))\n",
        "#\n",
        "#   for fold in fold_indices:\n",
        "#     val_indices   = fold\n",
        "#     train_indices = np.array(\n",
        "#         [ x for x in indices if x not in val_indices ]\n",
        "#     )\n",
        "#     train_x, train_y = data[train_indices], labels[train_indices]\n",
        "#     valid_x, valid_y = data[val_indices], labels[val_indices]\n",
        "#\n",
        "#     train_x, train_y, valid_x, valid_y = data_post_process(train_x, train_y, valid_x, valid_y, scaling, label_smoothing)\n",
        "#\n",
        "#     yield train_x, train_y, valid_x, valid_y'''\n",
        "\n",
        "def generate_folds(data, labels, scaling, label_smoothing, folds=3):\n",
        "  fold_separators = [ 1 - (1/( folds - i)) for i in range(folds - 1) ]\n",
        "  folds = []\n",
        "  for i, sep in enumerate(fold_separators):\n",
        "    data, labels, fold_x, fold_y = balanced_train_test_split(data, labels, sep, 'round')\n",
        "    folds.append((fold_x, fold_y))\n",
        "    if i == len(fold_separators) - 1:\n",
        "      folds.append((data, labels))\n",
        "\n",
        "  for i, fold in enumerate(folds):\n",
        "    train_x, train_y = [], []\n",
        "    [ ( train_x.append(folds[k][0]), train_y.append(folds[k][1]) ) for k in range(len(folds)) if k != i ]\n",
        "\n",
        "    train_x, train_y = np.array(train_x), np.array(train_y)\n",
        "    train_x, train_y = np.concatenate(train_x, axis=0), np.concatenate(train_y, axis=0)\n",
        "\n",
        "    yield data_post_process(train_x, train_y, fold[0], fold[1], scaling, label_smoothing)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "feet_erosion_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "feet_narrowing_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "hand_erosion_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "hand_narrowing_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "wrist_erosion_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "wrist_narrowing_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "#@title Visualize some augmented samples\n",
        "\n",
        "# def normalize_image(image):\n",
        "#   return (image - np.min(image))/(np.max(image) - np.min(image))\n",
        "#\n",
        "# def display_generator(gen, data, labels=None, n_cols=3, n_rows=2, scale = 3):\n",
        "#\n",
        "#   fig, ax = plt.subplots(figsize=(scale * n_cols, scale * n_rows), ncols=n_cols, nrows=n_rows)\n",
        "#\n",
        "#   samples = gen.flow(data, labels, batch_size=n_cols * n_rows).next()\n",
        "#\n",
        "#   print('Displaying images normalized to [0, 1] with mean', samples[0].mean(axis=(0,1,2)), 'and std', samples[0].std(axis=(0,1,2)))\n",
        "#\n",
        "#   for index, axis in enumerate(ax.flat):\n",
        "#     sample = samples[0][index] if labels is not None else samples[index]\n",
        "#     axis.imshow(normalize_image(sample))\n",
        "#     if labels is not None:\n",
        "#       label = samples[1][index]\n",
        "#       axis.set_xlabel(label)\n",
        "#\n",
        "#   plt.show()\n",
        "#\n",
        "# display_generator(feet_erosion_train_datagen,    *naive_oversampling(feet_joints, feet_erosion))\n",
        "# display_generator(hand_erosion_train_datagen,    *naive_oversampling(finger_erosion_images, finger_erosion))\n",
        "# display_generator(wrist_erosion_train_datagen,   *naive_oversampling(wrist_erosion_images, wrist_erosion))\n",
        "# display_generator(wrist_narrowing_train_datagen, *naive_oversampling(wrist_narrowing_images, wrist_narrowing))\n",
        "\n",
        "#@title Modular live loss plotter callback\n",
        "\n",
        "#Modular live loss plotter for Keras models\n",
        "#Allows to create custom layouts of per-batch or per-epoch plots for different metrics\n",
        "\n",
        "#Monitor class defines a plot, which either may be batch or epoch-scoped, and may contain several graphs\n",
        "#Batch monitor plots its values per batch, and refreshes itself on new epoch begin\n",
        "#Epoch monitor plots its values per epoch, and performs no refresh\n",
        "#All values/last N values displaying\n",
        "#Log-scale/Linear scale displaying\n",
        "\n",
        "#Plotter callback handles different Monitors and responds to the actual plotting\n",
        "#Defines a grid where Monitors will be drawn, grid size, refresh rate in batches\n",
        "#when the Monitors will be re-drawn in addition to per-epoch update\n",
        "#Plotter can be silenced to disable plotting and only archivate per-epoch data\n",
        "\n",
        "# from IPython.display import clear_output\n",
        "# from keras.callbacks import Callback\n",
        "# import matplotlib.pyplot as plt\n",
        "#\n",
        "# class Monitor():\n",
        "#     def __init__(self, scope='epoch', monitors= [ 'loss' ], plot_last=-1, log_scale=False, precision=4):\n",
        "#         self.scope = scope.lower()\n",
        "#         self.monitors = [ monitor.lower() for monitor in monitors ]\n",
        "#         self.plot_last = max(0, plot_last)\n",
        "#         self.x = []\n",
        "#         self.ys = [ [] for monitor in monitors ]\n",
        "#         self.log_scale = log_scale\n",
        "#         self.precision = precision\n",
        "#\n",
        "#     def reinit(self):\n",
        "#         self.x = []\n",
        "#         self.ys = [ [] for monitor in self.monitors ]\n",
        "#\n",
        "#     def update(self, iteration, logs={}):\n",
        "#         self.x.append(iteration)\n",
        "#\n",
        "#         for i, monitor in enumerate(self.monitors):\n",
        "#             if logs.get(monitor) is not None:\n",
        "#                 self.ys[i].append(logs.get(monitor))\n",
        "#             else:\n",
        "#                 pass #Action to execute when cannot get info for a certain monitor\n",
        "#\n",
        "#     def plot(self, axis):\n",
        "#         x_data = self.x[ -self.plot_last : ]\n",
        "#         y_array = [ y_data[ -self.plot_last : ] for y_data in self.ys ]\n",
        "#\n",
        "#         for i, y_data in enumerate(y_array):\n",
        "#             label = self.monitors[i] + '_' + self.scope #Compose graph name\n",
        "#             if self.log_scale:\n",
        "#                 axis.set_yscale('log') #Set up scale\n",
        "#\n",
        "#             if len(x_data) == len(y_data): #If data are coherent, plot them\n",
        "#                 axis.plot(x_data, y_data, label=label)\n",
        "#\n",
        "#                 if self.precision > 0 and len(y_data) > 0: #If there's a last point plotted, print its value\n",
        "#                     text = str(round(y_data[-1],  self.precision))\n",
        "#                     axis.text(x_data[-1], y_data[-1], text)\n",
        "#             else:\n",
        "#                 continue\n",
        "#\n",
        "#         label = {'batch' : 'Batches', 'epoch' : 'Epochs'} #Set up x-label\n",
        "#         axis.set_xlabel(label[self.scope])\n",
        "#\n",
        "#         axis.legend()\n",
        "#\n",
        "#\n",
        "# class Plotter(Callback):\n",
        "#     def __init__(self, scale=5, n_cols=2, n_rows=1, monitors=[], refresh_rate=-1, silent=False):\n",
        "#         if (n_cols * n_rows < len(monitors)):\n",
        "#             raise ValueError('Grid is too small to fit all monitors!')\n",
        "#\n",
        "#         self.n_cols = n_cols\n",
        "#         self.n_rows = n_rows\n",
        "#         self.scale = scale\n",
        "#\n",
        "#         self.monitors = monitors\n",
        "#\n",
        "#         self.batch_monitors, self.epoch_monitors = [], []\n",
        "#\n",
        "#         for monitor in monitors:\n",
        "#             if monitor.scope == 'epoch':\n",
        "#                 self.epoch_monitors.append(monitor)\n",
        "#             elif monitor.scope == 'batch':\n",
        "#                 self.batch_monitors.append(monitor)\n",
        "#\n",
        "#         self.refresh_rate = refresh_rate\n",
        "#         self.silent = False\n",
        "#\n",
        "#     def on_train_begin(self, logs={}):\n",
        "#         pass\n",
        "#\n",
        "#     def on_epoch_begin(self, epoch, logs={}):\n",
        "#         [ monitor.reinit() for monitor in self.batch_monitors ]\n",
        "#\n",
        "#     def plot(self):\n",
        "#         clear_output(wait=True)\n",
        "#\n",
        "#         figsize = ( self.scale * self.n_cols, self.scale * self.n_rows)\n",
        "#         fig, ax = plt.subplots(figsize=figsize, ncols=self.n_cols, nrows=self.n_rows)\n",
        "#\n",
        "#         if self.n_cols * self.n_rows == 1:\n",
        "#           ax = np.array([ax])\n",
        "#\n",
        "#         for index, axis in enumerate(ax.flat):\n",
        "#           if index < len(self.monitors):\n",
        "#               self.monitors[index].plot(axis)\n",
        "#\n",
        "#         plt.show()\n",
        "#\n",
        "#     def on_batch_end(self, batch, logs={}):\n",
        "#         [ monitor.update(batch, logs) for monitor in self.batch_monitors ]\n",
        "#\n",
        "#         if self.silent or batch == 0 or self.refresh_rate <= 0 or batch % self.refresh_rate != 0:\n",
        "#             return\n",
        "#\n",
        "#         self.plot()\n",
        "#\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         [ monitor.update(epoch, logs) for monitor in self.epoch_monitors ]\n",
        "#\n",
        "#         if self.silent:\n",
        "#             return\n",
        "#\n",
        "#         self.plot()\n",
        "#\n",
        "#     def reinit(self):\n",
        "#       [ monitor.reinit() for monitor in self.monitors ]\n",
        "\n",
        "#@title Batch sizes and step counts\n",
        "\n",
        "def calculate_parameters(image_size, train_samples, validation_samples):\n",
        "  batch_size     = 512 // (image_size // 32) ** 2\n",
        "  val_batch_size = min(\n",
        "      validation_samples,\n",
        "      512 // (image_size // 64) ** 2\n",
        "  )\n",
        "  steps_per_epoch = max(\n",
        "    1, round(train_samples / batch_size)\n",
        "  )\n",
        "  validation_steps = max(\n",
        "      1, round(validation_samples / val_batch_size)\n",
        "  )\n",
        "  return {\n",
        "    'batch_size'       : batch_size,\n",
        "    'val_batch_size'   : val_batch_size,\n",
        "    'steps_per_epoch'  : steps_per_epoch,\n",
        "    'validation_steps' : validation_steps,\n",
        "  }\n",
        "\n",
        "#@title Custom metrics callback\n",
        "#Validates model on a given data generator\n",
        "\n",
        "#from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "#class MultiOutputMetric(Callback):\n",
        "#  def __init__(self, output_names, metric_name, output_metric_name = None):\n",
        "#    self.output_names = output_names\n",
        "#    self.metric_name = metric_name\n",
        "#    self.output_metric_name = output_metric_name if output_metric_name else metric_name\n",
        "\n",
        "#  def on_epoch_end(self, epoch, logs={}):\n",
        "#   metric_value = [ logs['val_' + name + '_' + self.metric_name] for name in self.output_names ]\n",
        "#   metric_value = sum(metric_value)\n",
        "\n",
        "#   logs[self.output_metric_name] = metric_value\n",
        "\n",
        "#@title Auxilary callback functions definition\n",
        "\n",
        "#from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def exp_schedule(base_lr = 0.001, momentum=0.995):\n",
        "  return lambda x: base_lr * momentum ** x\n",
        "\n",
        "def sine_schedule(min_lr=2.5e-5, max_lr=1e-4, period=200, phase=0):\n",
        "  base_lr = (min_lr + max_lr) / 2\n",
        "  amp = (max_lr - min_lr) / 2\n",
        "  return lambda x: (base_lr + amp * np.sin(np.pi * 2 / period * x + phase))\n",
        "\n",
        "\n",
        "#MONITORS = [\n",
        "#    Monitor(scope='epoch', monitors = ['loss', 'val_loss'], plot_last=96),\n",
        "\n",
        "#    Monitor(scope='epoch', monitors = ['mean_squared_error', 'val_mean_squared_error'], precision=3, plot_last=32),\n",
        "#    Monitor(scope='epoch', monitors = ['lr'], log_scale=False, precision=7), #lr represents custom metric defined to watch the learning rate\n",
        "\n",
        "#    Monitor(scope='epoch', monitors = ['loss', 'val_loss']),\n",
        "\n",
        "#    Monitor(scope='epoch', monitors = ['mae', 'val_mae'], precision=4),\n",
        "#    Monitor(scope='epoch', monitors = ['mse', 'val_mse'], precision=4),\n",
        "\n",
        "#]\n",
        "\n",
        "SCHEDULE = exp_schedule(base_lr=1e-4, momentum=1.0)\n",
        "#schedule = sine_schedule(min_lr =2.5e-5, max_lr=1.25e-4, period=50, phase= -np.pi / 2)\n",
        "\n",
        "#plotter = Plotter(monitors=monitors, n_rows=2, n_cols=3, scale=6, refresh_rate=-1)\n",
        "\n",
        "#LR_SCHEDULER = LearningRateScheduler(SCHEDULE)\n",
        "#custom_validator = CustomMetricValidator(validation_generator_factory(), steps=validation_steps)\n",
        "\n",
        "# ax = get_ax()\n",
        "# ax.plot([ SCHEDULE(i) for i in range(150) ])\n",
        "# ax.set_xlabel('Epoch')\n",
        "# ax.set_ylabel('Learning rate')\n",
        "\n",
        "#@title Define an object to build training fronts and organize trained models\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, Callback\n",
        "\n",
        "class WeightReverter(Callback):\n",
        "  def __init__(self, stopper):\n",
        "    self.stopper = stopper\n",
        "\n",
        "  def on_train_end(self, logs={}):\n",
        "    if self.stopper.best_weights:\n",
        "      print('Reverting model to the best weights')\n",
        "      self.model.set_weights(self.stopper.best_weights)\n",
        "\n",
        "\n",
        "class Saver(Callback):\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "\n",
        "  def on_train_end(self, logs={}):\n",
        "    print('Saving model to', self.path)\n",
        "    self.model.save(self.path)\n",
        "\n",
        "def callback_factory(output_path, monitor, mode='min'):\n",
        "\n",
        "  checkpoint = ModelCheckpoint(\n",
        "      output_path,\n",
        "      monitor=monitor,\n",
        "      verbose=1,\n",
        "      save_best_only=True,\n",
        "      save_weights_only=False,\n",
        "      mode=mode,\n",
        "      period=EPOCHS\n",
        "  )\n",
        "\n",
        "  stopper = EarlyStopping(monitor=monitor, patience=PATIENCE, verbose=1, mode=mode, restore_best_weights=True)\n",
        "  indifferent_stopper = EarlyStopping(monitor=monitor, patience=EPOCHS, verbose=1, mode=mode, restore_best_weights=True)\n",
        "\n",
        "  saver = Saver(output_path)\n",
        "\n",
        "  scheduler = LearningRateScheduler(SCHEDULE)\n",
        "\n",
        "  # monitors = [\n",
        "  #     Monitor(scope='epoch', monitors = ['loss', 'val_loss'], plot_last=96),\n",
        "  #\n",
        "  #     Monitor(scope='epoch', monitors = ['mean_squared_error', 'val_mean_squared_error'], precision=3, plot_last=32),\n",
        "  #     Monitor(scope='epoch', monitors = ['lr'], log_scale=False, precision=7), #lr represents custom metric defined to watch the learning rate\n",
        "  #\n",
        "  #     Monitor(scope='epoch', monitors = ['loss', 'val_loss']),\n",
        "  #\n",
        "  #     Monitor(scope='epoch', monitors = ['mae', 'val_mae'], precision=4),\n",
        "  #     Monitor(scope='epoch', monitors = ['mse', 'val_mse'], precision=4),\n",
        "  #\n",
        "  # ]\n",
        "  #\n",
        "  # plotter = Plotter(monitors=monitors, n_rows=2, n_cols=3, scale=6, refresh_rate=-1)\n",
        "\n",
        "  callbacks = [ scheduler ]\n",
        "\n",
        "  if SAVE_MODELS:\n",
        "      if not EARLY_STOPPING: callbacks = [ checkpoint ] + callbacks\n",
        "      if EARLY_STOPPING: callbacks = [ stopper, WeightReverter(stopper), saver ] + callbacks\n",
        "  else:\n",
        "      if not EARLY_STOPPING: callbacks = [ indifferent_stopper, WeightReverter(indifferent_stopper) ] + callbacks\n",
        "      if EARLY_STOPPING: callbacks = [ stopper, WeightReverter(stopper) ] + callbacks\n",
        "  #if include_plotter: callbacks = [ plotter ] + callbacks\n",
        "\n",
        "  return callbacks\n",
        "\n",
        "class TrainingFront():\n",
        "  def __init__(self):\n",
        "    self.front = []\n",
        "    self.histories = []\n",
        "    self.ensembles = dict()\n",
        "\n",
        "    self.saved_models_paths = []\n",
        "    self.saved_models_by_ensemble = dict()\n",
        "\n",
        "  def append(self, model_factory, fold_generator, image_generators, callback_factory, ensemble = None):\n",
        "    self.front.append((model_factory, fold_generator, image_generators, callback_factory, ensemble))\n",
        "\n",
        "  def train(self):\n",
        "    for model_factory, fold_gen, img_gens, callback_factory, ensemble in self.front:\n",
        "      train_datagen, valid_datagen = img_gens\n",
        "\n",
        "      for i, data in enumerate(fold_gen):\n",
        "        #Process data, create a model through a given factory and compute training parameters\n",
        "        train_x, train_y, valid_x, valid_y = data\n",
        "        model = model_factory()\n",
        "        parameters = calculate_parameters(train_x.shape[1], train_x.shape[0], valid_x.shape[0])\n",
        "\n",
        "        batch_size       = parameters['batch_size']\n",
        "        valid_batch_size = parameters['val_batch_size']\n",
        "        validation_steps = parameters['validation_steps']\n",
        "\n",
        "        #Compose model name and checkpoint output path\n",
        "        model_name = model.name + '_fold_' + str(i)\n",
        "        output_path = MODEL_OUTPUT_PATH + model_name + '.h5'\n",
        "\n",
        "        #Get callbacks through factory\n",
        "        callback_list = callback_factory(output_path, 'val_mse', 'min')\n",
        "\n",
        "        #Log the training start\n",
        "        print('Model ', model_name, ' training started with parameters: epochs = ', EPOCHS,\n",
        "              ', batch_size = ', batch_size, ', valid_batch_size = ', valid_batch_size,\n",
        "              ', validation_steps = ', validation_steps, ', early stopping = ', EARLY_STOPPING, sep='')\n",
        "\n",
        "        print('Data shapes:', train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
        "\n",
        "        #Train the model\n",
        "        history = model.fit_generator(\n",
        "            train_datagen.flow(train_x, train_y, batch_size=batch_size, shuffle=True),\n",
        "            validation_data = val_datagen.flow(valid_x, valid_y, batch_size=valid_batch_size, shuffle=False),\n",
        "            validation_steps= validation_steps,\n",
        "            epochs          = EPOCHS,\n",
        "            callbacks       = callback_list,\n",
        "            verbose         = VERBOSE,\n",
        "        )\n",
        "        self.histories.append(history)\n",
        "        self.saved_models_paths.append(output_path)\n",
        "\n",
        "        #If an ensemble family defined, add to the corresponding list\n",
        "        if ensemble is not None:\n",
        "          if ensemble not in self.ensembles.keys():\n",
        "            self.ensembles[ensemble] = []\n",
        "          if ensemble not in self.saved_models_by_ensemble.keys():\n",
        "            self.saved_models_by_ensemble[ensemble] = []\n",
        "          self.ensembles[ensemble].append(model)\n",
        "          self.saved_models_by_ensemble[ensemble].append(output_path)\n",
        "\n",
        "\n",
        "#@title Define training fronts\n",
        "\n",
        "short_front = TrainingFront()\n",
        "\n",
        "short_front.append(\n",
        "    model_factory    = lambda: get_feet_erosion_estimator(dropout_rate=0.75, regularizer_factory=lambda: l2(1e-4)),\n",
        "    fold_generator   = generate_folds(feet_joints, feet_erosion, FEET_EROSION_SCALING, SMOOTHING, FOLDS),\n",
        "    image_generators = (feet_erosion_train_datagen, val_datagen),\n",
        "    callback_factory = callback_factory,\n",
        "    ensemble         = 'FEET_EROSION',\n",
        ")\n",
        "\n",
        "# short_front.append(\n",
        "#     model_factory    = lambda: get_feet_narrowing_estimator(dropout_rate=0.875, regularizer_factory=lambda: l2(1e-4)),\n",
        "#     fold_generator   = generate_folds(feet_joints, feet_narrowing, FEET_NARROWING_SCALING, SMOOTHING, FOLDS),\n",
        "#     image_generators = (feet_narrowing_train_datagen, val_datagen),\n",
        "#     callback_factory = callback_factory,\n",
        "#     ensemble         = 'FEET_NARROWING',\n",
        "# )\n",
        "\n",
        "short_front.append(\n",
        "    model_factory    = lambda: get_hand_erosion_estimator(dropout_rate=0.875, regularizer_factory=lambda: l2(1e-4)),\n",
        "    fold_generator   = generate_folds(\n",
        "                            np.concatenate([ finger_erosion_images, wrist_erosion_images ], axis=0),\n",
        "                            np.concatenate([ finger_erosion, wrist_erosion ], axis=0),\n",
        "                            HAND_EROSION_SCALING, SMOOTHING, FOLDS\n",
        "                        ),\n",
        "    image_generators = (hand_erosion_train_datagen, val_datagen),\n",
        "    callback_factory = callback_factory,\n",
        "    ensemble         = 'HAND_EROSION',\n",
        ")\n",
        "\n",
        "# short_front.append(\n",
        "#     model_factory   = lambda: get_hand_narrowing_estimator(dropout_rate=0.875, regularizer_factory=lambda: l2(1e-4)),\n",
        "#     fold_generator  = generate_folds(\n",
        "#                             np.concatenate([ finger_narrowing_images, wrist_narrowing_images ], axis=0),\n",
        "#                             np.concatenate([ finger_narrowing, wrist_narrowing ], axis=0),\n",
        "#                             HAND_NARROWING_SCALING, SMOOTHING, FOLDS\n",
        "#                       ),\n",
        "#    image_generators = (hand_narrowing_train_datagen, val_datagen),\n",
        "#    callback_factory = callback_factory,\n",
        "#    ensemble         = 'HAND_NARROWING',\n",
        "# )\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "short_front.append(\n",
        "    model_factory   = lambda: get_hand_narrowing_estimator(dropout_rate=0.875, regularizer_factory=lambda: l2(1e-4)),\n",
        "    fold_generator  = generate_folds(\n",
        "                            np.concatenate([ finger_narrowing_images, wrist_narrowing_images, feet_joints ], axis=0),\n",
        "                            np.concatenate([ finger_narrowing, wrist_narrowing, feet_narrowing ], axis=0),\n",
        "                            HAND_NARROWING_SCALING, SMOOTHING, FOLDS\n",
        "                      ),\n",
        "   image_generators = (hand_narrowing_train_datagen, val_datagen),\n",
        "   callback_factory = callback_factory,\n",
        "   ensemble         = 'HAND_NARROWING',\n",
        ")\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "\n",
        "print('SMOOTHING =', SMOOTHING, 'FOLDS =', FOLDS)\n",
        "short_front.train()\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "short_front.ensembles['FEET_NARROWING'] = short_front.ensembles['HAND_NARROWING']\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def read_model(path):\n",
        "  def placeholder(placeholder1, placeholder2):\n",
        "    return K.constant(-1.0, np.float32)\n",
        "\n",
        "  custom_objects = {        #The loaded model is not mentioned to be used for training\n",
        "      'mae' : placeholder,  #so I don't need auxilary metrics\n",
        "      'mse' : placeholder,\n",
        "      'lr'  : placeholder,\n",
        "  }\n",
        "\n",
        "  return keras.models.load_model(path, custom_objects)\n",
        "\n",
        "def read_ensembles(front):\n",
        "  paths = front.saved_models_by_ensemble\n",
        "  ensembles = dict()\n",
        "  for ensemble, path_list in paths.items():\n",
        "    ensembles[ensemble] = []\n",
        "\n",
        "    for path in path_list:\n",
        "      print('Loading model:', path)\n",
        "      model = read_model(path)\n",
        "      print('Model', path, 'loaded')\n",
        "\n",
        "      ensembles[ensemble].append(model)\n",
        "  return ensembles\n",
        "\n",
        "def evaluate_ensemble(ensemble, input_image):\n",
        "  results = []\n",
        "  for model in ensemble:\n",
        "    input_tensor = input_image[np.newaxis, :]\n",
        "    gen = val_datagen.flow(input_tensor, batch_size=1, shuffle=False)\n",
        "\n",
        "    results.append(\n",
        "        model.predict_generator(gen).flat[0]\n",
        "    )\n",
        "\n",
        "  return np.mean(np.array(results))\n",
        "\n",
        "def evaluate_ensemble_batch(ensemble, input_tensor):\n",
        "  results = []\n",
        "  for model in ensemble:\n",
        "    gen = val_datagen.flow(input_tensor, batch_size = input_tensor.shape[0], shuffle=False)\n",
        "\n",
        "    results.append(\n",
        "        model.predict_generator(gen)[:, 0]\n",
        "    )\n",
        "\n",
        "  results = np.array(results)\n",
        "  results = np.mean(results, axis=0)\n",
        "  return results\n",
        "\n",
        "def evaluate_hands(hand, right, erosion_ensemble, narrowing_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  E_COLUMNS = LH_FINGER_EROSION_REGION_NAMES   if not right else RH_FINGER_EROSION_REGION_NAMES\n",
        "  N_COLUMNS = LH_FINGER_NARROWING_REGION_NAMES if not right else RH_FINGER_NARROWING_REGION_NAMES\n",
        "\n",
        "  erosion_columns = []\n",
        "  erosion_images  = []\n",
        "\n",
        "  [ (erosion_columns.append(E_COLUMNS[ region_class - 1 ]), erosion_images.append(image)) for region_class, image in hand.items() ]\n",
        "  erosion_images = np.array([ cv2.resize(image, FINGER_REGION_IMAGE_SHAPE[:2]) for image in erosion_images ])\n",
        "\n",
        "  narrowing_columns = []\n",
        "  narrowing_images  = []\n",
        "\n",
        "  [ (narrowing_columns.append(N_COLUMNS[ region_class - 2 ]), narrowing_images.append(image)) for region_class, image in hand.items() if region_class > 1 ]\n",
        "  narrowing_images = np.array([ cv2.resize(image, FINGER_REGION_IMAGE_SHAPE[:2]) for image in narrowing_images ])\n",
        "\n",
        "  erosion_scores   = evaluate_ensemble_batch(erosion_ensemble, erosion_images)\n",
        "  narrowing_scores = evaluate_ensemble_batch(narrowing_ensemble, narrowing_images)\n",
        "\n",
        "  erosion_scores = denormalize_labels(erosion_scores, HAND_EROSION_SCALING - 1, SMOOTHING)\n",
        "  narrowing_scores = denormalize_labels(narrowing_scores, HAND_NARROWING_SCALING - 1, SMOOTHING)\n",
        "\n",
        "  assert len(erosion_columns) == len(erosion_scores)\n",
        "  assert len(narrowing_columns) == len(narrowing_scores)\n",
        "  #assert len(narrowing_columns) > 0 and len(erosion_columns) > 0\n",
        "\n",
        "  [ results.update({ column : value }) for column, value in zip(erosion_columns, erosion_scores) ]\n",
        "  [ results.update({ column : value }) for column, value in zip(narrowing_columns, narrowing_scores) ]\n",
        "\n",
        "  return results\n",
        "\n",
        "def evaluate_feet(foot, right, erosion_ensemble, narrowing_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  E_COLUMNS = LF_EROSION_REGION_NAMES   if not right else RF_EROSION_REGION_NAMES\n",
        "  N_COLUMNS = LF_NARROWING_REGION_NAMES if not right else RF_NARROWING_REGION_NAMES\n",
        "\n",
        "  erosion_columns   = []\n",
        "  narrowing_columns = []\n",
        "  images            = []\n",
        "\n",
        "  [ (erosion_columns.append(E_COLUMNS[ region_class - 1 ]), narrowing_columns.append(N_COLUMNS[ region_class - 1 ]),\n",
        "     images.append(image)) for region_class, image in foot.items() ]\n",
        "\n",
        "  images = np.array([cv2.resize(image, FEET_REGION_IMAGE_SHAPE[:2]) for image in images])\n",
        "\n",
        "  erosion_scores   = evaluate_ensemble_batch(erosion_ensemble, images)\n",
        "  narrowing_scores = evaluate_ensemble_batch(narrowing_ensemble, images)\n",
        "\n",
        "  erosion_scores = denormalize_labels(erosion_scores, FEET_EROSION_SCALING - 1, SMOOTHING)\n",
        "  narrowing_scores = denormalize_labels(narrowing_scores, FEET_NARROWING_SCALING - 1, SMOOTHING)\n",
        "\n",
        "  assert len(erosion_columns) == len(erosion_scores)\n",
        "  assert len(narrowing_columns) == len(narrowing_scores)\n",
        "  #assert len(narrowing_columns) > 0 and len(erosion_columns) > 0\n",
        "\n",
        "  [ results.update({ column : value }) for column, value in zip(erosion_columns, erosion_scores) ]\n",
        "  [ results.update({ column : value }) for column, value in zip(narrowing_columns, narrowing_scores) ]\n",
        "\n",
        "  return results\n",
        "\n",
        "def evaluate_wrist_erosion(wrist, right, erosion_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  E_COLUMNS = LH_WRIST_EROSION_REGION_NAMES if not right else RH_WRIST_EROSION_REGION_NAMES\n",
        "\n",
        "  columns = []\n",
        "  images  = []\n",
        "\n",
        "  [ (columns.append(E_COLUMNS[ region_class - 1 ]), images.append(image)) for region_class, image in wrist.items() ]\n",
        "  images = np.array([ cv2.resize(image, WRIST_E_REGION_IMAGE_SHAPE[:2]) for image in images ])\n",
        "\n",
        "  score = evaluate_ensemble_batch(erosion_ensemble, images)\n",
        "  score = denormalize_labels(score, HAND_EROSION_SCALING - 1, SMOOTHING)\n",
        "\n",
        "  assert len(columns) == len(score)\n",
        "  #assert len(columns) > 0\n",
        "\n",
        "  [ results.update({ column : value }) for column, value in zip(columns, score) ]\n",
        "\n",
        "  return results\n",
        "\n",
        "def evaluate_wrist_narrowing(wrist, right, narrowing_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  N_COLUMNS = LH_WRIST_NARROWING_REGION_NAMES if not right else RH_WRIST_NARROWING_REGION_NAMES\n",
        "\n",
        "  columns = []\n",
        "  images  = []\n",
        "\n",
        "  [ (columns.append(N_COLUMNS[ region_class - 1 ]), images.append(image)) for region_class, image in wrist.items() ]\n",
        "  images = np.array([ cv2.resize(image, WRIST_N_REGION_IMAGE_SHAPE[:2]) for image in images ])\n",
        "\n",
        "  score = evaluate_ensemble_batch(narrowing_ensemble, images)\n",
        "  score = denormalize_labels(score, HAND_NARROWING_SCALING - 1, SMOOTHING)\n",
        "\n",
        "  assert len(columns) == len(score)\n",
        "  #assert len(columns) > 0\n",
        "\n",
        "  [ results.update({ column : value }) for column, value in zip(columns, score) ]\n",
        "\n",
        "  return results\n",
        "\n",
        "def compose_results_and_fill_empties(LH, RH, LF, RF, LWE, RWE, LWN, RWN):\n",
        "  #Extract evaluations per category to later combine them and compute mean values\n",
        "  #which will be used to fill empties\n",
        "\n",
        "  LHE = [ LH[name] for name in LH_FINGER_EROSION_REGION_NAMES if name in LH.keys() ]\n",
        "  RHE = [ RH[name] for name in RH_FINGER_EROSION_REGION_NAMES if name in RH.keys() ]\n",
        "\n",
        "  LHN = [ LH[name] for name in LH_FINGER_NARROWING_REGION_NAMES if name in LH.keys() ]\n",
        "  RHN = [ RH[name] for name in RH_FINGER_NARROWING_REGION_NAMES if name in RH.keys() ]\n",
        "\n",
        "  LWE_NUM = [ LWE[name] for name in LH_WRIST_EROSION_REGION_NAMES if name in LWE.keys() ]\n",
        "  RWE_NUM = [ RWE[name] for name in RH_WRIST_EROSION_REGION_NAMES if name in RWE.keys() ]\n",
        "\n",
        "  LWN_NUM = [ LWN[name] for name in LH_WRIST_NARROWING_REGION_NAMES if name in LWN.keys() ]\n",
        "  RWN_NUM = [ RWN[name] for name in RH_WRIST_NARROWING_REGION_NAMES if name in RWN.keys() ]\n",
        "\n",
        "  LFE = [ LF[name] for name in LF_EROSION_REGION_NAMES if name in LF.keys() ]\n",
        "  RFE = [ RF[name] for name in RF_EROSION_REGION_NAMES if name in RF.keys() ]\n",
        "\n",
        "  LFN = [ LF[name] for name in LF_NARROWING_REGION_NAMES if name in LF.keys() ]\n",
        "  RFN = [ RF[name] for name in RF_NARROWING_REGION_NAMES if name in RF.keys() ]\n",
        "\n",
        "  HAND_E_MEAN = np.array(LHE + RHE + LWE_NUM + RWE_NUM).mean()\n",
        "  HAND_N_MEAN = np.array(LHN + RHN + LWN_NUM + RWN_NUM).mean()\n",
        "\n",
        "  FEET_E_MEAN = np.array(LFE + RFE).mean()\n",
        "  FEET_N_MEAN = np.array(LFN + RFN).mean()\n",
        "\n",
        "  results = dict()\n",
        "  [ results.update(d) for d in [ LH, RH, LF, RF, LWE, RWE, LWN, RWN ] ] #Fill dict with values evaluated\n",
        "\n",
        "  HAND_E_NAMES = LH_FINGER_EROSION_REGION_NAMES + LH_WRIST_EROSION_REGION_NAMES + RH_FINGER_EROSION_REGION_NAMES + RH_WRIST_EROSION_REGION_NAMES\n",
        "  HAND_N_NAMES = LH_FINGER_NARROWING_REGION_NAMES + LH_WRIST_NARROWING_REGION_NAMES + RH_FINGER_NARROWING_REGION_NAMES + RH_WRIST_NARROWING_REGION_NAMES\n",
        "\n",
        "  FEET_E_NAMES = LF_EROSION_REGION_NAMES + RF_EROSION_REGION_NAMES\n",
        "  FEET_N_NAMES = LF_NARROWING_REGION_NAMES + RF_NARROWING_REGION_NAMES\n",
        "\n",
        "  [ results.update({ name : HAND_E_MEAN }) for name in HAND_E_NAMES if name not in results.keys() ]\n",
        "  [ results.update({ name : HAND_N_MEAN }) for name in HAND_N_NAMES if name not in results.keys() ]\n",
        "  [ results.update({ name : FEET_E_MEAN }) for name in FEET_E_NAMES if name not in results.keys() ]\n",
        "  [ results.update({ name : FEET_N_MEAN }) for name in FEET_N_NAMES if name not in results.keys() ]\n",
        "\n",
        "  Overall_erosion = np.array([results[name] for name in HAND_E_NAMES + FEET_E_NAMES ]).sum()\n",
        "  Overall_narrowing = np.array([results[name] for name in HAND_N_NAMES + FEET_N_NAMES ]).sum()\n",
        "  Overall_Tol = Overall_erosion + Overall_narrowing\n",
        "\n",
        "  results.update({\n",
        "      'Overall_erosion' : Overall_erosion,\n",
        "      'Overall_narrowing' : Overall_narrowing,\n",
        "      'Overall_Tol' : Overall_Tol,\n",
        "  })\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def test(dataframe_path, output_path, images_path, ensembles):\n",
        "  data = read_training_dataset(dataframe_path, images_path)\n",
        "  output_df = pd.read_csv(dataframe_path)\n",
        "  evaluations = dict()\n",
        "\n",
        "  FEET_E = ensembles['FEET_EROSION']\n",
        "  FEET_N = ensembles['FEET_NARROWING']\n",
        "  HAND_E = ensembles['HAND_EROSION']\n",
        "  HAND_N = ensembles['HAND_NARROWING']\n",
        "\n",
        "  WRIST_E = ensembles['WRIST_EROSION'] if 'WRIST_EROSION' in ensembles.keys() else HAND_E\n",
        "  WRIST_N = ensembles['WRIST_NARROWING'] if 'WRIST_NARROWING' in ensembles.keys() else HAND_N\n",
        "\n",
        "  for p_id, regions in data.items():\n",
        "    p_data = data[p_id]\n",
        "    LH, RH = p_data['LH'], p_data['RH']\n",
        "    LF, RF = p_data['LF'], p_data['RF']\n",
        "    LWE, RWE = p_data['LWE'], p_data['RWE']\n",
        "    LWN, RWN = p_data['LWN'], p_data['RWN']\n",
        "\n",
        "    print('Evaluating patient', p_id)\n",
        "\n",
        "    LH, RH = evaluate_hands(LH, False, HAND_E, HAND_N), evaluate_hands(RH, True, HAND_E, HAND_N)\n",
        "    LF, RF = evaluate_feet(LF, False, FEET_E, FEET_N), evaluate_feet(RF, True, FEET_E, FEET_N)\n",
        "    LWE, RWE = evaluate_wrist_erosion(LWE, False, WRIST_E), evaluate_wrist_erosion(RWE, True, WRIST_E)\n",
        "    LWN, RWN = evaluate_wrist_narrowing(LWN, False, WRIST_N), evaluate_wrist_narrowing(RWN, True, WRIST_N)\n",
        "    evaluations[p_id] = compose_results_and_fill_empties(LH, RH, LF, RF, LWE, RWE, LWN, RWN)\n",
        "\n",
        "  for index, row in output_df.iterrows():\n",
        "    p_id = row['Patient_ID']\n",
        "    for column in output_df.columns:\n",
        "      if column != 'Patient_ID':\n",
        "        output_df.loc[index, column] = evaluations[p_id][column]\n",
        "\n",
        "  output_df.to_csv(output_path, index=False)\n",
        "\n",
        "trained_models = read_ensembles(short_front) if SAVE_MODELS else short_front.ensembles\n",
        "test(TEST_DATAFRAME_PATH, OUTPUT_DATAFRAME_PATH, TEST_SET_PATH, trained_models)\n",
        "\n",
        "\"\"\"#TODO: TEST MODEL RELOADING\n",
        "#TODO: TEST PREPROCESSING\n",
        "#----\n",
        "#TODO: Change paths\n",
        "#----\n",
        "#TODO: Check the estimator definitions\n",
        "#TODO: Check the data set parsing\n",
        "#TODO: Test empty wrist\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}