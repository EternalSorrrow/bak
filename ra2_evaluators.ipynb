{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ra2_evaluators.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOLxmxzR56Q+EbekntVhClM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EternalSorrrow/bak/blob/master/ra2_evaluators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7utoiErS35Vg",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "1003aa8c-ba15-4d73-a99f-239bef097fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "!pip3 uninstall tensorflow\n",
        "!pip3 install tensorflow==1.15\n",
        "\n",
        "!git clone https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "%cd Mask_RCNN\n",
        "!python setup.py install\n",
        "!pip show mask-rcnn\n",
        "\n",
        "%cd ..\n",
        "#!pip3 install imgaug"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "101_aZ1s3-9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Mount the Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QmMqlSLCDxP",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import Mask R-CNN dependencies\n",
        "\n",
        "%cd Mask_RCNN/\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n",
        "import mrcnn\n",
        "from mrcnn.utils import Dataset\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "from os import listdir\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4MjaFGV4Rf2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Configurations for pre-trained joint extractors\n",
        "\n",
        "max_instances_to_detect = 128\n",
        "\n",
        "FEET_REGIONS = 6\n",
        "HAND_REGIONS = 11\n",
        "WRIST_REGIONS = 6\n",
        "\n",
        "MODEL_DIR = 'logs'\n",
        "\n",
        "class FeetJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"FeetJoints_config\"\n",
        " \n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        " \n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = FEET_REGIONS + 1\n",
        "   \n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = 1\n",
        "    #VALIDATION_STEPS = 1\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "    \n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "    \n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=FEET_REGIONS\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "\n",
        "class HandJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"HandJoints_config\"\n",
        " \n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        " \n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = HAND_REGIONS + 1\n",
        "   \n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = max(1, train_samples // IMAGES_PER_GPU)\n",
        "    #VALIDATION_STEPS = max(1, val_samples // IMAGES_PER_GPU)\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "    \n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "    \n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=HAND_REGIONS\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "class WristJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"WristJoints_config\"\n",
        " \n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        " \n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = WRIST_REGIONS + 1\n",
        "   \n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = max(1, train_samples // IMAGES_PER_GPU)\n",
        "    #VALIDATION_STEPS = max(1, val_samples // IMAGES_PER_GPU)\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 256\n",
        "    IMAGE_MAX_DIM = 256\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    MEAN_PIXEL = [117.8, 117.8, 117.8]\n",
        "    USE_MINI_MASK = False\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "    \n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "    \n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=6\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "\n",
        "f_config = FeetJointsConfig()\n",
        "h_config = HandJointsConfig()\n",
        "w_config = WristJointsConfig()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gXvah7Y-nkT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Paths definition\n",
        "\n",
        "TRAIN_PATH      = 'drive/My Drive/Work/ML/RA2/ra2/train/'\n",
        "DATAFRAME_PATH  = 'drive/My Drive/Work/ML/RA2/ra2/train/training.csv'\n",
        "\n",
        "PUBLIC_SET_PATH = ''\n",
        "PUBLIC_DATAFRAME_PATH = ''\n",
        "\n",
        "TEST_SET_PATH = 'drive/My Drive/Work/ML/RA2/ra2/example_input/'\n",
        "TEST_DATAFRAME_PATH = 'drive/My Drive/Work/ML/RA2/ra2/example_input/template.csv'\n",
        "\n",
        "MODEL_OUTPUT_PATH = 'drive/My Drive/Work/ML/RA2/ra2/example_output/'\n",
        "\n",
        "HAND_DETECTOR_PATH    = '/content/drive/My Drive/Work/ML/RA2/ra2/hands_subset/weights/model_6/mrcnn_hand_mrcnn_class_loss_best-200.hdf5'\n",
        "FEET_DETECTOR_PATH    = '/content/drive/My Drive/Work/ML/RA2/ra2/feet_subset/weights/model_5/mrcnn_feet_mrcnn_class_loss_best-160.hdf5'\n",
        "WRIST_E_DETECTOR_PATH = '/content/drive/My Drive/Work/ML/RA2/ra2/wrist_subset/erosion/mrcnn_we_loss_best-320.hdf5'\n",
        "WRIST_N_DETECTOR_PATH = '/content/drive/My Drive/Work/ML/RA2/ra2/wrist_subset/narrowing/weights/model1/mrcnn_wn_mrcnn_class_loss_best-320.hdf5'\n",
        "\n",
        "DENSENET121_WEIGHT_PATH_NO_TOP = '/content/drive/My Drive/Work/ML/RA2/other/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "DENSENET169_WEIGHT_PATH_NO_TOP = '/content/drive/My Drive/Work/ML/RA2/other/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "DENSENET201_WEIGHT_PATH_NO_TOP = '/content/drive/My Drive/Work/ML/RA2/other/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evFYNpiJ-hwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Image sizes setup\n",
        "\n",
        "HAND_REGION_SCALE = 1.2\n",
        "FEET_REGION_SCALE = 1.2\n",
        "\n",
        "WRIST_E_REGION_SCALE = 1.2\n",
        "WRIST_N_REGION_SCALE = 1.2\n",
        "\n",
        "FEET_REGION_IMAGE_SHAPE    = (64,  64,  3)\n",
        "FINGER_REGION_IMAGE_SHAPE  = (64,  64,  3)\n",
        "WRIST_TEMP_REGION_IMAGE_SHAPE  = (256, 256, 3)\n",
        "WRIST_E_REGION_IMAGE_SHAPE = (64,  64,  3)\n",
        "WRIST_N_REGION_IMAGE_SHAPE = (64,  64,  3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4dVF4hKCVgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title General training constants\n",
        "\n",
        "EPOCHS = 150\n",
        "VERBOSE= 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-R9RCxWBTMb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Detectors's region names\n",
        "\n",
        "FEET_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'mtp_1',\n",
        "    2 : 'mtp_2',\n",
        "    3 : 'mtp_3',\n",
        "    4 : 'mtp_4',\n",
        "    5 : 'mtp_5',\n",
        "    6 : 'mtp_ip',\n",
        "}\n",
        "\n",
        "HAND_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'pip_1',\n",
        "    2 : 'pip_2',\n",
        "    3 : 'pip_3',\n",
        "    4 : 'pip_4',\n",
        "    5 : 'pip_5',\n",
        "    6 : 'mcp_1',\n",
        "    7 : 'mcp_2',\n",
        "    8 : 'mcp_3',\n",
        "    9 : 'mcp_4',\n",
        "    10 : 'mcp_5',\n",
        "    11 : 'carp',\n",
        "}\n",
        "\n",
        "WRIST_N_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'cmc3',\n",
        "    2 : 'cmc4',\n",
        "    3 : 'cmc5',\n",
        "    4 : 'mna',\n",
        "    5 : 'capnlun',\n",
        "    6 : 'radcar',\n",
        "}\n",
        "\n",
        "WRIST_E_DETECTOR_REGION_NAMES = {\n",
        "    0 : 'background',\n",
        "    1 : 'mc1',\n",
        "    2 : 'mul',\n",
        "    3 : 'nav',\n",
        "    4 : 'lunate',\n",
        "    5 : 'ulna',\n",
        "    6 : 'radius',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUhELSe66Qvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Column names\n",
        "\n",
        "LF_NARROWING_REGION_NAMES = [\n",
        "    'LF_mtp_J__1',\n",
        "    'LF_mtp_J__2',\n",
        "    'LF_mtp_J__3',\n",
        "    'LF_mtp_J__4',\n",
        "    'LF_mtp_J__5',\n",
        "    'LF_mtp_J__ip',\n",
        "]\n",
        "\n",
        "RF_NARROWING_REGION_NAMES = [\n",
        "    'RF_mtp_J__1',\n",
        "    'RF_mtp_J__2',\n",
        "    'RF_mtp_J__3',\n",
        "    'RF_mtp_J__4',\n",
        "    'RF_mtp_J__5',\n",
        "    'RF_mtp_J__ip',\n",
        "]\n",
        "\n",
        "LF_EROSION_REGION_NAMES = [\n",
        "    'LF_mtp_E__1',\n",
        "    'LF_mtp_E__2',\n",
        "    'LF_mtp_E__3',\n",
        "    'LF_mtp_E__4',\n",
        "    'LF_mtp_E__5',\n",
        "    'LF_mtp_E__ip',\n",
        "]\n",
        "\n",
        "RF_EROSION_REGION_NAMES = [\n",
        "    'RF_mtp_E__1',\n",
        "    'RF_mtp_E__2',\n",
        "    'RF_mtp_E__3',\n",
        "    'RF_mtp_E__4',\n",
        "    'RF_mtp_E__5',\n",
        "    'RF_mtp_E__ip',\n",
        "]\n",
        "\n",
        "LH_FINGER_EROSION_REGION_NAMES = [\n",
        "    'LH_mcp_E__ip',\n",
        "    'LH_pip_E__2',\n",
        "    'LH_pip_E__3',\n",
        "    'LH_pip_E__4',\n",
        "    'LH_pip_E__5',\n",
        "    'LH_mcp_E__1',\n",
        "    'LH_mcp_E__2',\n",
        "    'LH_mcp_E__3',\n",
        "    'LH_mcp_E__4',\n",
        "    'LH_mcp_E__5',\n",
        "]\n",
        "\n",
        "RH_FINGER_EROSION_REGION_NAMES = [\n",
        "    'RH_mcp_E__ip',\n",
        "    'RH_pip_E__2',\n",
        "    'RH_pip_E__3',\n",
        "    'RH_pip_E__4',\n",
        "    'RH_pip_E__5',\n",
        "    'RH_mcp_E__1',\n",
        "    'RH_mcp_E__2',\n",
        "    'RH_mcp_E__3',\n",
        "    'RH_mcp_E__4',\n",
        "    'RH_mcp_E__5',\n",
        "]\n",
        "\n",
        "LH_FINGER_NARROWING_REGION_NAMES = [\n",
        "    'LH_pip_J__2',\n",
        "    'LH_pip_J__3',\n",
        "    'LH_pip_J__4',\n",
        "    'LH_pip_J__5',\n",
        "    'LH_mcp_J__1',\n",
        "    'LH_mcp_J__2',\n",
        "    'LH_mcp_J__3',\n",
        "    'LH_mcp_J__4',\n",
        "    'LH_mcp_J__5',\n",
        "]\n",
        "\n",
        "RH_FINGER_NARROWING_REGION_NAMES = [\n",
        "    'RH_pip_J__2',\n",
        "    'RH_pip_J__3',\n",
        "    'RH_pip_J__4',\n",
        "    'RH_pip_J__5',\n",
        "    'RH_mcp_J__1',\n",
        "    'RH_mcp_J__2',\n",
        "    'RH_mcp_J__3',\n",
        "    'RH_mcp_J__4',\n",
        "    'RH_mcp_J__5',\n",
        "]\n",
        "\n",
        "LH_WRIST_EROSION_REGION_NAMES = [\n",
        "    'LH_wrist_E__mc1',\n",
        "    'LH_wrist_E__mul',\n",
        "    'LH_wrist_E__nav',\n",
        "    'LH_wrist_E__lunate',\n",
        "    'LH_wrist_E__ulna',\n",
        "    'LH_wrist_E__radius',\n",
        "]\n",
        "\n",
        "RH_WRIST_EROSION_REGION_NAMES = [\n",
        "    'RH_wrist_E__mc1',\n",
        "    'RH_wrist_E__mul',\n",
        "    'RH_wrist_E__nav',\n",
        "    'RH_wrist_E__lunate',\n",
        "    'RH_wrist_E__ulna',\n",
        "    'RH_wrist_E__radius',\n",
        "]\n",
        "\n",
        "LH_WRIST_NARROWING_REGION_NAMES = [\n",
        "    'LH_wrist_J__cmc3',\n",
        "    'LH_wrist_J__cmc4',\n",
        "    'LH_wrist_J__cmc5',\n",
        "    'LH_wrist_J__mna',\n",
        "    'LH_wrist_J__capnlun',\n",
        "    'LH_wrist_J__radcar',\n",
        "]\n",
        "\n",
        "RH_WRIST_NARROWING_REGION_NAMES = [\n",
        "    'RH_wrist_J__cmc3',\n",
        "    'RH_wrist_J__cmc4',\n",
        "    'RH_wrist_J__cmc5',\n",
        "    'RH_wrist_J__mna',\n",
        "    'RH_wrist_J__capnlun',\n",
        "    'RH_wrist_J__radcar',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VtsRSMfaOL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Max scores per evaluation\n",
        "\n",
        "HAND_EROSION_SCALING = 6\n",
        "HAND_NARROWING_SCALING = 5\n",
        "\n",
        "FEET_EROSION_SCALING = 11\n",
        "FEET_NARROWING_SCALING = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgxFOcAB-xlD",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Create detectors and load weights\n",
        "\n",
        "feet_detector    = modellib.MaskRCNN(mode=\"inference\", config=f_config, model_dir=MODEL_DIR)\n",
        "hand_detector    = modellib.MaskRCNN(mode=\"inference\", config=h_config, model_dir=MODEL_DIR)\n",
        "wrist_e_detector = modellib.MaskRCNN(mode=\"inference\", config=w_config, model_dir=MODEL_DIR)\n",
        "wrist_n_detector = modellib.MaskRCNN(mode=\"inference\", config=w_config, model_dir=MODEL_DIR)\n",
        "\n",
        "feet_detector.load_weights(FEET_DETECTOR_PATH, by_name=True)\n",
        "hand_detector.load_weights(HAND_DETECTOR_PATH, by_name=True)\n",
        "wrist_e_detector.load_weights(WRIST_E_DETECTOR_PATH, by_name=True)\n",
        "wrist_n_detector.load_weights(WRIST_N_DETECTOR_PATH, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efvcrNPzA6X8",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Read example dataframe with labels\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "pd.read_csv(DATAFRAME_PATH).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDx3VPJlzE8o",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Auxilary drawing function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQTUgEY72lqw",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Functions to extract regions and fix errors\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def fix_feet_misclassification(instances):\n",
        "\n",
        "  #Order in which missing regions will be fixed\n",
        "  #Most important -> less important\n",
        "  priority_order = [1, 6, 2, 3, 4, 5]\n",
        "\n",
        "  #Define, which classes where not detected\n",
        "  classes_to_detect = sorted(feet_detector_class_names.keys())[1:]\n",
        "  empty_classes = set(classes_to_detect) - set(instances.keys())\n",
        "  empty_classes = list(empty_classes)\n",
        "  empty_classes = [ item for item in priority_order if item in empty_classes ] #Sort empty classes according to the priority order\n",
        "\n",
        "  fixed_instances = deepcopy(instances)\n",
        "\n",
        "  detected_instances_count = sum(\n",
        "      [ len(item) for key, item in instances.items() ]\n",
        "  )\n",
        "\n",
        "  #Look for a missing region in another class's detections\n",
        "  for empty_class in empty_classes:\n",
        "\n",
        "    if empty_class == 6:#If mtp_ip not detected\n",
        "      \n",
        "      if(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ] #Use the second activation of mtp_1\n",
        "      \n",
        "    elif empty_class == 1: #If mtp_1 not detected\n",
        "\n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ] #Use the second activation of mtp_ip\n",
        "\n",
        "    elif empty_class == 2:#If mtp_2 not detected\n",
        "      \n",
        "      if(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ] #Use the second activation of mtp_3\n",
        "      \n",
        "      elif (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of mtp_4\n",
        "    \n",
        "    elif empty_class == 3:#If mtp_3 not detected, may be the most common case \n",
        "      \n",
        "      if (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of mtp_4\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ] #Use the second activation of mtp_2\n",
        "    \n",
        "    elif empty_class == 4:#If mtp_4 not detected, may be the second most common case \n",
        "      \n",
        "      if (len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ]#Use the second activation of mtp_3\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ]#Use the second activation of mtp_2\n",
        "\n",
        "    elif empty_class == 5:#If mtp_5 not detected \n",
        "      \n",
        "      if (len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mtp_ip\n",
        "\n",
        "      elif(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ]#Use the second activation of mtp_1\n",
        "\n",
        "      elif(len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ]#Use the second activation of mtp_4\n",
        "\n",
        "  return fixed_instances\n",
        "\n",
        "def fix_hands_misclassification(instances):\n",
        "  \n",
        "  #Order in which missing regions will be fixed\n",
        "  #Most important -> less important\n",
        "  priority_order = [11, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5] #carp, mcp, pip\n",
        "\n",
        "  #Define, which classes where not detected\n",
        "  classes_to_detect = sorted(hand_detector_class_names.keys())[1:]\n",
        "  empty_classes = set(classes_to_detect) - set(instances.keys())\n",
        "  empty_classes = list(empty_classes)\n",
        "  empty_classes = [ item for item in priority_order if item in empty_classes ] #Sort empty classes according to the priority order\n",
        "\n",
        "  fixed_instances = deepcopy(instances)\n",
        "\n",
        "  detected_instances_count = sum(\n",
        "      [ len(item) for key, item in instances.items() ]\n",
        "  )\n",
        "\n",
        "  #Look for a missing region in another class's detections\n",
        "  for empty_class in empty_classes:\n",
        "    \n",
        "    if empty_class == 6:#If mcp_1 not detected\n",
        "      \n",
        "      if(len(fixed_instances[10]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[10].pop(1) ] #Use the second activation of mcp_5\n",
        "      \n",
        "      elif (len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ] #Use the second activation of pip_1\n",
        "    \n",
        "    elif empty_class == 7:#If mcp_2 not detected\n",
        "      \n",
        "      if(len(fixed_instances[8]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[8].pop(1) ] #Use the second activation of mcp_3\n",
        "      \n",
        "      elif (len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ] #Use the second activation of mcp_4\n",
        "    \n",
        "    elif empty_class == 8:#If mcp_3 not detected\n",
        "      \n",
        "      if (len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ] #Use the second activation of mcp_4\n",
        "\n",
        "      elif(len(fixed_instances[7]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[7].pop(1) ] #Use the second activation of mcp_2\n",
        "    \n",
        "    elif empty_class == 9:#If mcp_4 not detected \n",
        "      \n",
        "      if (len(fixed_instances[8]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[8].pop(1) ]#Use the second activation of mcp_3\n",
        "\n",
        "      elif(len(fixed_instances[7]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[7].pop(1) ]#Use the second activation of mcp_2\n",
        "\n",
        "    elif empty_class == 10:#If mcp_5 not detected \n",
        "      \n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mcp_1\n",
        "\n",
        "      elif(len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ]#Use the second activation of mcp_4\n",
        "\n",
        "    elif empty_class == 1:#If pip_1 not detected\n",
        "      \n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ] #Use the second activation of mcp_1\n",
        "      \n",
        "      elif (len(fixed_instances[5]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[5].pop(1) ] #Use the second activation of pip_5\n",
        "    \n",
        "    elif empty_class == 2:#If pip_2 not detected\n",
        "      \n",
        "      if(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ] #Use the second activation of pip_3\n",
        "      \n",
        "      elif (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of pip_4\n",
        "    \n",
        "    elif empty_class == 3:#If pip_3 not detected\n",
        "      \n",
        "      if (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of pip_4\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ] #Use the second activation of pip_2\n",
        "    \n",
        "    elif empty_class == 4:#If pip_4 not detected \n",
        "      \n",
        "      if (len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ]#Use the second activation of pip_2\n",
        "\n",
        "      elif(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ]#Use the second activation of pip_3\n",
        "\n",
        "    elif empty_class == 5:#If pip_5 not detected \n",
        "      \n",
        "      if(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ]#Use the second activation of pip_5\n",
        "\n",
        "      elif(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mcp_1\n",
        "  \n",
        "  return fixed_instances\n",
        "\n",
        "def extract_regions(detector, image, classes_num, fix_function):\n",
        "  result = detector.detect([ image ], verbose=0)\n",
        "  result = result[0]\n",
        "\n",
        "  #Reshape masks from a 3d [H, W, Instances] tensor to the array of masks\n",
        "  masks = []\n",
        "  for i in range(result['masks'].shape[2]):\n",
        "    masks.append(result['masks'][:, :, i])\n",
        "  \n",
        "  #Reshape dictionary to list\n",
        "  result = list(zip(\n",
        "      result['rois'],\n",
        "      result['class_ids'],\n",
        "      result['scores'],\n",
        "      masks\n",
        "  ))\n",
        "\n",
        "  groupped = dict()\n",
        "  \n",
        "  #Group detected instances by class\n",
        "  for item in result:\n",
        "    roi, class_id, score, mask = item\n",
        "\n",
        "    if class_id not in groupped.keys():\n",
        "      groupped[class_id] = []\n",
        "    \n",
        "    groupped[class_id].append((score, roi, mask))\n",
        "\n",
        "  #Sort each group by confidence scores\n",
        "  for key in groupped.keys():\n",
        "    groupped[key] = sorted(groupped[key], key = lambda x: x[0], reverse=True)\n",
        "\n",
        "  detected_classes = len(list(groupped.keys()))\n",
        "\n",
        "\n",
        "  #Ensure the model has detected all needed regions\n",
        "  misclassified = False\n",
        "\n",
        "  #Some joints may be misclassified, in most cases it may be fixed in trivial way\n",
        "  if detected_classes < classes_num:\n",
        "    groupped = fix_function(groupped)\n",
        "    misclassified = True\n",
        "\n",
        "  #Take max confidence result for each class as a region proposal \n",
        "  result = dict(\n",
        "      [ (key, groupped[key][0]) for key in groupped.keys() ]\n",
        "  )\n",
        "\n",
        "  return result, misclassified\n",
        "\n",
        "def convert_to_original_format(instances):\n",
        "  #Reshape { class : (score, roi, mask) } dictionary to the format the results are yielded by models\n",
        "  #Useful for visualization with MRCNN built-in functions\n",
        "  classes, rois, scores, masks = [], [], [], []\n",
        "\n",
        "  for key, value in instances.items():\n",
        "    score, roi, mask = value\n",
        "\n",
        "    classes.append(key)\n",
        "    rois.append(roi)\n",
        "    scores.append(score)\n",
        "    masks.append(mask)\n",
        "  \n",
        "  result = {\n",
        "      'rois' : np.array(rois),\n",
        "      'scores' : np.array(scores),\n",
        "      'class_ids' : np.array(classes),\n",
        "      'masks' : np.array(masks).swapaxes(0,2).swapaxes(0,1)\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "def extract_regions_from_image(image, result, scale=1.0):\n",
        "  regions = dict()\n",
        "\n",
        "  for class_, region in zip(result['class_ids'], result['rois']):\n",
        "    #Rescale the box\n",
        "    x1, y1, x2, y2, = region\n",
        "    cx, cy = (x2 + x1)/2, (y2 + y1)/2\n",
        "    x1, x2 = int((x1-cx) * scale + cx), int((x2-cx) * scale + cx)\n",
        "    y1, y2 = int((y1-cy) * scale + cy), int((y2-cy) * scale + cy) \n",
        "\n",
        "    #Yes, yes, i know, y-axis is the 0 axis, but whatever\n",
        "    x1, x2, y1, y2 = max(0, x1), min(x2, image.shape[0]), max(0, y1), min(y2, image.shape[1])\n",
        "\n",
        "    regions[class_] = image[ x1:x2, y1:y2 ]\n",
        "\n",
        "  return regions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Xag8xq-B_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Extract regions from images\n",
        "\n",
        "def apply_detector(detector, image, classes_num, scale, fix_function=lambda x: x):\n",
        "  regions, misclassified = extract_regions(detector, image, classes_num, fix_function)\n",
        "  \n",
        "  #Convert function output to the Matterport format compatible with their functions\n",
        "  regions = convert_to_original_format(regions)\n",
        "  \n",
        "  #TODO: Create an output for badly classified image\n",
        "\n",
        "  regions = extract_regions_from_image(image, regions, scale)\n",
        "  return regions\n",
        "\n",
        "def process_wrist(image):\n",
        "  #Erosion and narrowing regions\n",
        "  E, N = dict(), dict()\n",
        "\n",
        "  if image is not None:\n",
        "    image = cv2.resize(image, WRIST_TEMP_REGION_IMAGE_SHAPE[:2])\n",
        "    E, N = apply_detector(wrist_e_detector, image, WRIST_REGIONS, WRIST_E_REGION_SCALE), \\\n",
        "           apply_detector(wrist_n_detector, image, WRIST_REGIONS, WRIST_N_REGION_SCALE)\n",
        "\n",
        "  return E, N\n",
        "\n",
        "def read_training_dataset(dataframe_path, images_path):\n",
        "  ExtractedRegions = dict()\n",
        "  \n",
        "  df = pd.read_csv(dataframe_path)\n",
        "  \n",
        "  for i, P_ID in enumerate(df['Patient_ID']):\n",
        "    PatientRegions = {\n",
        "        'info' : df[ df['Patient_ID'] == P_ID ]\n",
        "    }\n",
        "    \n",
        "    #Compose full image filenames\n",
        "    LF = images_path + P_ID + '-LF.jpg'\n",
        "    RF = images_path + P_ID + '-RF.jpg'\n",
        "    LH = images_path + P_ID + '-LH.jpg'\n",
        "    RH = images_path + P_ID + '-RH.jpg'\n",
        "\n",
        "    #Read images\n",
        "    LF, RF, LH, RH = cv2.imread(LF), cv2.imread(RF), cv2.imread(LH), cv2.imread(RH)\n",
        "\n",
        "    #Flip right limb images since detectors were trained on left limb images only\n",
        "    RF, RH = np.flip(RF, axis=1), np.flip(RH, axis=1)\n",
        "    \n",
        "    #Extract regions from images\n",
        "    LF, RF, LH, RH = apply_detector(feet_detector, LF, FEET_REGIONS, FEET_REGION_SCALE, fix_feet_misclassification),  \\\n",
        "                     apply_detector(feet_detector, RF, FEET_REGIONS, FEET_REGION_SCALE, fix_feet_misclassification),  \\\n",
        "                     apply_detector(hand_detector, LH, HAND_REGIONS, HAND_REGION_SCALE, fix_hands_misclassification), \\\n",
        "                     apply_detector(hand_detector, RH, HAND_REGIONS, HAND_REGION_SCALE, fix_hands_misclassification)\n",
        "    \n",
        "    LW = LH.pop(11, None) #Get the wrist image for further segmentation\n",
        "    RW = RH.pop(11, None)\n",
        "\n",
        "    LWE, LWN, RWE, RWN = *process_wrist(LW), *process_wrist(RW)\n",
        "    \n",
        "    PatientRegions['LF']  = LF\n",
        "    PatientRegions['RF']  = RF\n",
        "    PatientRegions['LH']  = LH\n",
        "    PatientRegions['RH']  = RH\n",
        "    PatientRegions['LWE'] = LWE\n",
        "    PatientRegions['LWN'] = LWN\n",
        "    PatientRegions['RWE'] = RWE\n",
        "    PatientRegions['RWN'] = RWN\n",
        "\n",
        "    ExtractedRegions[P_ID] = PatientRegions\n",
        "\n",
        "    if (not ( (i + 1) % 50 )) or (i + 1) == len(df['Patient_ID']):\n",
        "      print('Applied detectors on', i + 1, 'PIDs')\n",
        "    \n",
        "  return ExtractedRegions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me5jXYKORQhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Functions to build numpy arrays from image metadata and extracted regions\n",
        "\n",
        "def get_sorted_patient_data(region_dictionary):\n",
        "  region_dictionary = sorted(\n",
        "      list( region_dictionary.items() ), key = lambda x: x[0]\n",
        "  )\n",
        "  return region_dictionary\n",
        "\n",
        "def form_feet_datasets(sorted_region_data):\n",
        "  Images  = []\n",
        "  Erosion = []\n",
        "  Narrowing = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LF_data = sorted(\n",
        "        list( data['LF'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LF_data:\n",
        "      image = cv2.resize(image, FEET_REGION_IMAGE_SHAPE[:2])\n",
        "      \n",
        "      erosion_score_column   = LF_EROSION_REGION_NAMES   [ region_class - 1 ]\n",
        "      narrowing_score_column = LF_NARROWING_REGION_NAMES [ region_class - 1 ]\n",
        "      \n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "      narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Erosion.append(erosion_score)\n",
        "      Narrowing.append(narrowing_score)\n",
        "    \n",
        "    sorted_RF_data = sorted(\n",
        "        list( data['RF'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RF_data:\n",
        "      image = cv2.resize(image, FEET_REGION_IMAGE_SHAPE[:2])\n",
        "      \n",
        "      erosion_score_column   = RF_EROSION_REGION_NAMES   [ region_class - 1 ]\n",
        "      narrowing_score_column = RF_NARROWING_REGION_NAMES [ region_class - 1 ]\n",
        "      \n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "      narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Erosion.append(erosion_score)\n",
        "      Narrowing.append(narrowing_score)\n",
        "  \n",
        "  return np.array(Images), np.array(Erosion), np.array(Narrowing)\n",
        "\n",
        "def form_hand_datasets(sorted_region_data):\n",
        "  ErosionImages = []\n",
        "  ErosionLabels = []\n",
        "\n",
        "  NarrowingImages = []\n",
        "  NarrowingLabels = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LH_data = sorted(\n",
        "        list( data['LH'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LH_data:\n",
        "      image = cv2.resize(image, FINGER_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      erosion_score_column   = LH_FINGER_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      narrowing_score_column = LH_FINGER_NARROWING_REGION_NAMES[ region_class - 2 ] if region_class > 1 else None\n",
        "\n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "\n",
        "      ErosionImages.append(image)\n",
        "      ErosionLabels.append(erosion_score)\n",
        "\n",
        "      if narrowing_score_column is not None:\n",
        "        narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "        NarrowingImages.append(image)\n",
        "        NarrowingLabels.append(narrowing_score)\n",
        "\n",
        "    sorted_RH_data = sorted(\n",
        "        list( data['RH'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RH_data:\n",
        "      image = cv2.resize(image, FINGER_REGION_IMAGE_SHAPE[:2])\n",
        "      \n",
        "      erosion_score_column   = RH_FINGER_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      narrowing_score_column = RH_FINGER_NARROWING_REGION_NAMES[ region_class - 2 ] if region_class > 1 else None\n",
        "      \n",
        "      erosion_score   = list( info[ erosion_score_column   ] )[0]\n",
        "\n",
        "      ErosionImages.append(image)\n",
        "      ErosionLabels.append(erosion_score)    \n",
        "\n",
        "      if narrowing_score_column is not None:\n",
        "        narrowing_score = list( info[ narrowing_score_column ] )[0]\n",
        "\n",
        "        NarrowingImages.append(image)\n",
        "        NarrowingLabels.append(narrowing_score) \n",
        "\n",
        "  return np.array(ErosionImages), np.array(ErosionLabels), np.array(NarrowingImages), np.array(NarrowingLabels) \n",
        "\n",
        "def form_wrist_erosion_dataset(sorted_region_data):\n",
        "  Images = []\n",
        "  Labels = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LWE_data = sorted(\n",
        "        list( data['LWE'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LWE_data:\n",
        "      image = cv2.resize(image, WRIST_E_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = LH_WRIST_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "    sorted_RWE_data = sorted(\n",
        "        list( data['RWE'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RWE_data:\n",
        "      image = cv2.resize(image, WRIST_E_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = RH_WRIST_EROSION_REGION_NAMES[ region_class - 1 ]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "  return np.array(Images), np.array(Labels)\n",
        "\n",
        "def form_wrist_narrowing_dataset(sorted_region_data):\n",
        "  Images = []\n",
        "  Labels = []\n",
        "\n",
        "  for PID, data in sorted_region_data:\n",
        "    info = data['info']\n",
        "\n",
        "    sorted_LWN_data = sorted(\n",
        "        list( data['LWN'].items() ), key = lambda x: [0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_LWN_data:\n",
        "      image = cv2.resize(image, WRIST_N_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = LH_WRIST_NARROWING_REGION_NAMES[ region_class - 1 ]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "    sorted_RWN_data = sorted(\n",
        "        list( data['RWN'].items() ), key = lambda x: x[0]\n",
        "    )\n",
        "\n",
        "    for region_class, image in sorted_RWN_data:\n",
        "      image = cv2.resize(image, WRIST_N_REGION_IMAGE_SHAPE[:2])\n",
        "\n",
        "      score_column = RH_WRIST_NARROWING_REGION_NAMES[ region_class - 1]\n",
        "      score = list( info[ score_column ] )[0]\n",
        "\n",
        "      Images.append(image)\n",
        "      Labels.append(score)\n",
        "\n",
        "  return np.array(Images), np.array(Labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ajjG_MvjxJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = read_training_dataset(DATAFRAME_PATH, TRAIN_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efwo8mrpNmVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_data = get_sorted_patient_data(training_data)\n",
        "\n",
        "feet_joints, feet_erosion, feet_narrowing = form_feet_datasets(sorted_data)\n",
        "finger_erosion_images, finger_erosion, finger_narrowing_images, finger_narrowing = form_hand_datasets(sorted_data)\n",
        "wrist_erosion_images, wrist_erosion = form_wrist_erosion_dataset(sorted_data)\n",
        "wrist_narrowing_images, wrist_narrowing = form_wrist_narrowing_dataset(sorted_data)\n",
        "\n",
        "feet_joints.shape, feet_erosion.shape, feet_narrowing.shape, \\\n",
        "finger_erosion_images.shape, finger_erosion.shape,finger_narrowing_images.shape, finger_narrowing.shape, \\\n",
        "wrist_erosion_images.shape, wrist_erosion.shape, wrist_narrowing_images.shape, wrist_narrowing.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkFYbGQxam6i",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Dataset processing functions\n",
        "\n",
        "def normalize_labels(vector, smoothing=0.0, max_value = None):\n",
        "  #Normalizes integer non-negative values to [0, 1] interval\n",
        "  #Smoothing value in range [0, 1] shifts value to the center\n",
        "\n",
        "  #If no scale value provided, get the max\n",
        "  if max_value is None or max_value < np.max(vector):\n",
        "    max_value = np.max(vector)\n",
        "\n",
        "  #Normalize\n",
        "  vector = vector.astype(np.float64) / max_value\n",
        "  \n",
        "  #Smooth\n",
        "  smooth = lambda x: (0.5 + (x - 0.5) * (1 - smoothing))\n",
        "  return np.array([\n",
        "          smooth(value) for value in vector\n",
        "  ])\n",
        "\n",
        "def denormalize_labels(vector, max_value, smoothing=0.0):\n",
        "  #Inverse operation, denormalizes [0, 1] values to [0, N] range\n",
        "\n",
        "  extend = lambda x: (0.5 + (x - 0.5) / (1 - smoothing))\n",
        "  vector = np.array([ extend(value) for value in vector ]) * max_value\n",
        "  return vector\n",
        "\n",
        "def downsample_class(samples, labels, multiplier, target_class):\n",
        "  #Get classes represented in data set, and their sample counts\n",
        "  classes, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "  #Compute index of the class to downsample\n",
        "  for i, class_ in enumerate(classes):\n",
        "    if class_ == target_class:\n",
        "      target_class = i\n",
        "      break\n",
        "  \n",
        "  #Compute the count of the downsampled class's samples\n",
        "  target_count = int(counts[target_class] * multiplier)\n",
        "\n",
        "  output_data   = []\n",
        "  output_labels = []\n",
        "\n",
        "  #Permute the data set\n",
        "  permutation = np.random.permutation(len(labels))\n",
        "  samples = [ samples[i] for i in permutation ]\n",
        "  labels  = [ labels[ i] for i in permutation]\n",
        "\n",
        "  #Counter for already sampled samples of class to downsample\n",
        "  sampled_values = 0\n",
        "\n",
        "  #Traverse the data set\n",
        "  for i in range(len(samples)):\n",
        "    if labels[i] == target_class:\n",
        "      if sampled_values >= target_count:\n",
        "        pass\n",
        "      else:\n",
        "        sampled_values += 1\n",
        "        \n",
        "        output_data.append(samples[i])\n",
        "        output_labels.append(labels[i])\n",
        "    else:\n",
        "      output_data.append(samples[i])\n",
        "      output_labels.append(labels[i])\n",
        "\n",
        "  return np.array(output_data), np.array(output_labels)\n",
        "\n",
        "def train_test_split(data, labels, split):\n",
        "  permutation = np.random.permutation(range(labels.shape[0]))\n",
        "  \n",
        "  split = int(labels.shape[0] * split)\n",
        "  train_indices, val_indices = permutation[:split], permutation[split:]\n",
        "\n",
        "  return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]\n",
        "\n",
        "def balanced_train_test_split(data, labels, split, rounding='trunc'):\n",
        "  #Initialize dict with labels as keys and empty \n",
        "  data_per_label = dict([ (label, []) for label in np.unique(labels) ])\n",
        "  [ data_per_label[label].append(sample) for sample, label in zip(data, labels) ]\n",
        "  \n",
        "  #Select function which will round the train sample count in certain class\n",
        "  #Distinct effect only on classes with few samples\n",
        "  rounding_functions = {\n",
        "      'trunc'  : lambda x: int(x),\n",
        "      'round'  : lambda x: int(round(x)),\n",
        "      'random' : lambda x: int(x) if np.random.rand() < 0.5 else int(round(x))\n",
        "  }\n",
        "  rounding = rounding_functions[rounding]\n",
        "\n",
        "  x_train, y_train, x_valid, y_valid = [], [], [], []\n",
        "\n",
        "  #Permute samples of several class and split them into train/val according to 'split' value\n",
        "  for label, sample_list in data_per_label.items():\n",
        "    permutation = np.random.permutation(len(sample_list))\n",
        "    separator = rounding(split * len(permutation))\n",
        "    sample_list = [ sample_list[i] for i in permutation ]\n",
        "    \n",
        "    [ (x_train.append(sample_list[i]), y_train.append(label)) for i in permutation[:separator] ]\n",
        "    [ (x_valid.append(sample_list[i]), y_valid.append(label)) for i in permutation[separator:] ]\n",
        "\n",
        "  #Permute splitted sets\n",
        "  train_permutation = np.random.permutation(len(x_train))\n",
        "  valid_permutation = np.random.permutation(len(x_valid))\n",
        "\n",
        "  x_train, y_train = [ x_train[i] for i in train_permutation ], [ y_train[i] for i in train_permutation ]\n",
        "  x_valid, y_valid = [ x_valid[i] for i in valid_permutation ], [ y_valid[i] for i in valid_permutation ]\n",
        "\n",
        "  return np.array(x_train), np.array(y_train), np.array(x_valid), np.array(y_valid)\n",
        "\n",
        "def naive_oversampling(data, labels):\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  classes, frequencies = np.unique(labels, return_counts=True) #labels count\n",
        "\n",
        "  oversampled_count = np.max(frequencies) * len(frequencies) - data.shape[0] #oversampled data size\n",
        "\n",
        "  frequencies = np.max(frequencies) * 1 / frequencies #inverse values to convert them to sampling weights\n",
        "  frequencies = frequencies - frequencies[np.argmin(frequencies)]  #do not oversample the most common class\n",
        "  \n",
        "  oversampled_data = np.zeros(\n",
        "      shape = (oversampled_count,) + data.shape[1:],\n",
        "      dtype = data.dtype\n",
        "  )\n",
        "\n",
        "  oversampled_labels = np.zeros(\n",
        "      shape = oversampled_count,\n",
        "      dtype = labels.dtype\n",
        "  )\n",
        "\n",
        "  frequencies = np.array([\n",
        "            frequencies[np.argwhere( classes == class_ ).flat[0]] for class_ in labels\n",
        "  ])\n",
        "  frequencies = frequencies / np.sum(frequencies)\n",
        "\n",
        "  indices = np.random.choice(data.shape[0], size = oversampled_count, p = frequencies)\n",
        "\n",
        "  for i, index in enumerate(indices):\n",
        "    oversampled_data[i] = data[index]\n",
        "    oversampled_labels[i] = labels[index]\n",
        "\n",
        "  return np.concatenate([data, oversampled_data], axis=0), np.concatenate([labels, oversampled_labels], axis=0)\n",
        "\n",
        "def downsample_most_common_class(data, labels):\n",
        "  #Downsamples the most common class to the values number of the second common class\n",
        "  classes, frequencies = np.unique(labels, return_counts=True)\n",
        "  second_maximum, first_maximum = sorted(frequencies)[-2], sorted(frequencies)[-1]\n",
        "  downsample_factor = second_maximum / first_maximum\n",
        "  class_to_downsample = classes[ np.argmax(frequencies) ]\n",
        "\n",
        "  return downsample_class(data, labels, downsample_factor, class_to_downsample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JBfiDzcbSzr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Modified DenseNet definition\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "import keras.utils as keras_utils\n",
        "\n",
        "from keras_applications import imagenet_utils\n",
        "from keras_applications.imagenet_utils import decode_predictions\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.layers import SpatialDropout2D\n",
        "\n",
        "def dense_block(x, blocks, name, regularizer_factory):\n",
        "    for i in range(blocks):\n",
        "        x = conv_block(x, 32, name=name + '_block' + str(i + 1), regularizer_factory=regularizer_factory)\n",
        "    return x\n",
        "\n",
        "def sd_dense_block(x, blocks, name, min_dropout_rate, max_dropout_rate, regularizer_factory):\n",
        "    \n",
        "  bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "  \n",
        "  rates = np.linspace(\n",
        "      start=min_dropout_rate,\n",
        "      stop=max_dropout_rate,\n",
        "      num=blocks,\n",
        "      endpoint=True\n",
        "  )\n",
        "  \n",
        "  outputs = [x]\n",
        "  \n",
        "  for i in range(blocks):\n",
        "    rates_slice = rates[:i+1]\n",
        "    block_name = name + '_block' + str(i + 1)\n",
        "\n",
        "    specific_block_input = [ SpatialDropout2D(rate)(x) for rate, x in zip(rates_slice, outputs) ]\n",
        "    if len(specific_block_input) > 1:\n",
        "      specific_block_input = layers.Concatenate(axis=bn_axis, name=block_name + '_concat')(specific_block_input)\n",
        "    else:\n",
        "      specific_block_input = specific_block_input[0]\n",
        "    \n",
        "    output = sd_conv_block(specific_block_input, 32, name=block_name, regularizer_factory=regularizer_factory)\n",
        "    outputs = [output] + outputs\n",
        "\n",
        "  return layers.Concatenate(axis=bn_axis, name=name + '_concat')(outputs)\n",
        "\n",
        "def transition_block(x, reduction, name, regularizer_factory):\n",
        "    \n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_bn')(x)\n",
        "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
        "    x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n",
        "                      use_bias=False,\n",
        "                      name=name + '_conv',\n",
        "                      kernel_regularizer=regularizer_factory())(x)\n",
        "    x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(x, growth_rate, name, regularizer_factory):\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    \n",
        "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
        "                                   epsilon=1.001e-5,\n",
        "                                   name=name + '_0_bn')(x)\n",
        "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
        "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
        "                       use_bias=False,\n",
        "                       name=name + '_1_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                   name=name + '_1_bn')(x1)\n",
        "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3,\n",
        "                       padding='same',\n",
        "                       use_bias=False,\n",
        "                       name=name + '_2_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
        "    return x\n",
        "\n",
        "def sd_conv_block(x, growth_rate, name, regularizer_factory):\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    \n",
        "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
        "                                   epsilon=1.001e-5,\n",
        "                                   name=name + '_0_bn')(x)\n",
        "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
        "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
        "                       use_bias=False,\n",
        "                       name=name + '_1_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                   name=name + '_1_bn')(x1)\n",
        "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3,\n",
        "                       padding='same',\n",
        "                       use_bias=False,\n",
        "                       name=name + '_2_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "                       \n",
        "    return x1\n",
        "\n",
        "\n",
        "def DenseNet(blocks,\n",
        "             include_top=True,\n",
        "             weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "             input_tensor=None,\n",
        "             input_shape=None,\n",
        "             pooling=None,\n",
        "             classes=1000,\n",
        "             blocks_to_include=3,\n",
        "             include_large_conv=False,\n",
        "             min_dropout_rate=0.0,\n",
        "             max_dropout_rate=0.0,\n",
        "             regularizer_factory=lambda: None):\n",
        "    \n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=224,\n",
        "                                      min_size=32,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    if include_large_conv:\n",
        "      x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
        "      x = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv', kernel_regularizer=regularizer_factory())(x)\n",
        "    else:\n",
        "      #new layer\n",
        "      x = layers.Conv2D(64, 3, use_bias=False, padding='same', name='conv1/conv_replaced', kernel_regularizer=regularizer_factory())(img_input)\n",
        "    \n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
        "    x = layers.Activation('relu', name='conv1/relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\n",
        "    \n",
        "    if abs(max_dropout_rate) < 1e-4: #If dropout rate defined as zero, use non-dropout version\n",
        "      x = dense_block(x, blocks[0], name='conv2', regularizer_factory=regularizer_factory)\n",
        "      x = transition_block(x, 0.5, name='pool2',  regularizer_factory=regularizer_factory)\n",
        "      x = dense_block(x, blocks[1], name='conv3', regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 2:\n",
        "        x = transition_block(x, 0.5, name='pool3',  regularizer_factory=regularizer_factory)\n",
        "        x = dense_block(x, blocks[2], name='conv4', regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 3:\n",
        "        x = transition_block(x, 0.5, name='pool4',  regularizer_factory=regularizer_factory)\n",
        "        x = dense_block(x, blocks[3], name='conv5', regularizer_factory=regularizer_factory)\n",
        "    else: #If non-zero dropout rate, use specialized dropout blocks\n",
        "      x = sd_dense_block(x, blocks[0], 'conv2', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      x = transition_block(x, 0.5, 'pool2', regularizer_factory=regularizer_factory)\n",
        "      x = sd_dense_block(x, blocks[1], 'conv3', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 2:\n",
        "        x = transition_block(x, 0.5, 'pool3', regularizer_factory=regularizer_factory)\n",
        "        x = sd_dense_block(x, blocks[2], 'conv4', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 3:\n",
        "        x = transition_block(x, 0.5, 'pool4', regularizer_factory=regularizer_factory)\n",
        "        x = sd_dense_block(x, blocks[3], 'conv5', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "    \n",
        "    bn_name = 'bn'\n",
        "    if blocks_to_include < 3:\n",
        "      bn_name = 'bn_replaced'\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=bn_name)(x)\n",
        "    x = layers.Activation('relu', name='relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "\n",
        "    #Compute conv layers number to name the model 'densenetX'\n",
        "    \n",
        "    true_blocks = blocks[:blocks_to_include]\n",
        "    model_name = sum([1] + list(map(lambda x: x * 2 + 1, true_blocks)))\n",
        "    model_name = 'densenet' + str(model_name)\n",
        "    \n",
        "    model = models.Model(inputs, x, name=model_name)\n",
        "\n",
        "    # Load weights.\n",
        "    if weights is not None:\n",
        "        model.load_weights(weights, by_name=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def ModifiedDenseNet(\n",
        "      blocks_set = 121,\n",
        "      weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      blocks_to_include=3,\n",
        "      include_large_conv=False,\n",
        "      min_dropout_rate = 0.0,\n",
        "      max_dropout_rate = 0.0,\n",
        "      regularizer_factory=lambda: None\n",
        "    ):\n",
        "    \n",
        "    blocks = {\n",
        "        121 : [6, 12, 24, 16],\n",
        "        169 : [6, 12, 32, 32],\n",
        "        201 : [6, 12, 48, 32]\n",
        "    }\n",
        "    if isinstance(blocks_set, list):\n",
        "      blocks = blocks_set\n",
        "    elif isinstance(blocks_set, int):\n",
        "      if blocks_set in blocks.keys():\n",
        "        blocks = blocks[blocks_set]\n",
        "      else:\n",
        "        raise ValueError('No such blocks set defined')\n",
        "    else:\n",
        "      raise ValueError('No such blocks set defined')\n",
        "    \n",
        "    return DenseNet(blocks,\n",
        "                    False, weights,\n",
        "                    None, input_shape,\n",
        "                    pooling, 1000, blocks_to_include, include_large_conv, min_dropout_rate, max_dropout_rate, regularizer_factory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6shlJmLWbdfb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Model defining functions\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import re\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.applications.resnet import ResNet50, ResNet101, ResNet152\n",
        "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201, preprocess_input\n",
        "from keras.layers import Dense, Dropout, SpatialDropout2D\n",
        "from keras.models import Model\n",
        "from keras.losses import categorical_crossentropy, mean_absolute_error, mean_squared_error, logcosh\n",
        "from keras.optimizers import adam, sgd\n",
        "from keras import regularizers\n",
        "from keras.models import model_from_json\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "\n",
        "def denormalized_mae(y_true, y_pred, max_value, smoothing):\n",
        "  y_true = K.constant(0.5) + (y_true - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "  y_pred = K.constant(0.5) + (y_pred - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "\n",
        "  y_true = y_true * max_value\n",
        "  y_pred = y_pred * max_value\n",
        "\n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "def denormalized_mse(y_true, y_pred, max_value, smoothing):\n",
        "  y_true = K.constant(0.5) + (y_true - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "  y_pred = K.constant(0.5) + (y_pred - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "\n",
        "  y_true = y_true * max_value\n",
        "  y_pred = y_pred * max_value\n",
        "\n",
        "  return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "def get_denormalized_function(max_value, func_name, smoothing):\n",
        "  def mae(y_true, y_pred):\n",
        "    return denormalized_mae(y_true, y_pred, max_value, smoothing)\n",
        "\n",
        "  def mse(y_true, y_pred):\n",
        "    return denormalized_mse(y_true, y_pred, max_value, smoothing)\n",
        "\n",
        "  funcs = {\n",
        "      'mae' : mae,\n",
        "      'mse' : mse,\n",
        "  }\n",
        "\n",
        "  return funcs[func_name]\n",
        "\n",
        "def get_lr_metric(optimizer): #Custom metric to monitor learning rate\n",
        "  def lr(y_true, y_pred):\n",
        "      return optimizer.lr\n",
        "  return lr\n",
        "\n",
        "def get_feet_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FEET_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_feet_erosion'\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(FEET_EROSION_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(FEET_EROSION_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def get_feet_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FEET_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_feet_narrowing'\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(FEET_NARROWING_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(FEET_NARROWING_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def get_hand_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FINGER_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory,\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_hand_erosion'\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def get_hand_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=FINGER_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_hand_narrowing'\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def get_wrist_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=WRIST_E_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_wrist_erosion'\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_EROSION_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def get_wrist_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)):\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=WRIST_N_REGION_IMAGE_SHAPE,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = dropout_rate,\n",
        "    regularizer_factory=regularizer_factory\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name + '_wrist_narrowing'\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mae', SMOOTHING)\n",
        "  mse = get_denormalized_function(HAND_NARROWING_SCALING - 1, 'mse', SMOOTHING)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h4WxHatzqxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SMOOTHING=0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok2YFa2qoUnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functions to apply on data before fold splitting and after fold splitting\n",
        "#Pre: downsampling most frequent class\n",
        "#Post: oversampling and normalizing labels to [0, 1]\n",
        "\n",
        "def data_pre_sample(data, labels):\n",
        "  return downsample_most_common_class(data, labels)\n",
        "\n",
        "def data_post_process(x_train, y_train, x_val, y_val, c_num, smoothing):\n",
        "  x_train, y_train = naive_oversampling(x_train, y_train)\n",
        "  x_val,   y_val   = naive_oversampling(x_val,   y_val)\n",
        "\n",
        "  y_train = normalize_labels(y_train, max_value = c_num - 1, smoothing=smoothing)\n",
        "  y_val   = normalize_labels(y_val,   max_value = c_num - 1, smoothing=smoothing)\n",
        "\n",
        "  return x_train, y_train, x_val, y_val\n",
        "\n",
        "#Function to yield folds of a given samples/labels pair, applying the functions defined above\n",
        "def generate_folds(data, labels, scaling, label_smoothing, folds=3):\n",
        "  data, labels = data_pre_sample(data, labels)\n",
        "\n",
        "  indices     = np.arange(len(labels))\n",
        "  permutation = np.random.permutation(indices)\n",
        "  permutation = list(permutation)\n",
        "  \n",
        "  fold_indices = []\n",
        "  fold_len     = len(labels) // folds\n",
        "  \n",
        "  for i in range(folds):\n",
        "    fold = None\n",
        "\n",
        "    if i < folds - 1:\n",
        "      fold = np.array([ permutation.pop(-1) for i in range(fold_len) ])\n",
        "    else:\n",
        "      fold = permutation\n",
        "    \n",
        "    fold_indices.append(np.array(fold))\n",
        "\n",
        "  for fold in fold_indices:\n",
        "    val_indices   = fold\n",
        "    train_indices = np.array(\n",
        "        [ x for x in indices if x not in val_indices ]\n",
        "    )\n",
        "    train_x, train_y = data[train_indices], labels[train_indices]\n",
        "    valid_x, valid_y = data[val_indices], labels[val_indices] \n",
        "    \n",
        "    train_x, train_y, valid_x, valid_y = data_post_process(train_x, train_y, valid_x, valid_y, scaling, label_smoothing)\n",
        "    \n",
        "    yield train_x, train_y, valid_x, valid_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mag5kgK4BID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "feet_erosion_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "    \n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "feet_narrowing_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "    \n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "hand_erosion_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "    \n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "hand_narrowing_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "    \n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "wrist_erosion_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "    \n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "wrist_narrowing_train_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    horizontal_flip = True,\n",
        "    rotation_range=45,\n",
        "    \n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "\n",
        "    dtype='float64'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    samplewise_center = True,\n",
        "    samplewise_std_normalization = True,\n",
        "\n",
        "    dtype='float64'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4XBvthy4tys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Visualize some augmented samples\n",
        "\n",
        "def normalize_image(image):\n",
        "  return (image - np.min(image))/(np.max(image) - np.min(image))\n",
        "\n",
        "def display_generator(gen, data, labels=None, n_cols=3, n_rows=2, scale = 3):\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(scale * n_cols, scale * n_rows), ncols=n_cols, nrows=n_rows)\n",
        "\n",
        "  samples = gen.flow(data, labels, batch_size=n_cols * n_rows).next()\n",
        "\n",
        "  print('Displaying images normalized to [0, 1] with mean', samples[0].mean(axis=(0,1,2)), 'and std', samples[0].std(axis=(0,1,2)))\n",
        "\n",
        "  for index, axis in enumerate(ax.flat):\n",
        "    sample = samples[0][index] if labels is not None else samples[index]\n",
        "    axis.imshow(normalize_image(sample))\n",
        "    if labels is not None:\n",
        "      label = samples[1][index]\n",
        "      axis.set_xlabel(label)\n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "display_generator(feet_erosion_train_datagen,    *naive_oversampling(feet_joints, feet_erosion))\n",
        "display_generator(hand_erosion_train_datagen,    *naive_oversampling(finger_erosion_images, finger_erosion))\n",
        "display_generator(wrist_erosion_train_datagen,   *naive_oversampling(wrist_erosion_images, wrist_erosion))\n",
        "display_generator(wrist_narrowing_train_datagen, *naive_oversampling(wrist_narrowing_images, wrist_narrowing))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZ-7md36Q4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Modular live loss plotter callback\n",
        "\n",
        "#Modular live loss plotter for Keras models\n",
        "#Allows to create custom layouts of per-batch or per-epoch plots for different metrics\n",
        "\n",
        "#Monitor class defines a plot, which either may be batch or epoch-scoped, and may contain several graphs\n",
        "#Batch monitor plots its values per batch, and refreshes itself on new epoch begin\n",
        "#Epoch monitor plots its values per epoch, and performs no refresh\n",
        "#All values/last N values displaying\n",
        "#Log-scale/Linear scale displaying\n",
        "\n",
        "#Plotter callback handles different Monitors and responds to the actual plotting\n",
        "#Defines a grid where Monitors will be drawn, grid size, refresh rate in batches\n",
        "#when the Monitors will be re-drawn in addition to per-epoch update\n",
        "#Plotter can be silenced to disable plotting and only archivate per-epoch data\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Monitor():\n",
        "    def __init__(self, scope='epoch', monitors= [ 'loss' ], plot_last=-1, log_scale=False, precision=4):\n",
        "        self.scope = scope.lower()\n",
        "        self.monitors = [ monitor.lower() for monitor in monitors ]\n",
        "        self.plot_last = max(0, plot_last)\n",
        "        self.x = []\n",
        "        self.ys = [ [] for monitor in monitors ]\n",
        "        self.log_scale = log_scale\n",
        "        self.precision = precision\n",
        "\n",
        "    def reinit(self):\n",
        "        self.x = []\n",
        "        self.ys = [ [] for monitor in self.monitors ]\n",
        "\n",
        "    def update(self, iteration, logs={}):\n",
        "        self.x.append(iteration)\n",
        "        \n",
        "        for i, monitor in enumerate(self.monitors):\n",
        "            if logs.get(monitor) is not None:\n",
        "                self.ys[i].append(logs.get(monitor))\n",
        "            else:\n",
        "                pass #Action to execute when cannot get info for a certain monitor\n",
        "\n",
        "    def plot(self, axis):\n",
        "        x_data = self.x[ -self.plot_last : ]\n",
        "        y_array = [ y_data[ -self.plot_last : ] for y_data in self.ys ]\n",
        "\n",
        "        for i, y_data in enumerate(y_array):\n",
        "            label = self.monitors[i] + '_' + self.scope #Compose graph name\n",
        "            if self.log_scale:\n",
        "                axis.set_yscale('log') #Set up scale\n",
        "                \n",
        "            if len(x_data) == len(y_data): #If data are coherent, plot them\n",
        "                axis.plot(x_data, y_data, label=label)\n",
        "\n",
        "                if self.precision > 0 and len(y_data) > 0: #If there's a last point plotted, print its value\n",
        "                    text = str(round(y_data[-1],  self.precision))\n",
        "                    axis.text(x_data[-1], y_data[-1], text)\n",
        "            else:\n",
        "                continue\n",
        "                \n",
        "        label = {'batch' : 'Batches', 'epoch' : 'Epochs'} #Set up x-label\n",
        "        axis.set_xlabel(label[self.scope])\n",
        "        \n",
        "        axis.legend()\n",
        "\n",
        "\n",
        "class Plotter(Callback):\n",
        "    def __init__(self, scale=5, n_cols=2, n_rows=1, monitors=[], refresh_rate=-1, silent=False):\n",
        "        if (n_cols * n_rows < len(monitors)):\n",
        "            raise ValueError('Grid is too small to fit all monitors!')\n",
        "\n",
        "        self.n_cols = n_cols\n",
        "        self.n_rows = n_rows\n",
        "        self.scale = scale\n",
        "\n",
        "        self.monitors = monitors\n",
        "\n",
        "        self.batch_monitors, self.epoch_monitors = [], []\n",
        "\n",
        "        for monitor in monitors:\n",
        "            if monitor.scope == 'epoch':\n",
        "                self.epoch_monitors.append(monitor)\n",
        "            elif monitor.scope == 'batch':\n",
        "                self.batch_monitors.append(monitor)\n",
        "\n",
        "        self.refresh_rate = refresh_rate\n",
        "        self.silent = False\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        pass\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        [ monitor.reinit() for monitor in self.batch_monitors ]\n",
        "\n",
        "    def plot(self):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        figsize = ( self.scale * self.n_cols, self.scale * self.n_rows)\n",
        "        fig, ax = plt.subplots(figsize=figsize, ncols=self.n_cols, nrows=self.n_rows)\n",
        "\n",
        "        if self.n_cols * self.n_rows == 1:\n",
        "          ax = np.array([ax])\n",
        "\n",
        "        for index, axis in enumerate(ax.flat):\n",
        "          if index < len(self.monitors):\n",
        "              self.monitors[index].plot(axis)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        [ monitor.update(batch, logs) for monitor in self.batch_monitors ]\n",
        "\n",
        "        if self.silent or batch == 0 or self.refresh_rate <= 0 or batch % self.refresh_rate != 0:\n",
        "            return\n",
        "\n",
        "        self.plot()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        [ monitor.update(epoch, logs) for monitor in self.epoch_monitors ]\n",
        "\n",
        "        if self.silent:\n",
        "            return\n",
        "\n",
        "        self.plot()\n",
        "\n",
        "    def reinit(self):\n",
        "      [ monitor.reinit() for monitor in self.monitors ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg0cRzhrBcwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Batch sizes and step counts\n",
        "\n",
        "def calculate_parameters(image_size, train_samples, validation_samples):\n",
        "  batch_size     = 512 // (image_size // 32) ** 2\n",
        "  val_batch_size = min(\n",
        "      validation_samples,\n",
        "      1024 // (image_size // 64) ** 2\n",
        "  )\n",
        "  steps_per_epoch = max(\n",
        "    1, round(train_samples / batch_size)\n",
        "  )\n",
        "  validation_steps = max(\n",
        "      1, validation_samples // val_batch_size\n",
        "  )\n",
        "  return {\n",
        "    'batch_size'       : batch_size,\n",
        "    'val_batch_size'   : val_batch_size,  \n",
        "    'steps_per_epoch'  : steps_per_epoch,\n",
        "    'validation_steps' : validation_steps,\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DLk0mckD4eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Custom metrics callback\n",
        "#Validates model on a given data generator\n",
        "\n",
        "#from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "#class MultiOutputMetric(Callback):\n",
        "#  def __init__(self, output_names, metric_name, output_metric_name = None):\n",
        "#    self.output_names = output_names\n",
        "#    self.metric_name = metric_name\n",
        "#    self.output_metric_name = output_metric_name if output_metric_name else metric_name\n",
        "\n",
        "#  def on_epoch_end(self, epoch, logs={}):\n",
        "#   metric_value = [ logs['val_' + name + '_' + self.metric_name] for name in self.output_names ]\n",
        "#   metric_value = sum(metric_value)\n",
        "   \n",
        "#   logs[self.output_metric_name] = metric_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLicab2GrUF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Auxilary callback functions definition\n",
        "\n",
        "#from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def exp_schedule(base_lr = 0.001, momentum=0.995):\n",
        "  return lambda x: base_lr * momentum ** x\n",
        "\n",
        "def sine_schedule(min_lr=2.5e-5, max_lr=1e-4, period=200, phase=0):\n",
        "  base_lr = (min_lr + max_lr) / 2\n",
        "  amp = (max_lr - min_lr) / 2\n",
        "  return lambda x: (base_lr + amp * np.sin(np.pi * 2 / period * x + phase))\n",
        "\n",
        "\n",
        "#MONITORS = [\n",
        "#    Monitor(scope='epoch', monitors = ['loss', 'val_loss'], plot_last=96),\n",
        "    \n",
        "#    Monitor(scope='epoch', monitors = ['mean_squared_error', 'val_mean_squared_error'], precision=3, plot_last=32),\n",
        "#    Monitor(scope='epoch', monitors = ['lr'], log_scale=False, precision=7), #lr represents custom metric defined to watch the learning rate\n",
        "    \n",
        "#    Monitor(scope='epoch', monitors = ['loss', 'val_loss']),\n",
        "    \n",
        "#    Monitor(scope='epoch', monitors = ['mae', 'val_mae'], precision=4),\n",
        "#    Monitor(scope='epoch', monitors = ['mse', 'val_mse'], precision=4),\n",
        "    \n",
        "#]\n",
        "\n",
        "SCHEDULE = exp_schedule(base_lr=1e-4, momentum=1.0)\n",
        "#schedule = sine_schedule(min_lr =2.5e-5, max_lr=1.25e-4, period=50, phase= -np.pi / 2)\n",
        "\n",
        "#plotter = Plotter(monitors=monitors, n_rows=2, n_cols=3, scale=6, refresh_rate=-1)\n",
        "\n",
        "#LR_SCHEDULER = LearningRateScheduler(SCHEDULE)\n",
        "#custom_validator = CustomMetricValidator(validation_generator_factory(), steps=validation_steps)\n",
        "\n",
        "ax = get_ax()\n",
        "ax.plot([ SCHEDULE(i) for i in range(150) ])\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Learning rate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrhpjQu2JCHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define an object to build training fronts and organize trained models\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "def callback_factory(output_path, monitor, mode='min', include_plotter=False):\n",
        "  \n",
        "  checkpoint = ModelCheckpoint(\n",
        "      output_path,\n",
        "      monitor=monitor,\n",
        "      verbose=1,\n",
        "      save_best_only=True,\n",
        "      save_weights_only=False,\n",
        "      mode=mode,\n",
        "      period=EPOCHS\n",
        "  )\n",
        "\n",
        "  scheduler = LearningRateScheduler(SCHEDULE)\n",
        "\n",
        "  monitors = [\n",
        "      Monitor(scope='epoch', monitors = ['loss', 'val_loss'], plot_last=96),\n",
        "      \n",
        "      Monitor(scope='epoch', monitors = ['mean_squared_error', 'val_mean_squared_error'], precision=3, plot_last=32),\n",
        "      Monitor(scope='epoch', monitors = ['lr'], log_scale=False, precision=7), #lr represents custom metric defined to watch the learning rate\n",
        "      \n",
        "      Monitor(scope='epoch', monitors = ['loss', 'val_loss']),\n",
        "      \n",
        "      Monitor(scope='epoch', monitors = ['mae', 'val_mae'], precision=4),\n",
        "      Monitor(scope='epoch', monitors = ['mse', 'val_mse'], precision=4),\n",
        "      \n",
        "  ]\n",
        "\n",
        "  plotter = Plotter(monitors=monitors, n_rows=2, n_cols=3, scale=6, refresh_rate=-1)\n",
        "\n",
        "  callbacks = [\n",
        "               checkpoint,\n",
        "               scheduler,\n",
        "              ]\n",
        "  if include_plotter: callbacks = [ plotter ] + callbacks\n",
        "\n",
        "  return callbacks\n",
        "\n",
        "class TrainingFront():\n",
        "  def __init__(self):\n",
        "    self.front = []\n",
        "    self.histories = []\n",
        "    self.ensembles = dict()\n",
        "\n",
        "    self.saved_models_paths = []\n",
        "    self.saved_models_by_ensemble = dict()\n",
        "\n",
        "  def append(self, model_factory, fold_generator, image_generators, callback_factory, ensemble = None):\n",
        "    self.front.append((model_factory, fold_generator, image_generators, callback_factory, ensemble))\n",
        "\n",
        "  def train(self):\n",
        "    for model_factory, fold_gen, img_gens, callback_factory, ensemble in self.front:\n",
        "      train_datagen, valid_datagen = img_gens\n",
        "      \n",
        "      for i, data in enumerate(fold_gen):\n",
        "        #Process data, create a model through a given factory and compute training parameters\n",
        "        train_x, train_y, valid_x, valid_y = data\n",
        "        model = model_factory()\n",
        "        parameters = calculate_parameters(train_x.shape[1], train_x.shape[0], valid_x.shape[0])\n",
        "        \n",
        "        batch_size       = parameters['batch_size']\n",
        "        valid_batch_size = parameters['val_batch_size']\n",
        "        validation_steps = parameters['validation_steps']\n",
        "\n",
        "        #Compose model name and checkpoint output path\n",
        "        model_name = model.name + '_fold_' + str(i)\n",
        "        output_path = MODEL_OUTPUT_PATH + model_name + '.h5'\n",
        "\n",
        "        #Get callbacks through factory\n",
        "        callback_list = callback_factory(output_path, 'mse', 'min')\n",
        "\n",
        "        #Log the training start\n",
        "        print('Model ', model_name, ' training started with parameters: epochs = ', EPOCHS,\n",
        "              ', batch_size = ', batch_size, ', valid_batch_size = ', valid_batch_size,\n",
        "              ', validation_steps = ', validation_steps, sep='') \n",
        "\n",
        "        #Train the model\n",
        "        history = model.fit_generator(\n",
        "            train_datagen.flow(train_x, train_y, batch_size=batch_size, shuffle=True),\n",
        "            validation_data = val_datagen.flow(valid_x, valid_y, batch_size=valid_batch_size, shuffle=False),\n",
        "            validation_steps= validation_steps,\n",
        "            epochs          = EPOCHS,\n",
        "            callbacks       = callback_list,\n",
        "            verbose         = VERBOSE,\n",
        "        )\n",
        "        self.histories.append(history)\n",
        "        self.saved_models_paths.append(output_path)\n",
        "\n",
        "        #If an ensemble family defined, add to the corresponding list\n",
        "        if ensemble is not None:\n",
        "          if ensemble not in self.ensembles.keys():\n",
        "            self.ensembles[ensemble] = []\n",
        "          if ensemble not in self.saved_models_by_ensemble.keys():\n",
        "            self.saved_models_by_ensemble[ensemble] = []\n",
        "          self.ensembles[ensemble].append(model)\n",
        "          self.saved_models_by_ensemble[ensemble].append(output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHrRcVTbxSF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define training fronts\n",
        "\n",
        "short_front = TrainingFront()\n",
        "\n",
        "short_front.append(\n",
        "    model_factory    = lambda: get_feet_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)),\n",
        "    fold_generator   = generate_folds(feet_joints, feet_erosion, FEET_EROSION_SCALING, SMOOTHING),\n",
        "    image_generators = (feet_erosion_train_datagen, val_datagen),\n",
        "    callback_factory = callback_factory,\n",
        "    ensemble         = 'FEET_EROSION',\n",
        ")\n",
        "\n",
        "short_front.append(\n",
        "    model_factory    = lambda: get_feet_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)),\n",
        "    fold_generator   = generate_folds(feet_joints, feet_narrowing, FEET_NARROWING_SCALING, SMOOTHING),\n",
        "    image_generators = (feet_narrowing_train_datagen, val_datagen),\n",
        "    callback_factory = callback_factory,\n",
        "    ensemble         = 'FEET_NARROWING',\n",
        ")\n",
        "\n",
        "short_front.append(\n",
        "    model_factory    = lambda: get_hand_erosion_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)),\n",
        "    fold_generator   = generate_folds(\n",
        "                            np.concatenate([ finger_erosion_images, wrist_erosion_images ], axis=0),\n",
        "                            np.concatenate([ finger_erosion, wrist_erosion ], axis=0),\n",
        "                            HAND_EROSION_SCALING, SMOOTHING\n",
        "                        ),\n",
        "    image_generators = (hand_erosion_train_datagen, val_datagen),\n",
        "    callback_factory = callback_factory,  \n",
        "    ensemble         = 'HAND_EROSION',    \n",
        ")\n",
        "\n",
        "short_front.append(\n",
        "    model_factory   = lambda: get_hand_narrowing_estimator(dropout_rate=0.95, regularizer_factory=lambda: l1(1e-4)),\n",
        "    fold_generator  = generate_folds(\n",
        "                            np.concatenate([ finger_narrowing_images, wrist_narrowing_images ], axis=0),\n",
        "                            np.concatenate([ finger_narrowing, wrist_narrowing ], axis=0),\n",
        "                            HAND_NARROWING_SCALING, SMOOTHING\n",
        "                      ),\n",
        "   image_generators = (hand_narrowing_train_datagen, val_datagen),\n",
        "   callback_factory = callback_factory,\n",
        "   ensemble         = 'HAND_NARROWING',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuH3JshWxff-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "short_front.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbZA_TtCS7fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_ensemble(ensemble, input_image):\n",
        "  results = []\n",
        "  for model in ensemble:\n",
        "    input_tensor = input_image[np.newaxis, :]\n",
        "    gen = val_datagen.flow(input_tensor, batch_size=1)\n",
        "    \n",
        "    results.append(\n",
        "        model.predict_generator(gen).flat[0]\n",
        "    )\n",
        "\n",
        "  return np.mean(np.array(results))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wba0Q7dHVyx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b93e79f-9d6c-40c6-b1b1-26b1c9914fe0"
      },
      "source": [
        "def evaluate_hands(hand, right, erosion_ensemble, narrowing_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  E_COLUMNS = LH_FINGER_EROSION_REGION_NAMES   if not right else RH_FINGER_EROSION_REGION_NAMES\n",
        "  N_COLUMNS = LH_FINGER_NARROWING_REGION_NAMES if not right else RH_FINGER_NARROWING_REGION_NAMES\n",
        "\n",
        "  for region_class, image in hand.items():\n",
        "\n",
        "    erosion_score_column   = E_COLUMNS[ region_class - 1 ]\n",
        "    narrowing_score_column = N_COLUMNS[ region_class - 2 ] if region_class > 1 else None\n",
        "\n",
        "    erosion_score = evaluate_ensemble(erosion_ensemble, image)\n",
        "    erosion_score = denormalize_labels(np.array([ erosion_score ]), HAND_EROSION_SCALING, SMOOTHING)\n",
        "    results[erosion_score_column] = erosion_score\n",
        "\n",
        "    if narrowing_score_column is not None:\n",
        "      narrowing_score = evaluate_ensemble(narrowing_ensemble, image)\n",
        "      narrowing_score = denormalize_labels(np.array[ narrowing_score ], HAND_NARROWING_SCALING, SMOOTHING)\n",
        "      results[narrowing_score_column] = narrowing_score\n",
        "    \n",
        "  return results\n",
        "\n",
        "def evaluate_feet(foot, right, erosion_ensemble, narrowing_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  E_COLUMNS = LF_EROSION_REGION_NAMES   if not right else RF_EROSION_REGION_NAMES\n",
        "  N_COLUMNS = LF_NARROWING_REGION_NAMES if not right else RF_NARROWING_REGION_NAMES\n",
        "\n",
        "  for region_class, image in foot.items():\n",
        "    erosion_score_column   = E_COLUMNS[ region_class - 1 ]\n",
        "    narrowing_score_column = N_COLUMNS[ region_class - 1 ]\n",
        "\n",
        "    erosion_score = evaluate_ensemble(erosion_ensemble, image)\n",
        "    erosion_score = denormalize_labels(np.array([ erosion_score ]), FEET_EROSION_SCALING, SMOOTHING)\n",
        "    narrowing_score = evaluate_ensemble(narrowing_ensemble, image)\n",
        "    narrowing_score = denormalize_labels(np.array[ narrowing_score ], FEET_NARROWING_SCALING, SMOOTHING)\n",
        "    \n",
        "    results[erosion_score_column] = erosion_score\n",
        "    results[narrowing_score_column] = narrowing_score\n",
        "\n",
        "  return results\n",
        "\n",
        "def evaluate_wrist_erosion(wrist, right, erosion_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  E_COLUMNS = LH_WRIST_EROSION_REGION_NAMES if not right else RH_WRIST_EROSION_REGION_NAMES\n",
        "\n",
        "  for region_class, image in wrist.items():\n",
        "    score_column = E_COLUMNS[ region_class - 1 ]\n",
        "\n",
        "    score = evaluate_ensemble(erosion_ensemble, image)\n",
        "    score = denormalize_labels(np.array([ score ]), HAND_EROSION_SCALING, SMOOTHING)\n",
        "\n",
        "    results[score_column] = score\n",
        "\n",
        "  return results\n",
        "\n",
        "def evaluate_wrist_narrowing(wrist, right, narrowing_ensemble):\n",
        "  results = dict()\n",
        "\n",
        "  N_COLUMNS = LH_WRIST_NARROWING_REGION_NAMES if not right else RH_WRIST_NARROWING_REGION_NAMES\n",
        "\n",
        "  for region_class, image in wrist.items():\n",
        "    score_column = N_COLUMNS[ region_class - 1 ]\n",
        "\n",
        "    score = evaluate_ensemble(narrowing_ensemble, image)\n",
        "    score = denormalize_labels(np.array([ score ]), HAND_NARROWING_SCALING, SMOOTHING)\n",
        "\n",
        "    results[score_column] = score\n",
        "  \n",
        "  return results\n",
        "\n",
        "def compose_results_and_fill_empties(LH, RH, LF, RF, LWE, RWE, LWN, RWN):\n",
        "  #Extract evaluations per category to later combine them and compute mean values\n",
        "  #which will be used to fill empties\n",
        "\n",
        "  LHE = [ LH[name] for name in LH_FINGER_EROSION_REGION_NAMES if name in LH.keys() ]\n",
        "  RHE = [ RH[name] for name in RH_FINGER_EROSION_REGION_NAMES if name in RH.keys() ]\n",
        "\n",
        "  LHN = [ LH[name] for name in LH_FINGER_NARROWING_REGION_NAMES if name in LH.keys() ]\n",
        "  RHN = [ RH[name] for name in RH_FINGER_NARROWING_REGION_NAMES if name in RH.keys() ]\n",
        "\n",
        "  LWE = [ LWE[name] for name in LH_WRIST_EROSION_REGION_NAMES if name in LWE.keys() ]\n",
        "  RWE = [ RWE[name] for name in RH_WRIST_EROSION_REGION_NAMES if name in RWE.keys() ]\n",
        "\n",
        "  LWN = [ LWN[name] for name in LH_WRIST_NARROWING_REGION_NAMES if name in LWN.keys() ]\n",
        "  RWN = [ RWN[name] for name in RH_WRIST_NARROWING_REGION_NAMES if name in RWN.keys() ]\n",
        "\n",
        "  LFE = [ LF[name] for name in LF_EROSION_REGION_NAMES if name in LF.keys() ]\n",
        "  RFE = [ RF[name] for name in RF_EROSION_REGION_NAMES if name in RF.keys() ]\n",
        "\n",
        "  LFN = [ LF[name] for name in LF_NARROWING_REGION_NAMES if name in LF.keys() ]\n",
        "  RFN = [ RF[name] for name in RF_NARROWING_REGION_NAMES if name in RF.keys() ]\n",
        "\n",
        "  HAND_E_MEAN = np.array(LHE + RHE + LWE + RWE).mean()\n",
        "  HAND_N_MEAN = np.array(LHN + RHN + LWN + RWN).mean()\n",
        "\n",
        "  FEET_E_MEAN = np.array(LFE + RFE).mean()\n",
        "  FEET_N_MEAN = np.array(LFN + RFN).mean()\n",
        "\n",
        "  results = dict()\n",
        "  for d in [ LH, RH, LF, RF, LWE, RWE, LWN, RWN ]:\n",
        "    results.update(d)\n",
        "\n",
        "  HAND_E_NAMES = LH_FINGER_EROSION_REGION_NAMES + LH_WRIST_EROSION_REGION_NAMES + RH_FINGER_EROSION_REGION_NAMES + RH_WRIST_EROSION_REGION_NAMES\n",
        "  HAND_N_NAMES = LH_FINGER_NARROWING_REGION_NAMES + LH_WRIST_NARROWING_REGION_NAMES + RH_FINGER_NARROWING_REGION_NAMES + RH_WRIST_NARROWING_REGION_NAMES\n",
        "\n",
        "  FEET_E_NAMES = LF_EROSION_REGION_NAMES + RF_EROSION_REGION_NAMES\n",
        "  FEET_N_NAMES = LF_NARROWING_REGION_NAMES + RF_NARROWING_REGION_NAMES\n",
        "\n",
        "  [ results.update({ name : HAND_E_MEAN }) for name in HAND_E_NAMES if name not in results.keys() ]\n",
        "  [ results.update({ name : HAND_N_MEAN }) for name in HAND_N_NAMES if name not in results.keys() ]\n",
        "  [ results.update({ name : FEET_E_MEAN }) for name in FEET_E_NAMES if name not in results.keys() ]\n",
        "  [ results.update({ name : FEET_N_MEAN }) for name in FEET_N_NAMES if name not in results.keys() ]\n",
        "\n",
        "  Overall_erosion = np.array([results[name] for name in HAND_E_NAMES + FEET_E_NAMES ]).sum()\n",
        "  Overall_narrowing = np.array([results[name] for name in HAND_N_NAMES + FEET_N_NAMES ]).sum()\n",
        "  Overall_Tol = Overall_erosion + Overall_narrowing\n",
        "\n",
        "  results.update({\n",
        "      'Overall_erosion' : Overall_erosion,\n",
        "      'Overall_narrowing' : Overall_narrowing,\n",
        "      'Overall_Tol' : Overall_Tol,\n",
        "  })\n",
        "\n",
        "  return results\n",
        "\n",
        "def test(dataframe_path, images_path, df_output_path, ensembles):\n",
        "  data = read_training_dataset(dataframe_path, images_path)\n",
        "  output_df = pd.read_csv(dataframe_path)\n",
        "  evaluations = dict()\n",
        "\n",
        "  FEET_E = ensembles['FEET_EROSION']\n",
        "  FEET_N = ensembles['FEET_NARROWING']\n",
        "  HAND_E = ensembles['HAND_EROSION']\n",
        "  HAND_N = ensembles['HAND_NARROWING']\n",
        "\n",
        "  WRIST_E = ensembles['WRIST_EROSION'] if 'WRIST_EROSION' in ensembles.keys() else HAND_E\n",
        "  WRIST_N = ensembles['WRIST_NARROWING'] if 'WRIST_NARROWING' in ensembles.keys() else HAND_N\n",
        "  \n",
        "  for p_id, regions in data.items():\n",
        "    p_data = data[p_id]\n",
        "    LH, RH = p_data['LH'], p_data['RH']\n",
        "    LF, RF = p_data['LF'], p_data['RF']\n",
        "    LWE, RWE = p_data['LWE'], p_data['RWE']\n",
        "    LWN, RWN = p_data['LWN'], p_data['RWN']\n",
        "\n",
        "    LH, RH = evaluate_hands(LH, False, HAND_E, HAND_N), evaluate_hands(RH, True, HAND_E, HAND_N)\n",
        "    LF, RF = evaluate_feet(LF, False, FEET_E, FEET_N), evaluate_feet(RF, True, FEET_E, FEET_N)\n",
        "    LWE, RWE = evaluate_wrist_erosion(LWE, False, WRIST_E), evaluate_wrist_erosion(RWE, True, WRIST_E)\n",
        "    LWN, RWN = evaluate_wrist_narrowing(LWN, False, WRIST_N), evaluate_wrist_narrowing(RWN, TRUE, WRIST_N)\n",
        "    evaluations[p_id] = compose_results_and_fill_empties(LH,RH, LF, RF, LWE, RWE, LWN, RWN)\n",
        "\n",
        "  for index, row in output_df.iterrows():\n",
        "    p_id = row['Patient_ID']\n",
        "    for column in output_df.columns:\n",
        "      if column != 'Patient_ID':\n",
        "        output_df.loc[index, column] = evaluations[p_id][column]\n",
        "  \n",
        "  return output_df"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([10, 6, 9, 7, 8, 5, 1, 2, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj2PqTOeX8-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dGi1Cb-dr05",
        "colab_type": "text"
      },
      "source": [
        "#TODO: Test empty wrist\n",
        "#TODO: Model loading function\n",
        "#TODO: Check the estimator definitions\n",
        "#TODO: Check the data set parsing\n",
        "#----\n",
        "#TODO: Change paths"
      ]
    }
  ]
}