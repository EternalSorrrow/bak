{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ra2_evaluators.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNn5mUV4x+icWvp5J74X2hB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EternalSorrrow/bak/blob/master/ra2_evaluators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7utoiErS35Vg",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "ecfeb3fe-3424-45d8-ccf3-d3d9c9f86c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "!git clone https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "%cd Mask_RCNN\n",
        "!python setup.py install\n",
        "!pip show mask-rcnn\n",
        "\n",
        "%cd ..\n",
        "#!pip3 install imgaug"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 111.84 MiB | 17.53 MiB/s, done.\n",
            "Resolving deltas: 100% (570/570), done.\n",
            "/content/Mask_RCNN\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating mask_rcnn.egg-info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/mrcnn\n",
            "copying mrcnn/config.py -> build/lib/mrcnn\n",
            "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "copying mrcnn/model.py -> build/lib/mrcnn\n",
            "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.6.egg\n",
            "Copying mask_rcnn-2.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding mask-rcnn 2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n",
            "Name: mask-rcnn\n",
            "Version: 2.1\n",
            "Summary: Mask R-CNN for object detection and instance segmentation\n",
            "Home-page: https://github.com/matterport/Mask_RCNN\n",
            "Author: Matterport\n",
            "Author-email: waleed.abdulla@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Requires: \n",
            "Required-by: \n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "101_aZ1s3-9o",
        "colab_type": "code",
        "outputId": "c15bab42-c2a9-45ba-997d-998872424db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#@title Mount the Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QmMqlSLCDxP",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "20da0e3d-6407-487a-a9a6-f518e7551df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "#@title Import Mask R-CNN dependencies\n",
        "\n",
        "%cd Mask_RCNN/\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n",
        "import mrcnn\n",
        "from mrcnn.utils import Dataset\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "from os import listdir\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "%cd .."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Mask_RCNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4MjaFGV4Rf2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Configurations for pre-trained joint extractors\n",
        "\n",
        "max_instances_to_detect = 128\n",
        "\n",
        "feet_regions = 6\n",
        "hand_regions = 11\n",
        "\n",
        "MODEL_DIR = 'logs'\n",
        "\n",
        "class FeetJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"FeetJoints_config\"\n",
        " \n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        " \n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = feet_regions + 1\n",
        "   \n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = 1\n",
        "    #VALIDATION_STEPS = 1\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "    \n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "    \n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=feet_regions\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "\n",
        "class HandJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"HandJoints_config\"\n",
        " \n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        " \n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = hand_regions + 1\n",
        "   \n",
        "    # Number of training steps per epoch\n",
        "    #STEPS_PER_EPOCH = max(1, train_samples // IMAGES_PER_GPU)\n",
        "    #VALIDATION_STEPS = max(1, val_samples // IMAGES_PER_GPU)\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "    \n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "    \n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=hand_regions\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "\n",
        "f_config = FeetJointsConfig()\n",
        "h_config = HandJointsConfig()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gXvah7Y-nkT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Paths definition\n",
        "\n",
        "train_set_path     = 'drive/My Drive/Work/ML/RA2/ra2/train/'\n",
        "hand_detector_path = '/content/drive/My Drive/Work/ML/RA2/ra2/hands_subset/weights/model_6/mrcnn_hand_mrcnn_class_loss_best-200.hdf5'\n",
        "feet_detector_path = '/content/drive/My Drive/Work/ML/RA2/ra2/feet_subset/weights/model_5/mrcnn_feet_mrcnn_class_loss_best-160.hdf5'\n",
        "dataframe_path     = 'drive/My Drive/Work/ML/RA2/ra2/train/training.csv'\n",
        "\n",
        "DENSENET121_WEIGHT_PATH_NO_TOP = '/content/drive/My Drive/Work/ML/RA2/other/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "DENSENET169_WEIGHT_PATH_NO_TOP = '/content/drive/My Drive/Work/ML/RA2/other/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "DENSENET201_WEIGHT_PATH_NO_TOP = '/content/drive/My Drive/Work/ML/RA2/other/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-R9RCxWBTMb",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Column names per joint type\n",
        "\n",
        "feet_detector_class_names = {\n",
        "    0 : 'background',\n",
        "    1 : 'mtp_1',\n",
        "    2 : 'mtp_2',\n",
        "    3 : 'mtp_3',\n",
        "    4 : 'mtp_4',\n",
        "    5 : 'mtp_5',\n",
        "    6 : 'mtp_ip',\n",
        "}\n",
        "\n",
        "hand_detector_class_names = {\n",
        "    0 : 'background',\n",
        "    1 : 'pip_1',\n",
        "    2 : 'pip_2',\n",
        "    3 : 'pip_3',\n",
        "    4 : 'pip_4',\n",
        "    5 : 'pip_5',\n",
        "    6 : 'mcp_1',\n",
        "    7 : 'mcp_2',\n",
        "    8 : 'mcp_3',\n",
        "    9 : 'mcp_4',\n",
        "    10 : 'mcp_5',\n",
        "    11 : 'carp',\n",
        "}\n",
        "\n",
        "feet_left_narrowing_class_names = {\n",
        "    0 : 'background',\n",
        "    1 : 'LF_mtp_J__1',\n",
        "    2 : 'LF_mtp_J__2',\n",
        "    3 : 'LF_mtp_J__3',\n",
        "    4 : 'LF_mtp_J__4',\n",
        "    5 : 'LF_mtp_J__5',\n",
        "    6 : 'LF_mtp_J__ip',\n",
        "}\n",
        "\n",
        "feet_right_narrowing_class_names = {\n",
        "    0 : 'background',\n",
        "    1 : 'RF_mtp_J__1',\n",
        "    2 : 'RF_mtp_J__2',\n",
        "    3 : 'RF_mtp_J__3',\n",
        "    4 : 'RF_mtp_J__4',\n",
        "    5 : 'RF_mtp_J__5',\n",
        "    6 : 'RF_mtp_J__ip',\n",
        "}\n",
        "\n",
        "feet_left_erosion_class_names = {\n",
        "    0 : 'background',\n",
        "    1 : 'LF_mtp_E__1',\n",
        "    2 : 'LF_mtp_E__2',\n",
        "    3 : 'LF_mtp_E__3',\n",
        "    4 : 'LF_mtp_E__4',\n",
        "    5 : 'LF_mtp_E__5',\n",
        "    6 : 'LF_mtp_E__ip',\n",
        "}\n",
        "\n",
        "feet_right_erosion_class_names = {\n",
        "    0 : 'background',\n",
        "    1 : 'RF_mtp_E__1',\n",
        "    2 : 'RF_mtp_E__2',\n",
        "    3 : 'RF_mtp_E__3',\n",
        "    4 : 'RF_mtp_E__4',\n",
        "    5 : 'RF_mtp_E__5',\n",
        "    6 : 'RF_mtp_E__ip',\n",
        "}\n",
        "\n",
        "finger_left_erosion_names = {\n",
        "    0  : 'background',\n",
        "    1  : 'LH_mcp_E__ip',\n",
        "    2  : 'LH_pip_E__2',\n",
        "    3  : 'LH_pip_E__3',\n",
        "    4  : 'LH_pip_E__4',\n",
        "    5  : 'LH_pip_E__5',\n",
        "    6  : 'LH_mcp_E__1',\n",
        "    7  : 'LH_mcp_E__2',\n",
        "    8  : 'LH_mcp_E__3',\n",
        "    9  : 'LH_mcp_E__4',\n",
        "    10 : 'LH_mcp_E__5',\n",
        "}\n",
        "\n",
        "finger_right_erosion_names = {\n",
        "    0  : 'background',\n",
        "    1  : 'RH_mcp_E__ip',\n",
        "    2  : 'RH_pip_E__2',\n",
        "    3  : 'RH_pip_E__3',\n",
        "    4  : 'RH_pip_E__4',\n",
        "    5  : 'RH_pip_E__5',\n",
        "    6  : 'RH_mcp_E__1',\n",
        "    7  : 'RH_mcp_E__2',\n",
        "    8  : 'RH_mcp_E__3',\n",
        "    9  : 'RH_mcp_E__4',\n",
        "    10 : 'RH_mcp_E__5',\n",
        "}\n",
        "\n",
        "finger_left_narrowing_names = {\n",
        "    0  : 'background',\n",
        "    1  : 'LH_pip_J__ip', #Not needed to be evaluated\n",
        "    2  : 'LH_pip_J__2',\n",
        "    3  : 'LH_pip_J__3',\n",
        "    4  : 'LH_pip_J__4',\n",
        "    5  : 'LH_pip_J__5',\n",
        "    6  : 'LH_mcp_J__1',\n",
        "    7  : 'LH_mcp_J__2',\n",
        "    8  : 'LH_mcp_J__3',\n",
        "    9  : 'LH_mcp_J__4',\n",
        "    10 : 'LH_mcp_J__5',\n",
        "}\n",
        "\n",
        "finger_right_narrowing_names = {\n",
        "    0  : 'background',\n",
        "    1  : 'RH_pip_J__ip', #Not needed to be evaluated\n",
        "    2  : 'RH_pip_J__2',\n",
        "    3  : 'RH_pip_J__3',\n",
        "    4  : 'RH_pip_J__4',\n",
        "    5  : 'RH_pip_J__5',\n",
        "    6  : 'RH_mcp_J__1',\n",
        "    7  : 'RH_mcp_J__2',\n",
        "    8  : 'RH_mcp_J__3',\n",
        "    9  : 'RH_mcp_J__4',\n",
        "    10 : 'RH_mcp_J__5',\n",
        "}\n",
        "\n",
        "wrist_left_erosion_names = [\n",
        "    'LH_wrist_E__mc1',\n",
        "    'LH_wrist_E__mul',\n",
        "    'LH_wrist_E__nav',\n",
        "    'LH_wrist_E__radius',\n",
        "    'LH_wrist_E__lunate',\n",
        "    'LH_wrist_E__ulna',\n",
        "]\n",
        "\n",
        "wrist_right_erosion_names = [\n",
        "    'RH_wrist_E__mc1',\n",
        "    'RH_wrist_E__mul',\n",
        "    'RH_wrist_E__nav',\n",
        "    'RH_wrist_E__radius',\n",
        "    'RH_wrist_E__lunate',\n",
        "    'RH_wrist_E__ulna',\n",
        "]\n",
        "\n",
        "wrist_left_narrowing_names = [\n",
        "    'LH_wrist_J__mna',\n",
        "    'LH_wrist_J__capnlun',\n",
        "    'LH_wrist_J__radcar',\n",
        "    'LH_wrist_J__cmc3',\n",
        "    'LH_wrist_J__cmc4',\n",
        "    'LH_wrist_J__cmc5',\n",
        "]\n",
        "\n",
        "wrist_right_narrowing_names = [\n",
        "    'RH_wrist_J__mna',\n",
        "    'RH_wrist_J__capnlun',\n",
        "    'RH_wrist_J__radcar',\n",
        "    'RH_wrist_J__cmc3',\n",
        "    'RH_wrist_J__cmc4',\n",
        "    'RH_wrist_J__cmc5',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgxFOcAB-xlD",
        "colab_type": "code",
        "outputId": "8ed7213e-17ba-45fa-a8b7-f764f83887ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "cellView": "both"
      },
      "source": [
        "#@title Create detectors and load weights\n",
        "\n",
        "feet_detector = modellib.MaskRCNN(mode=\"inference\", config=f_config, model_dir=MODEL_DIR)\n",
        "hand_detector = modellib.MaskRCNN(mode=\"inference\", config=h_config, model_dir=MODEL_DIR)\n",
        "\n",
        "feet_detector.load_weights(feet_detector_path, by_name=True)\n",
        "hand_detector.load_weights(hand_detector_path, by_name=True)\n",
        "\n",
        "feet_detector_classes = len(list(feet_detector_class_names.keys())) - 1\n",
        "hand_detector_classes = len(list(hand_detector_class_names.keys())) - 1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:758: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:760: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efvcrNPzA6X8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "cellView": "both",
        "outputId": "ff4dad92-c213-4b30-c655-1f9d1f97957b"
      },
      "source": [
        "#@title Read dataframe with labels\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "df = pd.read_csv(dataframe_path)\n",
        "\n",
        "feet_column_names  = [ column for column in df.columns if column == 'Patient_ID' or 'RF' in column or 'LF' in column ]\n",
        "feet_left_columns  = [ column for column in df.columns if column == 'Patient_ID' or 'LF' in column ]\n",
        "feet_right_columns = [ column for column in df.columns if column == 'Patient_ID' or 'RF' in column ]\n",
        "\n",
        "hand_column_names  = [ column for column in df.columns if column == 'Patient_ID' or 'RH' in column or 'LH' in column ]\n",
        "hand_left_columns  = [ column for column in df.columns if column == 'Patient_ID' or 'LH' in column ]\n",
        "hand_right_columns = [ column for column in df.columns if column == 'Patient_ID' or 'RH' in column ]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>Overall_Tol</th>\n",
              "      <th>Overall_erosion</th>\n",
              "      <th>Overall_narrowing</th>\n",
              "      <th>LH_mcp_E__ip</th>\n",
              "      <th>LH_pip_E__2</th>\n",
              "      <th>LH_pip_E__3</th>\n",
              "      <th>LH_pip_E__4</th>\n",
              "      <th>LH_pip_E__5</th>\n",
              "      <th>LH_mcp_E__1</th>\n",
              "      <th>LH_mcp_E__2</th>\n",
              "      <th>LH_mcp_E__3</th>\n",
              "      <th>LH_mcp_E__4</th>\n",
              "      <th>LH_mcp_E__5</th>\n",
              "      <th>LH_wrist_E__mc1</th>\n",
              "      <th>LH_wrist_E__mul</th>\n",
              "      <th>LH_wrist_E__nav</th>\n",
              "      <th>LH_wrist_E__lunate</th>\n",
              "      <th>LH_wrist_E__radius</th>\n",
              "      <th>LH_wrist_E__ulna</th>\n",
              "      <th>RH_mcp_E__ip</th>\n",
              "      <th>RH_pip_E__2</th>\n",
              "      <th>RH_pip_E__3</th>\n",
              "      <th>RH_pip_E__4</th>\n",
              "      <th>RH_pip_E__5</th>\n",
              "      <th>RH_mcp_E__1</th>\n",
              "      <th>RH_mcp_E__2</th>\n",
              "      <th>RH_mcp_E__3</th>\n",
              "      <th>RH_mcp_E__4</th>\n",
              "      <th>RH_mcp_E__5</th>\n",
              "      <th>RH_wrist_E__mc1</th>\n",
              "      <th>RH_wrist_E__mul</th>\n",
              "      <th>RH_wrist_E__nav</th>\n",
              "      <th>RH_wrist_E__lunate</th>\n",
              "      <th>RH_wrist_E__radius</th>\n",
              "      <th>RH_wrist_E__ulna</th>\n",
              "      <th>LF_mtp_E__ip</th>\n",
              "      <th>LF_mtp_E__1</th>\n",
              "      <th>LF_mtp_E__2</th>\n",
              "      <th>LF_mtp_E__3</th>\n",
              "      <th>...</th>\n",
              "      <th>LH_pip_J__4</th>\n",
              "      <th>LH_pip_J__5</th>\n",
              "      <th>LH_mcp_J__1</th>\n",
              "      <th>LH_mcp_J__2</th>\n",
              "      <th>LH_mcp_J__3</th>\n",
              "      <th>LH_mcp_J__4</th>\n",
              "      <th>LH_mcp_J__5</th>\n",
              "      <th>LH_wrist_J__cmc3</th>\n",
              "      <th>LH_wrist_J__cmc4</th>\n",
              "      <th>LH_wrist_J__cmc5</th>\n",
              "      <th>LH_wrist_J__mna</th>\n",
              "      <th>LH_wrist_J__capnlun</th>\n",
              "      <th>LH_wrist_J__radcar</th>\n",
              "      <th>RH_pip_J__2</th>\n",
              "      <th>RH_pip_J__3</th>\n",
              "      <th>RH_pip_J__4</th>\n",
              "      <th>RH_pip_J__5</th>\n",
              "      <th>RH_mcp_J__1</th>\n",
              "      <th>RH_mcp_J__2</th>\n",
              "      <th>RH_mcp_J__3</th>\n",
              "      <th>RH_mcp_J__4</th>\n",
              "      <th>RH_mcp_J__5</th>\n",
              "      <th>RH_wrist_J__cmc3</th>\n",
              "      <th>RH_wrist_J__cmc4</th>\n",
              "      <th>RH_wrist_J__cmc5</th>\n",
              "      <th>RH_wrist_J__mna</th>\n",
              "      <th>RH_wrist_J__capnlun</th>\n",
              "      <th>RH_wrist_J__radcar</th>\n",
              "      <th>LF_mtp_J__ip</th>\n",
              "      <th>LF_mtp_J__1</th>\n",
              "      <th>LF_mtp_J__2</th>\n",
              "      <th>LF_mtp_J__3</th>\n",
              "      <th>LF_mtp_J__4</th>\n",
              "      <th>LF_mtp_J__5</th>\n",
              "      <th>RF_mtp_J__ip</th>\n",
              "      <th>RF_mtp_J__1</th>\n",
              "      <th>RF_mtp_J__2</th>\n",
              "      <th>RF_mtp_J__3</th>\n",
              "      <th>RF_mtp_J__4</th>\n",
              "      <th>RF_mtp_J__5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UAB001</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UAB002</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UAB003</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>UAB005</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UAB006</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 90 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID  Overall_Tol  ...  RF_mtp_J__4  RF_mtp_J__5\n",
              "0     UAB001           45  ...            0            2\n",
              "1     UAB002            2  ...            0            0\n",
              "2     UAB003            0  ...            0            0\n",
              "3     UAB005            6  ...            0            0\n",
              "4     UAB006            1  ...            0            0\n",
              "\n",
              "[5 rows x 90 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDx3VPJlzE8o",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Auxilary drawing function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQTUgEY72lqw",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Functions to extract regions and fix errors\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def fix_feet_misclassification(instances):\n",
        "\n",
        "  #Order in which missing regions will be fixed\n",
        "  #Most important -> less important\n",
        "  priority_order = [1, 6, 2, 3, 4, 5]\n",
        "\n",
        "  #Define, which classes where not detected\n",
        "  classes_to_detect = sorted(feet_detector_class_names.keys())[1:]\n",
        "  empty_classes = set(classes_to_detect) - set(instances.keys())\n",
        "  empty_classes = list(empty_classes)\n",
        "  empty_classes = [ item for item in priority_order if item in empty_classes ] #Sort empty classes according to the priority order\n",
        "\n",
        "  fixed_instances = deepcopy(instances)\n",
        "\n",
        "  detected_instances_count = sum(\n",
        "      [ len(item) for key, item in instances.items() ]\n",
        "  )\n",
        "\n",
        "  #Look for a missing region in another class's detections\n",
        "  for empty_class in empty_classes:\n",
        "\n",
        "    if empty_class == 6:#If mtp_ip not detected\n",
        "      \n",
        "      if(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ] #Use the second activation of mtp_1\n",
        "      \n",
        "    elif empty_class == 1: #If mtp_1 not detected\n",
        "\n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ] #Use the second activation of mtp_ip\n",
        "\n",
        "    elif empty_class == 2:#If mtp_2 not detected\n",
        "      \n",
        "      if(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ] #Use the second activation of mtp_3\n",
        "      \n",
        "      elif (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of mtp_4\n",
        "    \n",
        "    elif empty_class == 3:#If mtp_3 not detected, may be the most common case \n",
        "      \n",
        "      if (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of mtp_4\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ] #Use the second activation of mtp_2\n",
        "    \n",
        "    elif empty_class == 4:#If mtp_4 not detected, may be the second most common case \n",
        "      \n",
        "      if (len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ]#Use the second activation of mtp_3\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ]#Use the second activation of mtp_2\n",
        "\n",
        "    elif empty_class == 5:#If mtp_5 not detected \n",
        "      \n",
        "      if (len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mtp_ip\n",
        "\n",
        "      elif(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ]#Use the second activation of mtp_1\n",
        "\n",
        "      elif(len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ]#Use the second activation of mtp_4\n",
        "\n",
        "  return fixed_instances\n",
        "\n",
        "def fix_hands_misclassification(instances):\n",
        "  \n",
        "  #Order in which missing regions will be fixed\n",
        "  #Most important -> less important\n",
        "  priority_order = [11, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5] #carp, mcp, pip\n",
        "\n",
        "  #Define, which classes where not detected\n",
        "  classes_to_detect = sorted(hand_detector_class_names.keys())[1:]\n",
        "  empty_classes = set(classes_to_detect) - set(instances.keys())\n",
        "  empty_classes = list(empty_classes)\n",
        "  empty_classes = [ item for item in priority_order if item in empty_classes ] #Sort empty classes according to the priority order\n",
        "\n",
        "  fixed_instances = deepcopy(instances)\n",
        "\n",
        "  detected_instances_count = sum(\n",
        "      [ len(item) for key, item in instances.items() ]\n",
        "  )\n",
        "\n",
        "  #Look for a missing region in another class's detections\n",
        "  for empty_class in empty_classes:\n",
        "    \n",
        "    if empty_class == 6:#If mcp_1 not detected\n",
        "      \n",
        "      if(len(fixed_instances[10]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[10].pop(1) ] #Use the second activation of mcp_5\n",
        "      \n",
        "      elif (len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ] #Use the second activation of pip_1\n",
        "    \n",
        "    elif empty_class == 7:#If mcp_2 not detected\n",
        "      \n",
        "      if(len(fixed_instances[8]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[8].pop(1) ] #Use the second activation of mcp_3\n",
        "      \n",
        "      elif (len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ] #Use the second activation of mcp_4\n",
        "    \n",
        "    elif empty_class == 8:#If mcp_3 not detected\n",
        "      \n",
        "      if (len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ] #Use the second activation of mcp_4\n",
        "\n",
        "      elif(len(fixed_instances[7]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[7].pop(1) ] #Use the second activation of mcp_2\n",
        "    \n",
        "    elif empty_class == 9:#If mcp_4 not detected \n",
        "      \n",
        "      if (len(fixed_instances[8]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[8].pop(1) ]#Use the second activation of mcp_3\n",
        "\n",
        "      elif(len(fixed_instances[7]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[7].pop(1) ]#Use the second activation of mcp_2\n",
        "\n",
        "    elif empty_class == 10:#If mcp_5 not detected \n",
        "      \n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mcp_1\n",
        "\n",
        "      elif(len(fixed_instances[9]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[9].pop(1) ]#Use the second activation of mcp_4\n",
        "\n",
        "    elif empty_class == 1:#If pip_1 not detected\n",
        "      \n",
        "      if(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ] #Use the second activation of mcp_1\n",
        "      \n",
        "      elif (len(fixed_instances[5]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[5].pop(1) ] #Use the second activation of pip_5\n",
        "    \n",
        "    elif empty_class == 2:#If pip_2 not detected\n",
        "      \n",
        "      if(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ] #Use the second activation of pip_3\n",
        "      \n",
        "      elif (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of pip_4\n",
        "    \n",
        "    elif empty_class == 3:#If pip_3 not detected\n",
        "      \n",
        "      if (len(fixed_instances[4]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[4].pop(1) ] #Use the second activation of pip_4\n",
        "\n",
        "      elif(len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ] #Use the second activation of pip_2\n",
        "    \n",
        "    elif empty_class == 4:#If pip_4 not detected \n",
        "      \n",
        "      if (len(fixed_instances[2]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[2].pop(1) ]#Use the second activation of pip_2\n",
        "\n",
        "      elif(len(fixed_instances[3]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[3].pop(1) ]#Use the second activation of pip_3\n",
        "\n",
        "    elif empty_class == 5:#If pip_5 not detected \n",
        "      \n",
        "      if(len(fixed_instances[1]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[1].pop(1) ]#Use the second activation of pip_5\n",
        "\n",
        "      elif(len(fixed_instances[6]) > 1):\n",
        "        fixed_instances[empty_class] = [ fixed_instances[6].pop(1) ]#Use the second activation of mcp_1\n",
        "  \n",
        "  return fixed_instances\n",
        "\n",
        "def extract_regions(detector, image, classes_num, fix_function):\n",
        "  result = detector.detect([ image ], verbose=0)\n",
        "  result = result[0]\n",
        "\n",
        "  #Reshape masks from a 3d [H, W, Instances] tensor to the array of masks\n",
        "  masks = []\n",
        "  for i in range(result['masks'].shape[2]):\n",
        "    masks.append(result['masks'][:, :, i])\n",
        "  \n",
        "  #Reshape dictionary to list\n",
        "  result = list(zip(\n",
        "      result['rois'],\n",
        "      result['class_ids'],\n",
        "      result['scores'],\n",
        "      masks\n",
        "  ))\n",
        "\n",
        "  groupped = dict()\n",
        "  \n",
        "  #Group detected instances by class\n",
        "  for item in result:\n",
        "    roi, class_id, score, mask = item\n",
        "\n",
        "    if class_id not in groupped.keys():\n",
        "      groupped[class_id] = []\n",
        "    \n",
        "    groupped[class_id].append((score, roi, mask))\n",
        "\n",
        "  #Sort each group by confidence scores\n",
        "  for key in groupped.keys():\n",
        "    groupped[key] = sorted(groupped[key], key = lambda x: x[0], reverse=True)\n",
        "\n",
        "  detected_classes = len(list(groupped.keys()))\n",
        "\n",
        "\n",
        "  #Ensure the model has detected all needed regions\n",
        "  misclassified = False\n",
        "\n",
        "  #Some joints may be misclassified, in most cases it may be fixed in trivial way\n",
        "  if detected_classes < classes_num:\n",
        "    groupped = fix_function(groupped)\n",
        "    misclassified = True\n",
        "\n",
        "  #Take max confidence result for each class as a region proposal \n",
        "  result = dict(\n",
        "      [ (key, groupped[key][0]) for key in groupped.keys() ]\n",
        "  )\n",
        "\n",
        "  return result, misclassified\n",
        "\n",
        "def convert_to_original_format(instances):\n",
        "  #Reshape { class : (score, roi, mask) } dictionary to the format the results are yielded by models\n",
        "  #Useful for visualization with MRCNN built-in functions\n",
        "  classes, rois, scores, masks = [], [], [], []\n",
        "\n",
        "  for key, value in instances.items():\n",
        "    score, roi, mask = value\n",
        "\n",
        "    classes.append(key)\n",
        "    rois.append(roi)\n",
        "    scores.append(score)\n",
        "    masks.append(mask)\n",
        "  \n",
        "  result = {\n",
        "      'rois' : np.array(rois),\n",
        "      'scores' : np.array(scores),\n",
        "      'class_ids' : np.array(classes),\n",
        "      'masks' : np.array(masks).swapaxes(0,2).swapaxes(0,1)\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "def extract_regions_from_image(image, result, scale=1.0):\n",
        "  regions = dict()\n",
        "\n",
        "  for class_, region in zip(result['class_ids'], result['rois']):\n",
        "    #Rescale the box\n",
        "    x1, y1, x2, y2, = region\n",
        "    cx, cy = (x2 + x1)/2, (y2 + y1)/2\n",
        "    x1, x2 = int((x1-cx) * scale + cx), int((x2-cx) * scale + cx)\n",
        "    y1, y2 = int((y1-cy) * scale + cy), int((y2-cy) * scale + cy) \n",
        "\n",
        "    #Yes, yes, i know, y-axis is the 0 axis, but whatever\n",
        "    x1, x2, y1, y2 = max(0, x1), min(x2, image.shape[0]), max(0, y1), min(y2, image.shape[1])\n",
        "\n",
        "    regions[class_] = image[ x1:x2, y1:y2 ]\n",
        "\n",
        "  return regions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evFYNpiJ-hwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hand_scale = 1.2\n",
        "feet_scale = 1.2\n",
        "\n",
        "feet_image_shape   = (64,  64,  3)\n",
        "finger_image_shape = (64,  64,  3)\n",
        "wrist_image_shape  = (256, 256, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKqEq6CR43vU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "5c8507cd-1010-4285-9327-c9d8d59b8c53"
      },
      "source": [
        "#@title Read and apply detector on all images\n",
        "\n",
        "def apply_detectors():\n",
        "  extracted_regions = dict()\n",
        "\n",
        "  for i, p_id in zip(range(len(df['Patient_ID'])), df['Patient_ID']):\n",
        "    #Save dataframe line as images metadata\n",
        "    patient = {\n",
        "        'info' : df[ df['Patient_ID'] == p_id ],\n",
        "    }\n",
        "\n",
        "    #Compose full image filenames\n",
        "    left_foot_path  = train_set_path + p_id + '-LF.jpg'\n",
        "    right_foot_path = train_set_path + p_id + '-RF.jpg'\n",
        "    left_hand_path  = train_set_path + p_id + '-LH.jpg'\n",
        "    right_hand_path = train_set_path + p_id + '-RH.jpg'\n",
        "\n",
        "    #Read images\n",
        "    left_foot_image  = cv2.imread(left_foot_path)\n",
        "    right_foot_image = cv2.imread(right_foot_path)\n",
        "    left_hand_image  = cv2.imread(left_hand_path)\n",
        "    right_hand_image = cv2.imread(right_hand_path)\n",
        "\n",
        "    #Flip right images, since extractors are trained on left/flipped right foot images\n",
        "    right_foot_image = np.flip(right_foot_image, axis=1)\n",
        "    right_hand_image = np.flip(right_hand_image, axis=1)\n",
        "\n",
        "    #Detect regions on images\n",
        "    left_foot_regions,  lf_mis = extract_regions(feet_detector, left_foot_image,  feet_detector_classes, fix_feet_misclassification)\n",
        "    right_foot_regions, rf_mis = extract_regions(feet_detector, right_foot_image, feet_detector_classes, fix_feet_misclassification)\n",
        "    left_hand_regions,  lh_mis = extract_regions(hand_detector, left_hand_image,  hand_detector_classes, fix_hands_misclassification)\n",
        "    right_hand_regions, rh_mis = extract_regions(hand_detector, right_hand_image, hand_detector_classes, fix_hands_misclassification)\n",
        "\n",
        "    #Output info about possibly badly viewed images  \n",
        "    for misclassified, filename, regions in zip([lf_mis,            rf_mis,            lh_mis,             rh_mis],\n",
        "                                       [left_foot_path,    right_foot_path,   left_hand_path,     right_hand_path],\n",
        "                                       [left_foot_regions, right_foot_regions, left_hand_regions, right_hand_regions]):\n",
        "      if misclassified:\n",
        "        fixed_classes_count = len(list(regions.values()))\n",
        "        print('Classified less classes than expected in file', filename + ',', fixed_classes_count, 'classes after fix')\n",
        "\n",
        "    #Reshape a dictionary to a Matterport format, applicable for their visualization functions\n",
        "    left_foot_regions  = convert_to_original_format(left_foot_regions)\n",
        "    right_foot_regions = convert_to_original_format(right_foot_regions)\n",
        "    left_hand_regions  = convert_to_original_format(left_hand_regions)\n",
        "    right_hand_regions = convert_to_original_format(right_hand_regions)\n",
        "\n",
        "    #Extract detected region patches from images and save them\n",
        "    patient['LF'] = extract_regions_from_image(left_foot_image,  left_foot_regions,  scale=feet_scale)\n",
        "    patient['RF'] = extract_regions_from_image(right_foot_image, right_foot_regions, scale=feet_scale)\n",
        "    patient['LH'] = extract_regions_from_image(left_hand_image,  left_hand_regions,  scale=hand_scale)\n",
        "    patient['RH'] = extract_regions_from_image(right_hand_image, right_hand_regions, scale=hand_scale)\n",
        "    \n",
        "    extracted_regions[p_id] = patient\n",
        "\n",
        "    if (not ( (i + 1) % 50 )) or (i + 1) == len(df['Patient_ID']):\n",
        "      print('Applied detectors on', i+1, 'UIDs')\n",
        "    \n",
        "  return extracted_regions\n",
        "\n",
        "detected_regions = apply_detectors()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applied detectors on 50 UIDs\n",
            "Applied detectors on 100 UIDs\n",
            "Applied detectors on 150 UIDs\n",
            "Applied detectors on 200 UIDs\n",
            "Applied detectors on 250 UIDs\n",
            "Applied detectors on 300 UIDs\n",
            "Applied detectors on 350 UIDs\n",
            "Applied detectors on 368 UIDs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP8LumiCLJRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Form data sets of detected regions\n",
        "\n",
        "def form_datasets():\n",
        "\n",
        "  foot_images     = []\n",
        "  foot_erosion    = []\n",
        "  foot_narrowing  = []\n",
        "\n",
        "  hand_images     = []\n",
        "  hand_erosion    = []\n",
        "  hand_narrowing  = []\n",
        "\n",
        "  wrist_images    = []\n",
        "  wrist_erosion   = []\n",
        "  wrist_narrowing = []\n",
        "\n",
        "  for p_id, patient_data in sorted(\n",
        "        list(\n",
        "            detected_regions.items()\n",
        "        ), key = lambda x: x[0]\n",
        "    ):\n",
        "    info = patient_data['info']\n",
        "\n",
        "    #Resize and append left foot region images and labels\n",
        "    for region_class, image in sorted(\n",
        "          list(\n",
        "              patient_data['LF'].items()\n",
        "          ), key = lambda x: x[0]\n",
        "      ):\n",
        "      image = cv2.resize(image, feet_image_shape[:-1])\n",
        "      erosion_score = list(\n",
        "          info[ feet_left_erosion_class_names[ region_class ] ]\n",
        "      )[0]\n",
        "\n",
        "      narrowing_score = list(\n",
        "          info[ feet_left_narrowing_class_names[ region_class ] ]\n",
        "      )[0]\n",
        "\n",
        "      foot_images.append(image)\n",
        "      foot_erosion.append(erosion_score)\n",
        "      foot_narrowing.append(narrowing_score)\n",
        "\n",
        "    #Resize and append right foot region images and labels\n",
        "    for region_class, image in sorted(\n",
        "          list(\n",
        "              patient_data['RF'].items()\n",
        "          ), key = lambda x: x[0]\n",
        "      ):\n",
        "      image = cv2.resize(image, feet_image_shape[:-1])\n",
        "      erosion_score = list(\n",
        "          info[ feet_right_erosion_class_names[ region_class ] ]\n",
        "      )[0]\n",
        "\n",
        "      narrowing_score = list(\n",
        "          info[ feet_right_narrowing_class_names[ region_class ] ]\n",
        "      )[0]\n",
        "\n",
        "      foot_images.append(image)\n",
        "      foot_erosion.append(erosion_score)\n",
        "      foot_narrowing.append(narrowing_score)\n",
        "\n",
        "    #Resize and append left hand region images and labels\n",
        "    #Wrist requires some different approach\n",
        "    for region_class, image in sorted(\n",
        "          list(\n",
        "              patient_data['LH'].items()\n",
        "          ), key = lambda x: x[0]\n",
        "      ):\n",
        "      if region_class == 11: #If a wrist image\n",
        "        image = cv2.resize(image, wrist_image_shape[:-1])\n",
        "        erosion_score = list(\n",
        "            info[ wrist_left_erosion_names ].values.flat\n",
        "        )\n",
        "        narrowing_score = list(\n",
        "            info[ wrist_left_narrowing_names ].values.flat\n",
        "        )\n",
        "\n",
        "        wrist_images.append(image)\n",
        "        wrist_erosion.append(erosion_score)\n",
        "        wrist_narrowing.append(narrowing_score)\n",
        "      else:\n",
        "        image = cv2.resize(image, finger_image_shape[:-1])\n",
        "        erosion_score = list(\n",
        "          info[ finger_left_erosion_names[ region_class ] ]\n",
        "        )[0]\n",
        "\n",
        "        #If not mcp_ip, which's narrowing is not evaluated\n",
        "        if region_class != 1:\n",
        "          narrowing_score = list(\n",
        "            info[ finger_left_narrowing_names[ region_class ] ]\n",
        "          )[0]\n",
        "        else:\n",
        "          narrowing_score = -1\n",
        "        \n",
        "        hand_narrowing.append(narrowing_score)\n",
        "        hand_images.append(image)\n",
        "        hand_erosion.append(erosion_score)\n",
        "\n",
        "    for region_class, image in sorted(\n",
        "          list(\n",
        "              patient_data['RH'].items()\n",
        "          ), key = lambda x: x[0]\n",
        "      ):\n",
        "      if region_class == 11: #If a wrist image\n",
        "        image = cv2.resize(image, wrist_image_shape[:-1])\n",
        "        erosion_score = list(\n",
        "            info[ wrist_right_erosion_names ].values.flat\n",
        "        )\n",
        "        narrowing_score = list(\n",
        "            info[ wrist_right_narrowing_names ].values.flat\n",
        "        )\n",
        "\n",
        "        wrist_images.append(image)\n",
        "        wrist_erosion.append(erosion_score)\n",
        "        wrist_narrowing.append(narrowing_score)\n",
        "      else:\n",
        "        image = cv2.resize(image, finger_image_shape[:-1])\n",
        "        erosion_score = list(\n",
        "          info[ finger_right_erosion_names[ region_class ] ]\n",
        "        )[0]\n",
        "\n",
        "        #If not mcp_ip, which's narrowing is not evaluated\n",
        "        if region_class != 1:\n",
        "          narrowing_score = list(\n",
        "            info[ finger_right_narrowing_names[ region_class ] ]\n",
        "          )[0]\n",
        "        else:\n",
        "          narrowing_score = -1\n",
        "        \n",
        "        hand_narrowing.append(narrowing_score)\n",
        "        hand_images.append(image)\n",
        "        hand_erosion.append(erosion_score)\n",
        "\n",
        "  foot_images    = np.array(foot_images)\n",
        "  foot_erosion   = np.array(foot_erosion)\n",
        "  foot_narrowing = np.array(foot_narrowing)\n",
        "\n",
        "  hand_images    = np.array(hand_images)\n",
        "  hand_erosion   = np.array(hand_erosion)\n",
        "  hand_narrowing = np.array(hand_narrowing)\n",
        "\n",
        "  wrist_images   = np.array(wrist_images)\n",
        "  wrist_erosion  = np.array(wrist_erosion)\n",
        "  wrist_narrowing= np.array(wrist_narrowing) \n",
        "\n",
        "  return foot_images,  foot_erosion,  foot_narrowing, \\\n",
        "         hand_images,  hand_erosion,  hand_narrowing, \\\n",
        "         wrist_images, wrist_erosion, wrist_narrowing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efwo8mrpNmVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "191a1cdb-2425-4c2c-85ae-c777ced4235a"
      },
      "source": [
        "feet_joints,   feet_erosion,    feet_narrowing, \\\n",
        "finger_joints, finger_erosion, finger_narrowing, \\\n",
        "wrist_common_joints,  wrist_common_erosion, wrist_common_narrowing = form_datasets()\n",
        "\n",
        "feet_joints.shape,         feet_erosion.shape,         feet_narrowing.shape, \\\n",
        "finger_joints.shape,       finger_erosion.shape,       finger_narrowing.shape, \\\n",
        "wrist_common_joints.shape, wrist_common_erosion.shape, wrist_common_narrowing.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4416, 64, 64, 3),\n",
              " (4416,),\n",
              " (4416,),\n",
              " (7360, 64, 64, 3),\n",
              " (7360,),\n",
              " (7360,),\n",
              " (736, 256, 256, 3),\n",
              " (736, 6),\n",
              " (736, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VtsRSMfaOL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hand_erosion_values = 6\n",
        "hand_narrowing_values = 5\n",
        "\n",
        "feet_erosion_values = 11\n",
        "feet_narrowing_values = 5\n",
        "\n",
        "WRIST_EROSION_OUTPUTS = [\n",
        "    'mc1',\n",
        "    'mul',\n",
        "    'nav',\n",
        "    'radius',\n",
        "    'lunate',\n",
        "    'ulna',\n",
        "]\n",
        "\n",
        "WRIST_NARROWING_OUTPUTS = [\n",
        "    'mna',\n",
        "    'capnlun',\n",
        "    'radcar',\n",
        "    'cmc3',\n",
        "    'cmc4',\n",
        "    'cmc5',\n",
        "]\n",
        "\n",
        "actual_wrist_estimator_input_shape = wrist_image_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkFYbGQxam6i",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Dataset processing functions\n",
        "\n",
        "def remove_minus_ones(data, labels):\n",
        "  new_data, new_labels = [], []\n",
        "  \n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] != -1:\n",
        "      new_data.append(data[i])\n",
        "      new_labels.append(labels[i])\n",
        "  \n",
        "  return np.array(new_data), np.array(new_labels)\n",
        "\n",
        "def normalize_labels(vector, smoothing=0.0, max_value = None):\n",
        "  if max_value is None or max_value < np.max(vector):\n",
        "    max_value = np.max(vector)\n",
        "\n",
        "  vector = vector.astype(np.float64) / max_value\n",
        "  \n",
        "  smooth = lambda x: (0.5 + (x - 0.5) * (1 - smoothing))\n",
        "  return np.array([\n",
        "          smooth(value) for value in vector\n",
        "  ])\n",
        "\n",
        "def denormalize_labels(vector, max_value, smoothing=0.0):\n",
        "  extend = lambda x: (0.5 + (x - 0.5) / (1 - smoothing))\n",
        "  vector = np.array([ extend(value) for value in vector ]) * max_value\n",
        "  return vector\n",
        "\n",
        "def downsample_class(samples, labels, multiplier, target_class):\n",
        "  _, counts = np.unique(labels, return_counts=True)\n",
        "  target_count = int(counts[target_class] * multiplier)\n",
        "\n",
        "  output_data   = []\n",
        "  output_labels = []\n",
        "\n",
        "  sampled_values = 0\n",
        "  for i in range(samples.shape[0]):\n",
        "    if labels[i] == target_class:\n",
        "      if sampled_values >= target_count:\n",
        "        pass\n",
        "      else:\n",
        "        sampled_values += 1\n",
        "        \n",
        "        output_data.append(samples[i])\n",
        "        output_labels.append(labels[i])\n",
        "    else:\n",
        "      output_data.append(samples[i])\n",
        "      output_labels.append(labels[i])\n",
        "\n",
        "  return np.array(output_data), np.array(output_labels)\n",
        "\n",
        "def train_test_split(data, labels, split):\n",
        "  permutation = np.random.permutation(range(labels.shape[0]))\n",
        "  split = int(labels.shape[0] * split)\n",
        "  train_indices, val_indices = permutation[:split], permutation[split:]\n",
        "\n",
        "  return data[train_indices], labels[train_indices], data[val_indices], labels[val_indices]\n",
        "\n",
        "def balanced_train_test_split(data, labels, split, copy_single_instance_to_val=False):\n",
        "  \n",
        "  items_by_class = dict()\n",
        "\n",
        "  for i in range(data.shape[0]):\n",
        "    label = labels[i]\n",
        "\n",
        "    if label not in items_by_class.keys():\n",
        "      items_by_class[label] = []\n",
        "    items_by_class[label].append(data[i])\n",
        "  \n",
        "  x_t, y_t, x_v, y_v = [], [], [], []\n",
        "\n",
        "  for class_ in items_by_class.keys():\n",
        "    items_by_class[class_] = np.array(items_by_class[class_])[\n",
        "        np.random.permutation(range(len(items_by_class[class_])))\n",
        "    ]\n",
        "    items_by_class[class_] = list(items_by_class[class_])\n",
        "    separator = round(len(items_by_class[class_]) * split)\n",
        "\n",
        "    ts = items_by_class[class_][ :separator ]\n",
        "    vs = items_by_class[class_][ separator: ]\n",
        "\n",
        "    if ts == []:\n",
        "      ts = np.random.choice(range(len(items_by_class[class_])))\n",
        "      ts = [ items_by_class[class_][ts] ]\n",
        "\n",
        "    if vs == [] and copy_single_instance_to_val:\n",
        "      vs = np.random.choice(range(len(items_by_class[class_])))\n",
        "      vs = [ items_by_class[class_][vs] ]\n",
        "    \n",
        "    x_t = x_t + ts\n",
        "    y_t = y_t + ([ class_ ] * len(ts))\n",
        "\n",
        "    x_v = x_v + vs\n",
        "    y_v = y_v + ([ class_ ] * len(vs))\n",
        "\n",
        "  return np.array(x_t), np.array(y_t), np.array(x_v), np.array(y_v)\n",
        "\n",
        "def naive_oversampling(data, labels):\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  classes, frequencies = np.unique(labels, return_counts=True) #labels count\n",
        "\n",
        "  oversampled_count = np.max(frequencies) * len(frequencies) - data.shape[0] #oversampled data size\n",
        "\n",
        "  frequencies = np.max(frequencies) * 1 / frequencies #inverse values to convert them to sampling weights\n",
        "  frequencies = frequencies - frequencies[np.argmin(frequencies)]  #do not sample the most common class\n",
        "  \n",
        "  oversampled_data = np.zeros(\n",
        "      shape = (oversampled_count,) + data.shape[1:],\n",
        "      dtype = data.dtype\n",
        "  )\n",
        "\n",
        "  oversampled_labels = np.zeros(\n",
        "      shape = oversampled_count,\n",
        "      dtype = labels.dtype\n",
        "  )\n",
        "\n",
        "  frequencies = np.array([\n",
        "            frequencies[np.argwhere( classes == class_ ).flat[0]] for class_ in labels\n",
        "  ])\n",
        "  frequencies = frequencies / np.sum(frequencies)\n",
        "\n",
        "  indices = np.random.choice(data.shape[0], size = oversampled_count, p = frequencies)\n",
        "\n",
        "  for i, index in enumerate(indices):\n",
        "    oversampled_data[i] = data[index]\n",
        "    oversampled_labels[i] = labels[index]\n",
        "\n",
        "  return np.concatenate([data, oversampled_data], axis=0), np.concatenate([labels, oversampled_labels], axis=0)\n",
        "\n",
        "def generate_folds(data, labels, folds=3):\n",
        "  indices     = np.arange(len(labels))\n",
        "  permutation = np.random.permutation(indices)\n",
        "  \n",
        "  fold_indices = []\n",
        "  fold_len     = len(labels) // folds\n",
        "  \n",
        "  for i in range(folds):\n",
        "    fold = None\n",
        "\n",
        "    if i < folds - 1:\n",
        "      fold = np.array([ permutation.pop(-1) for i in range(fold_len) ])\n",
        "    else:\n",
        "      fold = permutation\n",
        "    \n",
        "    fold_indices.append(fold)\n",
        "\n",
        "  for fold in fold_indices:\n",
        "    val_indices   = fold\n",
        "    train_indices = np.array(\n",
        "        [ x for x in indices if x not in val_indices ]\n",
        "    )\n",
        "    yield data[train_indices], labels[train_indices], \\\n",
        "          data[val_indices  ], labels[val_indices  ]\n",
        "\n",
        "def normalize_multi_label(labels, max_value, smoothing=0.0):\n",
        "  new_labels = []\n",
        "\n",
        "  for label_line in labels.T:\n",
        "    new_label_line = normalize_labels(label_line, smoothing, max_value)\n",
        "    new_labels.append(new_label_line)\n",
        "  return np.array(new_labels).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JBfiDzcbSzr",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Modified DenseNet definition\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "import keras.utils as keras_utils\n",
        "\n",
        "from keras_applications import imagenet_utils\n",
        "from keras_applications.imagenet_utils import decode_predictions\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.layers import SpatialDropout2D\n",
        "\n",
        "def dense_block(x, blocks, name, regularizer_factory):\n",
        "    for i in range(blocks):\n",
        "        x = conv_block(x, 32, name=name + '_block' + str(i + 1), regularizer_factory=regularizer_factory)\n",
        "    return x\n",
        "\n",
        "def sd_dense_block(x, blocks, name, min_dropout_rate, max_dropout_rate, regularizer_factory):\n",
        "    \n",
        "  bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "  \n",
        "  rates = np.linspace(\n",
        "      start=min_dropout_rate,\n",
        "      stop=max_dropout_rate,\n",
        "      num=blocks,\n",
        "      endpoint=True\n",
        "  )\n",
        "  \n",
        "  outputs = [x]\n",
        "  \n",
        "  for i in range(blocks):\n",
        "    rates_slice = rates[:i+1]\n",
        "    block_name = name + '_block' + str(i + 1)\n",
        "\n",
        "    specific_block_input = [ SpatialDropout2D(rate)(x) for rate, x in zip(rates_slice, outputs) ]\n",
        "    if len(specific_block_input) > 1:\n",
        "      specific_block_input = layers.Concatenate(axis=bn_axis, name=block_name + '_concat')(specific_block_input)\n",
        "    else:\n",
        "      specific_block_input = specific_block_input[0]\n",
        "    \n",
        "    output = sd_conv_block(specific_block_input, 32, name=block_name, regularizer_factory=regularizer_factory)\n",
        "    outputs = [output] + outputs\n",
        "\n",
        "  return layers.Concatenate(axis=bn_axis, name=name + '_concat')(outputs)\n",
        "\n",
        "def transition_block(x, reduction, name, regularizer_factory):\n",
        "    \n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_bn')(x)\n",
        "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
        "    x = layers.Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n",
        "                      use_bias=False,\n",
        "                      name=name + '_conv',\n",
        "                      kernel_regularizer=regularizer_factory())(x)\n",
        "    x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_block(x, growth_rate, name, regularizer_factory):\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    \n",
        "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
        "                                   epsilon=1.001e-5,\n",
        "                                   name=name + '_0_bn')(x)\n",
        "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
        "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
        "                       use_bias=False,\n",
        "                       name=name + '_1_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                   name=name + '_1_bn')(x1)\n",
        "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3,\n",
        "                       padding='same',\n",
        "                       use_bias=False,\n",
        "                       name=name + '_2_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
        "    return x\n",
        "\n",
        "def sd_conv_block(x, growth_rate, name, regularizer_factory):\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    \n",
        "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
        "                                   epsilon=1.001e-5,\n",
        "                                   name=name + '_0_bn')(x)\n",
        "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
        "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
        "                       use_bias=False,\n",
        "                       name=name + '_1_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                   name=name + '_1_bn')(x1)\n",
        "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3,\n",
        "                       padding='same',\n",
        "                       use_bias=False,\n",
        "                       name=name + '_2_conv',\n",
        "                       kernel_regularizer=regularizer_factory())(x1)\n",
        "                       \n",
        "    return x1\n",
        "\n",
        "\n",
        "def DenseNet(blocks,\n",
        "             include_top=True,\n",
        "             weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "             input_tensor=None,\n",
        "             input_shape=None,\n",
        "             pooling=None,\n",
        "             classes=1000,\n",
        "             blocks_to_include=3,\n",
        "             include_large_conv=False,\n",
        "             min_dropout_rate=0.0,\n",
        "             max_dropout_rate=0.0,\n",
        "             regularizer_factory=lambda: None):\n",
        "    \n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=224,\n",
        "                                      min_size=32,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=include_top,\n",
        "                                      weights=weights)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    if include_large_conv:\n",
        "      x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
        "      x = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv', kernel_regularizer=regularizer_factory())(x)\n",
        "    else:\n",
        "      #new layer\n",
        "      x = layers.Conv2D(64, 3, use_bias=False, padding='same', name='conv1/conv_replaced', kernel_regularizer=regularizer_factory())(img_input)\n",
        "    \n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
        "    x = layers.Activation('relu', name='conv1/relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\n",
        "    \n",
        "    if abs(max_dropout_rate) < 1e-4: #If dropout rate defined as zero, use non-dropout version\n",
        "      x = dense_block(x, blocks[0], name='conv2', regularizer_factory=regularizer_factory)\n",
        "      x = transition_block(x, 0.5, name='pool2',  regularizer_factory=regularizer_factory)\n",
        "      x = dense_block(x, blocks[1], name='conv3', regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 2:\n",
        "        x = transition_block(x, 0.5, name='pool3',  regularizer_factory=regularizer_factory)\n",
        "        x = dense_block(x, blocks[2], name='conv4', regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 3:\n",
        "        x = transition_block(x, 0.5, name='pool4',  regularizer_factory=regularizer_factory)\n",
        "        x = dense_block(x, blocks[3], name='conv5', regularizer_factory=regularizer_factory)\n",
        "    else: #If non-zero dropout rate, use specialized dropout blocks\n",
        "      x = sd_dense_block(x, blocks[0], 'conv2', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      x = transition_block(x, 0.5, 'pool2', regularizer_factory=regularizer_factory)\n",
        "      x = sd_dense_block(x, blocks[1], 'conv3', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 2:\n",
        "        x = transition_block(x, 0.5, 'pool3', regularizer_factory=regularizer_factory)\n",
        "        x = sd_dense_block(x, blocks[2], 'conv4', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "      if blocks_to_include > 3:\n",
        "        x = transition_block(x, 0.5, 'pool4', regularizer_factory=regularizer_factory)\n",
        "        x = sd_dense_block(x, blocks[3], 'conv5', min_dropout_rate, max_dropout_rate, regularizer_factory=regularizer_factory)\n",
        "    \n",
        "    bn_name = 'bn'\n",
        "    if blocks_to_include < 3:\n",
        "      bn_name = 'bn_replaced'\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=bn_name)(x)\n",
        "    x = layers.Activation('relu', name='relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        elif pooling == 'max':\n",
        "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "\n",
        "    #Compute conv layers number to name the model 'densenetX'\n",
        "    \n",
        "    true_blocks = blocks[:blocks_to_include]\n",
        "    model_name = sum([1] + list(map(lambda x: x * 2 + 1, true_blocks)))\n",
        "    model_name = 'densenet' + str(model_name)\n",
        "    \n",
        "    model = models.Model(inputs, x, name=model_name)\n",
        "\n",
        "    # Load weights.\n",
        "    if weights is not None:\n",
        "        model.load_weights(weights, by_name=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def ModifiedDenseNet(\n",
        "      blocks_set = 121,\n",
        "      weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "      input_shape=None,\n",
        "      pooling=None,\n",
        "      blocks_to_include=3,\n",
        "      include_large_conv=False,\n",
        "      min_dropout_rate = 0.0,\n",
        "      max_dropout_rate = 0.0,\n",
        "      regularizer_factory=lambda: None\n",
        "    ):\n",
        "    \n",
        "    blocks = {\n",
        "        121 : [6, 12, 24, 16],\n",
        "        169 : [6, 12, 32, 32],\n",
        "        201 : [6, 12, 48, 32]\n",
        "    }\n",
        "    if isinstance(blocks_set, list):\n",
        "      blocks = blocks_set\n",
        "    elif isinstance(blocks_set, int):\n",
        "      if blocks_set in blocks.keys():\n",
        "        blocks = blocks[blocks_set]\n",
        "      else:\n",
        "        raise ValueError('No such blocks set defined')\n",
        "    else:\n",
        "      raise ValueError('No such blocks set defined')\n",
        "    \n",
        "    return DenseNet(blocks,\n",
        "                    False, weights,\n",
        "                    None, input_shape,\n",
        "                    pooling, 1000, blocks_to_include, include_large_conv, min_dropout_rate, max_dropout_rate, regularizer_factory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6shlJmLWbdfb",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Model defining functions\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import re\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.applications.resnet import ResNet50, ResNet101, ResNet152\n",
        "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201, preprocess_input\n",
        "from keras.layers import Dense, Dropout, SpatialDropout2D\n",
        "from keras.models import Model\n",
        "from keras.losses import categorical_crossentropy, mean_absolute_error, mean_squared_error, logcosh\n",
        "from keras.optimizers import adam, sgd\n",
        "from keras import regularizers\n",
        "from keras.models import model_from_json\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "\n",
        "def denormalized_mae(y_true, y_pred, max_value, smoothing):\n",
        "  y_true = K.constant(0.5) + (y_true - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "  y_pred = K.constant(0.5) + (y_pred - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "\n",
        "  y_true = y_true * max_value\n",
        "  y_pred = y_pred * max_value\n",
        "\n",
        "  return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "def denormalized_mse(y_true, y_pred, max_value, smoothing):\n",
        "  y_true = K.constant(0.5) + (y_true - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "  y_pred = K.constant(0.5) + (y_pred - K.constant(0.5))/K.constant(1 - smoothing)\n",
        "\n",
        "  y_true = y_true * max_value\n",
        "  y_pred = y_pred * max_value\n",
        "\n",
        "  return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "def get_denormalized_function(max_value, func_name, smoothing):\n",
        "  def mae(y_true, y_pred):\n",
        "    return denormalized_mae(y_true, y_pred, max_value, smoothing)\n",
        "\n",
        "  def mse(y_true, y_pred):\n",
        "    return denormalized_mse(y_true, y_pred, max_value, smoothing)\n",
        "\n",
        "  funcs = {\n",
        "      'mae' : mae,\n",
        "      'mse' : mse,\n",
        "  }\n",
        "\n",
        "  return funcs[func_name]\n",
        "\n",
        "def get_lr_metric(optimizer): #Custom metric to monitor learning rate\n",
        "  def lr(y_true, y_pred):\n",
        "      return optimizer.lr\n",
        "  return lr\n",
        "\n",
        "def get_feet_erosion_estimator():\n",
        "\n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=feet_image_shape,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = 0.95,\n",
        "    regularizer_factory=lambda: l1(1e-4)\n",
        "  )\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "  \n",
        "  x = Dense(1, name='output', activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs  = base_model.input,\n",
        "                outputs = x,\n",
        "                name = base_model.name\n",
        "                )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  #optimizer = sgd(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "\n",
        "  lr = get_lr_metric(optimizer)\n",
        "  \n",
        "  mae = get_denormalized_function(feet_erosion_values - 1, 'mae', 0.0)\n",
        "  mse = get_denormalized_function(feet_erosion_values - 1, 'mae', 0.0)\n",
        "\n",
        "  metrics = [\n",
        "             mean_absolute_error,\n",
        "             mean_squared_error,\n",
        "\n",
        "             mae,\n",
        "             mse,\n",
        "\n",
        "             lr\n",
        "            ]\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "                \n",
        "                #loss=mean_squared_error,\n",
        "                #loss=mean_absolute_error,\n",
        "                loss=logcosh,\n",
        "\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def get_wrist_erosion_estimator():\n",
        "    \n",
        "  base_model = ModifiedDenseNet(\n",
        "    blocks_set=121,\n",
        "    weights=DENSENET121_WEIGHT_PATH_NO_TOP,\n",
        "    input_shape=actual_wrist_estimator_input_shape,\n",
        "    pooling='avg',\n",
        "    blocks_to_include=3,\n",
        "    include_large_conv=False,\n",
        "    min_dropout_rate = 0.0,\n",
        "    max_dropout_rate = 0.95,\n",
        "    regularizer_factory=lambda: l1(1e-4)\n",
        "  )\n",
        "\n",
        "  x = base_model.output\n",
        "  #x = layers.Flatten()(x)\n",
        "\n",
        "  def output_model_factory(name):\n",
        "    return Dense(1, name=name, activation='sigmoid')(x)\n",
        "    #x = Dense(c_num, name=name, activation='softmax')(x)\n",
        "  \n",
        "  output_models = dict(\n",
        "      [ ( name, output_model_factory(name) ) for name in WRIST_EROSION_OUTPUTS  ]\n",
        "  )\n",
        "\n",
        "  model = Model(\n",
        "      inputs  = base_model.input,\n",
        "      outputs = list(output_models.values()),\n",
        "      name = base_model.name\n",
        "  )\n",
        "  \n",
        "  optimizer = adam(lr=0.0001)\n",
        "  lr = get_lr_metric(optimizer)\n",
        "\n",
        "  #Equal losses to all outputs:\n",
        "  losses = dict(\n",
        "      [ ( name, logcosh ) for name in WRIST_EROSION_OUTPUTS ]\n",
        "  )\n",
        "\n",
        "  #Equal loss weights for all outputs:\n",
        "  loss_weights = dict(\n",
        "      [ ( name, 1.0 ) for name in WRIST_EROSION_OUTPUTS ]\n",
        "  )\n",
        "\n",
        "\n",
        "  mae = get_denormalized_function(hand_erosion_values - 1, 'mae', 0.0)\n",
        "  mse = get_denormalized_function(hand_erosion_values - 1, 'mse', 0.0)\n",
        "\n",
        "  metrics = [\n",
        "            #'accuracy',\n",
        "            \n",
        "            mean_absolute_error,\n",
        "            mean_squared_error,\n",
        "            \n",
        "            mae,\n",
        "            mse,\n",
        "\n",
        "            lr\n",
        "          ]\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=losses,\n",
        "                loss_weights=loss_weights,\n",
        "                metrics=metrics)\n",
        "  \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok2YFa2qoUnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Normalize labels\n",
        "\n",
        "#feet_erosion   = normalize_labels(feet_erosion,   max_value = feet_erosion_values - 1, smoothing=0.0)\n",
        "#feet_narrowing = normalize_labels(feet_narrowing, max_value = feet_narrowing_values - 1,      smoothing=0.0)\n",
        "\n",
        "#finger_erosion   = normalize_labels(finger_erosion,   max_value = hand_erosion_values - 1,   smoothing=0.0)\n",
        "#finger_narrowing = normalize_labels(finger_narrowing, max_value = hand_narrowing_values - 1, smoothing=0.0)\n",
        "\n",
        "#wrist_common_erosion   = normalize_multi_label(wrist_common_erosion,   max_value = hand_erosion_values - 1,   smoothing=0.0)\n",
        "#wrist_common_narrowing = normalize_multi_label(wrist_common_narrowing, max_value = hand_narrowing_values - 1, smoothing=0.0)\n",
        "\n",
        "#for array in [ feet_erosion,\n",
        "#               feet_narrowing,\n",
        "#               finger_erosion,\n",
        "#               finger_narrowing,\n",
        "#               wrist_common_erosion,\n",
        "#               wrist_common_narrowing]:\n",
        "#  print(array.shape,\n",
        "#        array.mean(axis=0),\n",
        "#        array.std(axis=0)\n",
        "#        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN9TGbgTjVGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}