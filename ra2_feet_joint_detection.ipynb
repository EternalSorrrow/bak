{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ra2_feet_joint_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPgPg254A/y3HhjX3vUNqkQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EternalSorrrow/bak/blob/master/ra2_feet_joint_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aWZ6P1aVtsa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "!git clone https://github.com/matterport/Mask_RCNN.git\n",
        "\n",
        "%cd Mask_RCNN\n",
        "!python setup.py install\n",
        "!pip show mask-rcnn\n",
        "\n",
        "%cd ..\n",
        "#!pip3 install imgaug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErgeJwm0QTxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhuV_xBlSA94",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Paths definition\n",
        "\n",
        "train_set_path = 'drive/My Drive/Work/ML/RA2/ra2/train/'\n",
        "subset_path = 'drive/My Drive/Work/ML/RA2/ra2/feet_subset/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od7WKM7NVYL-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Temp set files count\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "#for item in items_to_select:\n",
        "#  shutil.copy(train_set_path + item, temp_set_path)\n",
        "\n",
        "len(os.listdir(subset_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJB1tFZWaVb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load annotation file\n",
        "\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "annotations = None\n",
        "\n",
        "with open(subset_path + 'project.json') as json_file:\n",
        "  annotations = json.load(json_file)\n",
        "  annotations = list(annotations['_via_img_metadata'].values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWsMsBPf1xj7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load images and parse annotations\n",
        "\n",
        "def load_images(anns):\n",
        "  files = dict()\n",
        "\n",
        "  for ann in anns:\n",
        "    if ann['regions']:\n",
        "      f = cv2.imread(subset_path + ann['filename'])\n",
        "      files[ann['filename']] = f\n",
        "    else: #Skip images with no annotations\n",
        "      pass\n",
        "  \n",
        "  return files\n",
        "\n",
        "def load_annotations(anns):\n",
        "  regions = dict()\n",
        "\n",
        "  for ann in anns:\n",
        "    path = ann['filename']\n",
        "    ann = ann['regions']\n",
        "    \n",
        "    if ann:\n",
        "      file_regions = dict()\n",
        "\n",
        "      for region in ann:\n",
        "        region_class = region['region_attributes']['joint']\n",
        "        poly = list(zip(\n",
        "            region['shape_attributes']['all_points_x'],\n",
        "            region['shape_attributes']['all_points_y']\n",
        "        ))\n",
        "        file_regions[region_class] = poly\n",
        "      \n",
        "      regions[path] = file_regions\n",
        "    else: #Skip images with no annotations\n",
        "      pass\n",
        "\n",
        "  return regions\n",
        "\n",
        "imgs = load_images(annotations)\n",
        "anns = load_annotations(annotations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6m7lgQB2auI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Example image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(len(anns.keys()), 'annotation sets found')\n",
        "ex_fname = np.random.choice(list(imgs.keys()))\n",
        "\n",
        "img = imgs[ex_fname].copy()\n",
        "ans = anns[ex_fname].values()\n",
        "\n",
        "randcol = lambda : (np.random.randint(255), np.random.randint(255), np.random.randint(255)) \n",
        "\n",
        "for poly in ans:\n",
        "  poly = np.array(poly, dtype=np.int32)\n",
        "  cv2.fillPoly(img, [poly], randcol())\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnp9LIYPohzj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modular live loss plotter callback\n",
        "\n",
        "#Modular live loss plotter for Keras models\n",
        "#Allows to create custom layouts of per-batch or per-epoch plots for different metrics\n",
        "\n",
        "#Monitor class defines a plot, which either may be batch or epoch-scoped, and may contain several graphs\n",
        "#Batch monitor plots its values per batch, and refreshes itself on new epoch begin\n",
        "#Epoch monitor plots its values per epoch, and performs no refresh\n",
        "#All values/last N values displaying\n",
        "#Log-scale/Linear scale displaying\n",
        "\n",
        "#Plotter callback handles different Monitors and responds to the actual plotting\n",
        "#Defines a grid where Monitors will be drawn, grid size, refresh rate in batches\n",
        "#when the Monitors will be re-drawn in addition to per-epoch update\n",
        "#Plotter can be silenced to disable plotting and only archivate per-epoch data\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Monitor():\n",
        "    def __init__(self, scope='epoch', monitors= [ 'loss' ], plot_last=-1, log_scale=False, precision=4):\n",
        "        self.scope = scope.lower()\n",
        "        self.monitors = [ monitor.lower() for monitor in monitors ]\n",
        "        self.plot_last = max(0, plot_last)\n",
        "        self.x = []\n",
        "        self.ys = [ [] for monitor in monitors ]\n",
        "        self.log_scale = log_scale\n",
        "        self.precision = precision\n",
        "\n",
        "    def reinit(self):\n",
        "        self.x = []\n",
        "        self.ys = [ [] for monitor in self.monitors ]\n",
        "\n",
        "    def update(self, iteration, logs={}):\n",
        "        self.x.append(iteration)\n",
        "        \n",
        "        for i, monitor in enumerate(self.monitors):\n",
        "            if logs.get(monitor) is not None:\n",
        "                self.ys[i].append(logs.get(monitor))\n",
        "            else:\n",
        "                pass #Action to execute when cannot get info for a certain monitor\n",
        "\n",
        "    def plot(self, axis):\n",
        "        x_data = self.x[ -self.plot_last : ]\n",
        "        y_array = [ y_data[ -self.plot_last : ] for y_data in self.ys ]\n",
        "\n",
        "        for i, y_data in enumerate(y_array):\n",
        "            label = self.monitors[i] + '_' + self.scope #Compose graph name\n",
        "            if self.log_scale:\n",
        "                axis.set_yscale('log') #Set up scale\n",
        "                \n",
        "            if len(x_data) == len(y_data): #If data are coherent, plot them\n",
        "                axis.plot(x_data, y_data, label=label)\n",
        "\n",
        "                if self.precision > 0 and len(y_data) > 0: #If there's a last point plotted, print its value\n",
        "                    text = str(round(y_data[-1],  self.precision))\n",
        "                    axis.text(x_data[-1], y_data[-1], text)\n",
        "            else:\n",
        "                continue\n",
        "                \n",
        "        label = {'batch' : 'Batches', 'epoch' : 'Epochs'} #Set up x-label\n",
        "        axis.set_xlabel(label[self.scope])\n",
        "        \n",
        "        axis.legend()\n",
        "\n",
        "\n",
        "class Plotter(Callback):\n",
        "    def __init__(self, scale=5, n_cols=2, n_rows=1, monitors=[], refresh_rate=-1, silent=False):\n",
        "        if (n_cols * n_rows < len(monitors)):\n",
        "            raise ValueError('Grid is too small to fit all monitors!')\n",
        "\n",
        "        self.n_cols = n_cols\n",
        "        self.n_rows = n_rows\n",
        "        self.scale = scale\n",
        "\n",
        "        self.monitors = monitors\n",
        "\n",
        "        self.batch_monitors, self.epoch_monitors = [], []\n",
        "\n",
        "        for monitor in monitors:\n",
        "            if monitor.scope == 'epoch':\n",
        "                self.epoch_monitors.append(monitor)\n",
        "            elif monitor.scope == 'batch':\n",
        "                self.batch_monitors.append(monitor)\n",
        "\n",
        "        self.refresh_rate = refresh_rate\n",
        "        self.silent = False\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        pass\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        [ monitor.reinit() for monitor in self.batch_monitors ]\n",
        "\n",
        "    def plot(self):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        figsize = ( self.scale * self.n_cols, self.scale * self.n_rows)\n",
        "        fig, ax = plt.subplots(figsize=figsize, ncols=self.n_cols, nrows=self.n_rows)\n",
        "\n",
        "        if self.n_cols * self.n_rows == 1:\n",
        "          ax = np.array([ax])\n",
        "\n",
        "        for index, axis in enumerate(ax.flat):\n",
        "          if index < len(self.monitors):\n",
        "              self.monitors[index].plot(axis)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        [ monitor.update(batch, logs) for monitor in self.batch_monitors ]\n",
        "\n",
        "        if self.silent or batch == 0 or self.refresh_rate <= 0 or batch % self.refresh_rate != 0:\n",
        "            return\n",
        "\n",
        "        self.plot()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        [ monitor.update(epoch, logs) for monitor in self.epoch_monitors ]\n",
        "\n",
        "        if self.silent:\n",
        "            return\n",
        "\n",
        "        self.plot()\n",
        "\n",
        "    def reinit(self):\n",
        "      [ monitor.reinit() for monitor in self.monitors ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jq6LEnCQBwA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Import Mask R-CNN dependencies\n",
        "\n",
        "%cd Mask_RCNN/\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n",
        "import mrcnn\n",
        "from mrcnn.utils import Dataset\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "from os import listdir\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLI38iRHv1yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = 80\n",
        "val_samples = 8\n",
        "\n",
        "classes_num = 6\n",
        "max_instances_to_detect = int(classes_num * 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslR_t43JTTw",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Configuration definition\n",
        "\n",
        "class FeetJointsConfig(Config):\n",
        "    # give the configuration a recognizable name\n",
        "    NAME = \"FeetJoints_config\"\n",
        " \n",
        "    # set the number of GPUs to use along with the number of images\n",
        "    # per GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 4\n",
        " \n",
        "    # number of classes (we would normally add +1 for the background)\n",
        "     # kangaroo + BG\n",
        "    NUM_CLASSES = classes_num + 1\n",
        "   \n",
        "    # Number of training steps per epoch\n",
        "    STEPS_PER_EPOCH = max(1, train_samples // IMAGES_PER_GPU)\n",
        "    VALIDATION_STEPS = max(1, val_samples // IMAGES_PER_GPU)\n",
        "\n",
        "    #Select backbone: resnet50 or resnet101\n",
        "    BACKBONE = \"resnet101\"\n",
        "\n",
        "    #Image resizing\n",
        "    #IMAGE_RESIZE_MODE = \"square\"\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    #IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Learning rate\n",
        "    LEARNING_RATE=0.001\n",
        "    \n",
        "    # Set lower confidence threshold\n",
        "    DETECTION_MIN_CONFIDENCE = 0.0\n",
        "    \n",
        "    # setting Max ground truth instances\n",
        "    MAX_GT_INSTANCES=6\n",
        "\n",
        "    # max detected instances\n",
        "    DETECTION_MAX_INSTANCES = max_instances_to_detect\n",
        "\n",
        "config = FeetJointsConfig()\n",
        "config.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdvOxljXJcDA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Dataset class definition\n",
        "\n",
        "class FeetJoints(Dataset):\n",
        "  def train_val_split(self, split = 32):\n",
        "    \n",
        "    annotations = json.load(open(os.path.join(subset_path, \"project.json\")))\n",
        "    annotations = annotations['_via_img_metadata']\n",
        "    annotations = list(annotations.values())  # don't need the dict keys\n",
        "    annotations = [a for a in annotations if a['regions']]\n",
        "\n",
        "    files = [ a['filename'] for a in annotations ]\n",
        "    files = np.random.permutation(files)\n",
        "\n",
        "    return files[:split], files[split:]\n",
        "\n",
        "  def load_joints(self, subset):\n",
        "    #Add Classes\n",
        "    \n",
        "    self.add_class(\"joints\", 1, \"mtp_1\")\n",
        "    self.add_class(\"joints\", 2, \"mtp_2\")\n",
        "    self.add_class(\"joints\", 3, \"mtp_3\")\n",
        "    self.add_class(\"joints\", 4, \"mtp_4\")\n",
        "    self.add_class(\"joints\", 5, \"mtp_5\")\n",
        "    self.add_class(\"joints\", 6, \"mtp_ip\")\n",
        "    \n",
        "    \n",
        "    # Load annotations\n",
        "    # VGG Image Annotator (up to version 1.6) saves each image in the form:\n",
        "    # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
        "    #   'regions': {\n",
        "    #       '0': {\n",
        "    #           'region_attributes': {},\n",
        "    #           'shape_attributes': {\n",
        "    #               'all_points_x': [...],\n",
        "    #               'all_points_y': [...],\n",
        "    #               'name': 'polygon'}},\n",
        "    #       ... more regions ...\n",
        "    #   },\n",
        "    #   'size': 100202\n",
        "    # }\n",
        "    # We mostly care about the x and y coordinates of each region\n",
        "    # Note: In VIA 2.0, regions was changed from a dict to a list.\n",
        "    annotations = json.load(open(os.path.join(subset_path, \"project.json\")))\n",
        "    annotations = annotations['_via_img_metadata']\n",
        "    annotations = list(annotations.values())  # don't need the dict keys\n",
        "\n",
        "    #Skip files not included into our subset\n",
        "\n",
        "    # The VIA tool saves images in the JSON even if they don't have any\n",
        "    # annotations. Skip unannotated images and images not included into passed subset\n",
        "    annotations = [a for a in annotations if a['regions'] and a['filename'] in subset ]\n",
        "    assert len(annotations) == len(subset)\n",
        "\n",
        "    # Add images\n",
        "    for a in annotations:\n",
        "        # Get the x, y coordinaets of points of the polygons that make up\n",
        "        # the outline of each object instance. These are stores in the\n",
        "        # shape_attributes (see json format above)\n",
        "        # The if condition is needed to support VIA versions 1.x and 2.x.\n",
        "        if type(a['regions']) is dict:\n",
        "            polygons = [(r['region_attributes']['joint'], r['shape_attributes']) for r in a['regions'].values()]\n",
        "        else:\n",
        "            polygons = [(r['region_attributes']['joint'], r['shape_attributes']) for r in a['regions']] \n",
        "\n",
        "        # load_mask() needs the image size to convert polygons to masks.\n",
        "        # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
        "        # the image. This is only managable since the dataset is tiny.\n",
        "        image_path = os.path.join(train_set_path, a['filename'])\n",
        "        image = cv2.imread(image_path)\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        self.add_image(\n",
        "            \"joints\",\n",
        "            image_id=a['filename'],  # use file name as a unique image id\n",
        "            path=image_path,\n",
        "            width=width, height=height,\n",
        "            polygons=polygons)\n",
        "        \n",
        "  def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        \n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"joints\":\n",
        "          return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Convert polygons to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "\n",
        "        info = self.image_info[image_id]\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
        "                        dtype=np.uint8)\n",
        "        \n",
        "        classes_dict = dict([ (item['name'], item['id']) for item in self.class_info ])\n",
        "        \n",
        "        for j, p in info[\"polygons\"]:\n",
        "            i = classes_dict[j] - 1\n",
        "            \n",
        "            # Get indexes of pixels inside the polygon and set them to 1\n",
        "            poly = np.array(list(zip(p['all_points_x'], p['all_points_y'])), dtype=np.int32)\n",
        "            poly_map = np.zeros(shape=(mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
        "            cv2.fillPoly(poly_map, [ poly ], 1)\n",
        "            mask[:, :, i] = poly_map\n",
        "\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # one class ID only, we return an array of 1s\n",
        "        return mask.astype(np.bool), np.arange(classes_num) + 1\n",
        "    \n",
        "  def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"joints\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n",
        "\n",
        "def get_data():#Training Dataset\n",
        "    \n",
        "    dataset_train = FeetJoints()\n",
        "\n",
        "    train_subset, val_subset = dataset_train.train_val_split(train_samples)\n",
        "\n",
        "    dataset_train.load_joints(train_subset)\n",
        "    dataset_train.prepare()#Validating Dataset\n",
        "    \n",
        "    dataset_val = FeetJoints()        \n",
        "    dataset_val.load_joints(val_subset)    \n",
        "    dataset_val.prepare()\n",
        "\n",
        "    return dataset_train, dataset_val\n",
        "\n",
        "train, val = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzKiKTSMhsnG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Sample some dataset images\n",
        "\n",
        "print(\"Image Count: {}, {}\".format(len(train.image_ids), len(val.image_ids)))\n",
        "print(\"Class Count: {}, {}\".format(train.num_classes, val.num_classes))\n",
        "for i, info in enumerate(train.class_info):\n",
        "    print(\"{:3}. {:50}\".format(i, info['name']))\n",
        "\n",
        "image_ids = np.random.choice(train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = train.load_image(image_id)\n",
        "    mask, class_ids = train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, train.class_names, limit=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoJqa5DJ3Mhh",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Display random image with regions and BBs\n",
        "\n",
        "from mrcnn import utils\n",
        "\n",
        "# Load random image and mask.\n",
        "image_id = np.random.choice(train.image_ids)\n",
        "image = train.load_image(image_id)\n",
        "mask, class_ids = train.load_mask(image_id)\n",
        "# Compute Bounding box\n",
        "bbox = utils.extract_bboxes(mask)\n",
        "\n",
        "# Display image and additional stats\n",
        "print(\"image_id \", image_id, train.image_reference(image_id))\n",
        "\n",
        "# Display image and instances\n",
        "visualize.display_instances(image, bbox, mask, class_ids, train.class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x2r9EzgD3s5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create logs folder and get COCO weights\n",
        "\n",
        "!mkdir logs\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2-uL58F9KKg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define the model building function\n",
        "\n",
        "MODEL_DIR = 'logs'\n",
        "COCO_MODEL_PATH = 'mask_rcnn_coco.h5'\n",
        "\n",
        "def get_model(model_dir,\n",
        "              init_with = \"coco\",  # imagenet, coco, or last\n",
        "              ):\n",
        "  model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=model_dir)\n",
        "  \n",
        "  if init_with == \"imagenet\":\n",
        "      model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "  elif init_with == \"coco\":\n",
        "      # Load weights trained on MS COCO, but skip layers that\n",
        "      # are different due to the different number of classes\n",
        "      # See README for instructions to download the COCO weights\n",
        "      model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                        exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-pOr1A_IHid",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Auxilary drawing function\n",
        "\n",
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHYBiWU8KfET",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define and visualize augmentations\n",
        "\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "seq = iaa.Sequential([\n",
        "    \n",
        "    iaa.Fliplr(0.5), # horizontal flips\n",
        "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
        "    \n",
        "    # Small gaussian blur with random sigma between 0 and 0.5.\n",
        "    # But we only blur about 50% of all images.\n",
        "    iaa.Sometimes(\n",
        "        0.5,\n",
        "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
        "    ),\n",
        "    \n",
        "    # Strengthen or weaken the contrast in each image.\n",
        "    iaa.LinearContrast((0.75, 1.5)),\n",
        "    \n",
        "    # Add gaussian noise.\n",
        "    # For 50% of all images, we sample the noise once per pixel.\n",
        "    # For the other 50% of all images, we sample the noise per pixel AND\n",
        "    # channel. This can change the color (not only brightness) of the\n",
        "    # pixels.\n",
        "    #iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
        "    \n",
        "    # Make some images brighter and some darker.\n",
        "    # In 20% of all cases, we sample the multiplier once per channel,\n",
        "    # which can end up changing the color of the images.\n",
        "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
        "    \n",
        "    # Apply affine transformations to each image.\n",
        "    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
        "    iaa.Affine(\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "        translate_percent={\"x\": (0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
        "        rotate=(-5, 5),\n",
        "        #shear=(-8, 8)\n",
        "    )\n",
        "], random_order=True) # apply augmenters in random order\n",
        "\n",
        "axes = get_ax(cols=4, rows=3, size=4)\n",
        "\n",
        "for ax in axes.flat:\n",
        "  image_id = np.random.choice(train.image_ids)\n",
        "  image = train.load_image(image_id)\n",
        "  image = seq(images=[ image ])[0]\n",
        "\n",
        "  ax.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVkvlDJ4C-4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_model(MODEL_DIR, init_with = 'coco')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGjBaD066yCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title LR scheduler\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def get_schedule(base_lr = 0.001, momentum=0.995):\n",
        "  return lambda x: base_lr * momentum ** x\n",
        "\n",
        "schedule = get_schedule(momentum=0.99)\n",
        "lr_scheduler = LearningRateScheduler(schedule)\n",
        "\n",
        "ax = get_ax()\n",
        "ax.plot([ schedule(i) for i in range(300) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjC2ysYYNucS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define plotter monitors\n",
        "\n",
        "monitors = [\n",
        "    Monitor(scope='epoch', monitors = ['loss', 'val_loss'], plot_last=128),\n",
        "]\n",
        "\n",
        "plotter = Plotter(monitors=monitors, n_rows=1, n_cols=1, scale=6, refresh_rate=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYfmbNVQholS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Remove previously saved checkpoints\n",
        "\n",
        "!rm -r logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTIrvRjaDCah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "head_epochs = 300\n",
        "\n",
        "model.train(\n",
        "    train, val, \n",
        "    learning_rate=config.LEARNING_RATE, \n",
        "    epochs=head_epochs, \n",
        "    layers='heads',\n",
        "    augmentation=seq,\n",
        "    custom_callbacks = [ plotter, lr_scheduler ]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RVeCMcGE0rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "fine_epochs = 0\n",
        "\n",
        "model.train(\n",
        "    train, val, \n",
        "    learning_rate=config.LEARNING_RATE * 0.1,\n",
        "    epochs=head_epochs + fine_epochs, \n",
        "    layers=\"all\",\n",
        "    augmentation=seq,\n",
        "    custom_callbacks = [ plotter, lr_scheduler ]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp2m9M2EF4wX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Rebuild the model for inference\n",
        "\n",
        "class InferenceConfig(FeetJointsConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaveNKIKHj7j",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Test on random image, display GT first\n",
        "\n",
        "# Val ground truth\n",
        "image_id = np.random.choice(val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            train.class_names, figsize=(16, 16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJT4A1RH192",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Prediction\n",
        "\n",
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            val.class_names, r['scores'], ax=get_ax(size=16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHGwuY5zICfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}